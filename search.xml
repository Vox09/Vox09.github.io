<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Machine Learning]]></title>
    <url>%2F2019%2F10%2F09%2FMachine-Learning%2F</url>
    <content type="text"><![CDATA[Basic Naive Bayes Classifier Bayes Rule \(P(A|B)={P(A\and B)\over P(B)}={P(B|A)P(A)\over P(B)}\) When \(h\) stands for hypothesis, \(D\) stands for data \(P(h|D) = {P(D|h)P(h)\over P(D)}={P(D|h)P(h)\over \sum_hP(D|h)P(h)}\) \(P(h)\) prior probability of \(h\). Initial probability before observing. \(P(h|D)\) posterior probability of \(h\) after observing the data \(D\) \(P(D|h)\) likelihood of observing the data \(D\) given hypothesis \(h\) \(P(D)\) observed probability that training data \(D\) will be observed Independence and Conditional Independence Two random variables \(X\) and \(Y\) are independent if \(P(X|Y)=P(X)\) or \(P(Y|X)=P(Y)\) Two random variables \(X\) and \(Y\) are conditionally independent given \(Z\) if \(P(X|Y,Z)=P(X,Z)\) Naive Bayes Classifier Assumption: Attributes are conditionally independent of each other given the class variable Objective: Maximum the posterior probability given the data. \(v_{NB} = arg\ max_{v_j\in V}P(v_j)\Pi^n_{i=1}P(a_i|v_j)\) where \(a\) is the attribute, \(v\) is the class variable. What if \(P(a_i|v_j)\) is zero? Laplace correction Add a virtual count of 1 to each attribute value. change \({n_c\over n}\to{n_c+1\over n+|a|}\) Overfit hypothesis \(h\in H\) overfits the training data if there is an alternative hypothesis \(h_0\in H\) such that \(h\) has a smaller error than \(h_0\) over the training examples, \(h\) has a larger error than \(h_0\) over the test examples (entire distribution of instances) k-fold cross-validation Divide m examples to k fold. Each time use one fold for validation, others for training. Repeat k times. k-fold stratified cross-validation partition the m examples into k folds such that each class is uniformly distributed among the k folds. Artificial Neural Network Perceptron inputs \(x_1,x_2...\) or \(\vec x\) weights \(w_1,w_2....\) or \(\vec w\) bias \(b\) or threshold \(t\) activation function \(f(x)\) output \(o = f(\vec x\vec w -b)\) Adaline A feed-forward network with one layer of adjustable weights connected to one or more linear units (as output units) \(o = \sum^n_{i=0}w_ix_i = \vec x\vec w\) target output for training data \(d: t_d\) output for the training data \(d: o_d\) squared training error : \(E(\vec w) = {1\over 2}\sum_{d\in D}(t_d-o_D)^2\) Gradient Descent \(\Delta E(\vec w) = [{\part E\over\part w_0},{\part E\over\part w_1},...{\part E\over\part w_n}]\) we update the \(\vec w\) with learning rate \(\eta\) by \(w\leftarrow w+\Delta w\) where \(\Delta w = -\eta\nabla E(\vec w)=\eta\sum(t_d-o_d)x_d\) Hidden Layer MLP Multi-layer perceptron If hidden units were linear, then multi-layer is no better than a single layer. We need nonlinearity, to make the network perform better. Common activation function and derivative: Sigmoid \(f = {1\over 1+e^{-x}}\) derivative \(f&#39; = f(1-f)\) Radial basis function network(RBF) \(f =exp(-{(x-w_j)^T(x-w_j)\over2\sigma_j^2})\) Gaussian distribution. ReLU \(f = max(0,x)\) derivative \(f&#39; = (x&gt;0)?1:0\) tanh \(f=tanh(x)\) derivative \(f&#39;=1-tanh^2(x)\) Back-Propagation \({\part E\over \part w_i} = {\part\over\part w_i}{1\over 2}(t-o)^2 = -(t-o)({\part o\over \part w_i}) = -(t-o){\part o\over\part f_a}{\part f_a\over\part w_i}\) where \(f_a\) is the input of the activation function, and \({\part f_a\over\part w_i} = x_i\) \({\part E\over \part w_i} =-(t-o){\part o\over \part f_a}x_i = -\delta x_i\) where \(\delta = (t-o){\part o \over \part f_a}\) Weight update \(w_{ji}\leftarrow w_{ji}+\Delta w_{ji}=w+\eta\delta_jx_i\) - batch gradient descent: use all examples in each iteration - stochastic gradient descent: use 1 example in each iteration - mini-batch gradient descent: use b examples in each iteration To speed up the BP algorithm, we can use the momentum term \(\Delta w(t+1)=-\eta{\part E\over\part w}+\alpha\Delta w(t)\). To avoid get in stuck in local minima, we can train multiple networks using the same data, but initialize each with different random weights. CNN Nonlinearity is from ReLU. Zero padding each layer to avoid data shrinking. Max pooling to extract features and approximately locate their location. 1x1 Convolution \(D_K\) is the dimension of kernel, \(D_F\) is the dimension of features, \(M\) is the input data channel, \(N\) is the output feature channel original cost of convolution \(D_K^2MND_F^2\) depth-wise convolution \(D_K^2M\) One filter per channel. Combine information from channels. 1 x 1 convolution: \(M\) a total \(D_K^2MD_F^2+MND_F^2\) speed up \(D_K^2N/(D_K^2+N)\) Empirically, 9 times less work with the same accuracy. RNN Hidden layers and output depend from previous states of the hidden layers and the current inputs. The same weights are used for different instances of the artificial neurons at different time stamps. image-20191029162522774 Problems: - vanishing gradients gradient signal get so small that learning stops. - exploding gradients gradients signal is so large that it can cause learning to diverge. LSTM Long Short-Term Memory image-20191029162412552 image-20191029183428636 \(\odot\) stands for element wise multifaction forget gate \(f_t=\sigma(W_f[h_{t-1},x_t]+b_f)\) input gate \(i_t=\sigma(W_i[h_{t-1},x_t]+b_i)\) Internal State\(\bar C_t=tanh(W_C[h_{t-1},x_t]+b_C)\quad C_t= f_t\odot C_{t-1}+i_t\odot\bar C_t\) Output gate \(o_t=\sigma(W_o[h_{t-1},x_t]+b_o)\quad h_t=o_t\odot tanh(C_t)\) Reinforcement Learning Learning Paradigms Supervised earning the learner is provided with a set of inputs together with the corresponding desired outputs. Unsupervised learning training examples as inputs patterns, with no associated output patterns Reinforcement learning Only input and evaluative out are given Concept Goal States \(s_i\) Actions \(a_i\) Rewards \(r_i\) Markov Assumption: \(s_{t+1}\) and \(r_t\) depend only on current state and action Non-Deterministic: Action may have uncertain outcomes. \(P(s,s&#39;,a)\) probability of transition from \(s\) to \(s&#39;\) given action \(a\) \(R(s,s&#39;,a)\) expected reward on transition \(s\) to \(s&#39;\) given action \(a\) Discounted Rewards: A reward in the future is not worth quite as much as a reward now. Introduce a discount factor for future reward discounted rewards = \(r_t+\gamma r_{t+1}+\gamma^2 r_{t+2}+...\) where \(0\le\gamma\le1\). shortsighted \(0\leftarrow\gamma\to1\) farsighted Policy Policy is denoted by \(\pi\) Learn action policy \(\pi\) that maximizes the expected future reward \(V^\pi(s)\equiv E[r_t+\gamma r_{t=1}+\gamma^2r_{t+2}+...]\) Deterministic \(V^\pi(s)\equiv E[r_t+\gamma r_{t=1}+\gamma^2r_{t+2}+...] = r_t+\gamma r_{t=1}+\gamma^2r_{t+2}+...\) At state \(s\), take action \(a\) immediate reward \(r(s,a)\) value of the immediate successor state \(V^\pi(\delta(s,a))\) \(V^\pi(s) = r(s,a) +\gamma V^\pi(\delta(s,a))\) (Bellman equation) Nondeterministic At state \(s\), take action \(a\) with probability \(\pi(s,a)\) from \(s\), take action \(a\), probability of transition to \(s&#39;:\ P(s,s&#39;,s)\) expected reward on transition given action \(a:\ R(s,s&#39;,a))\) \(V^\pi(s) = \sum_a\pi(s,a)\sum_{s&#39;}P(s,s&#39;,a)[R(s,s&#39;,a)+\gamma V^\pi(s&#39;)]\) Iterative Evaluation Instead of solving the linear system, we can also use an iterative method! Initialize \(V_0\) \(V_0\to V_1\to V_2\to ...V_k\to V_{k+1}...\to V^\pi\) Update Rule: \(V_{k+1}(s) = \sum_a\pi(s,a)\sum_{s&#39;}P(s,s&#39;,a)[R(s,s&#39;,a)+\gamma V_k(s&#39;)]\)]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>ML</tag>
        <tag>CNN</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHYS1003]]></title>
    <url>%2F2019%2F10%2F09%2FPHYS1003%2F</url>
    <content type="text"><![CDATA[Energy Energy Consumption and Population Growth I = P x A x T Human Impact = Population x Affluence x Technology 7.2 billion people now. 140 million born/year, 57 million die/year. 108 billion estimated to have ever lived. doubling time 2 = e^0.7 Fossil fuels 1 manpower = 75W 1kWh = 3.6 x 1e6 J 1 hp = 746 J 1 toe = 42 x 1e9 J 1 Cal = 4.19 x 1e3 J 1BTU = 1055 J Thermodynamics Temperature and heat Kelvin = Celsius + 273.15 Fahrenheit = 9/5 Celsius + 32 Internal Energy = Kinetic Energy(KE) + Potential Energy(PE) For gas, PE is almost zero. The internal energy of gas is the sum of kinetic energy of all molecules. \({1\over2}mv_{avg}^2={1\over2}jKT\) where \(j\) is the degree of freedom of motion. For ideal gas \(pV=nRT=NkT\) where \(R=8.315J/mol/K\) and \(k=1.38\times10^{-23}J/K\) and \(N_A=6.02\times10^{23}\) Laws of thermodynamics The first law of thermodynamics \(\Delta U=Q-W\) change in internal energy = Heat added - work done 2nd law: Isolated systems naturally move towards configurations of increasing probability. Ideal Gas Work done in isothermal process \(W=nRT\ ln(V_f/V_i)\) Work done in adiabatic expansion \(W = \int PdV = [1/(\gamma-1)](P_iV_i-P_fV_f)\) where \(\gamma=C_p/C_v\) Thermodynamic processes in a cycle \(a\to b\) P is constant \(\ W=P\Delta V,\Delta U=Q-W\) \(b\to c\) T is constant \(\ W=Q,\Delta U=0\) \(c\to d\) V is constant \(\ W=0,\Delta U=-W\) \(d\to a\) adiabatic compression \(Q=0, \Delta U=-W\) For an ideal gas \(\Delta U=nC_V\Delta T\) , where \(C_V\) is the molar specific heat capacity. At constant volume \(C_V=(j/2)R\). At constant pressure \(C_P=C_V+R\) Heat Engine Heat input to the engine \(Q_h\) equals to heat output from the engine \(Q_c\) plus the work done \(W\) \(Q_h=Q_c+W\) The thermal efficiency \(e={W\over Q_h}=1-{Q_c\over Q_h}\) Heat Pump Heat input to the engine \(Q_c\) plus the work done \(W\) equals to heat output from the engine \(Q_h\) \(Q_c+W=Q_h\) The effectiveness is described by COP, the ratio of useful heat movement per work input. Cooling mode \(COP={Q_c\over W}\le {T_c\over T_h-T_c}\) Heating mode \(COP={Q_h\over W}\le {T_h\over T_h-T_c}\) COP becomes most “efficient” for small temperature differences Carnot Engine If operates in an ideal, reversible Carnot cycle between two reservoirs and is the most efficient engine possible. 1571069346699 Isothermal expansion \(A\to B\) The gas absorbs \(Q_h\) from high temperature reservoir \(T_h\) and do work \(W_{AB}\) Adiabatic expansion \(B\to C\) Temperature drops from \(T_h\) to \(T_c\). The gas does work \(W_{BC}\gt 0\) Isothermal compression \(C\to D\) The gas expels \(Q_c\) to low temperature reservoir \(T_c\) and work \(W_{CD}\)is done on gas Adiabatic expansion \(D\to A\) Temperature rises from \(T_c\) to \(T_h\). The gas does work \(W_{DA}\lt 0\) The net work \(W_{eng}=|Q_h|-|Q_c|\). The efficiency \(e_C=1-{T_c\over T_h}\) Entropy \(\Delta S=\Delta Q_{rev}/T \to S = \int dQ_{rev}/T\) The effect of all naturally occurring processes is always to increase the total entropy of the universe. \(e_{any\ engine}\le 1-{T_c\over T_h}=e_{Carnot}\) Transport Engine Otto Gasoline Engine 20191015_030857946_iOS The efficiency of the Otto cycle is \(e_0=1-{1\over (V_1/V_2)^{\gamma-1}}\). Generally about 20% to 30% Energy dissipation = Energy for brake + Air drag \({1\over 2}v^3(m_c/d+\rho A)\) where \(A\) is the cross-sectional area of the car. Diesel Engine Diesel Engine Jet Engine Power required to overcome drag \(P_{drag}={1\over 2}\rho c_dA_pV^3\) where \(A_p\) is the frontal area of the plane. Power required to lift \(P_{lift}={(mg)^2\over 2\rho vA_s}\) where \(A_s\) is the cross-sectional area of the air cylinder. Total Power \(P_{total}=P_{drag}+P_{lift}\) Thrust can be optimized \(F={P\over v}={1\over 2}\rho c_dA_pV^2+{(mg)^2\over 2\rho v^2A_s}\ge c_d\rho A_pv_{opt}^2\) where \(\rho v_{opt}^2={mg\over\sqrt c_dA_pA_s}\) so that \(F_{opt}=\sqrt{c_df_A}mg\) where the \(f_A=A_p/A_s\) is the filling factor Transport cost = \({1\over\epsilon}g\sqrt{c_df_a}\quad(N/kg)\) Transport efficiency = \(N_{passengers}\times\epsilon\times E/V \over thrust\)(passenger-km per litre) Range = \({energy\times \epsilon\over force}\)]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Database]]></title>
    <url>%2F2019%2F07%2F07%2FDatabase%2F</url>
    <content type="text"><![CDATA[ER Model Keys If the values of some attributes can uniquely identify an entity instance then these attributes are said to be a key of the entity. An entity may have more than one key. A candidate key is a minimal set of attributes (i.e., all attributes are needed) that uniquely identifies an entity instance. One candidate key is selected to be the primary key. A candidate/primary key can be composed of a set of attributes =&gt; composite key. Strong and Weak Entity Strong entity: An entity that has a primary key Weak entity: An entity that does not have a primary key. A weak entity must be associated with a strong entity, called the identifying entity, to be meaningful. The relationship associating the weak entity to the strong entity is called the identifying relationship. A discriminator, if present, uniquely identifies a weak entity instance within its identifying relationship. A surrogate key is a new attribute introduced into an entity to be the primary key of the entity. It may be useful to make weak entities strong or to replace a strong entity's key, if it consists of many attributes. (usually sequentially assigned numbers.) Relation Model and Algebra Reduce from ER Model Generalization, two options: Reduce all entities to relation schema Add: the primary key, \(K\), of the superclass a foreign key constraint, reference superclass a referential integrity action: on delete cascade Reduce only subclass entities to relation schema. (Only for total, disjoint generalizations) Add: all the attributes of the super class entity. Composite attributes, two options: Create a single attribute and pack components in it. Create a separate attribute for each component. Multivalued attribute \(M\) , entity \(S\): Create a relation schema \(SM(K_S,A)\) where \(K_S\) is the primary key of \(S\), \(A\) is the \(M\). The primary key of relation \(SM\) is the union of all its attributes. Add: a foreign key constraint: foreign key \((FK_S)\) reference \(S(K_S)\) a referential integrity action: on delete cascade Strong entity: Create a relation with all the attributes. Weak entity \(T\) that depends on strong entity \(S\): Create a relation schema \(R_T\) with attributes of \(T\). Include attributes \(A_R\) of relationship \(R\) in \(R_T\) Include as foreign key attributes \(FK_S\) in relation \(R_T\) The primary key of relation \(R_T\) is the union of the foreign key attributes \(FK_S\) and the discriminator \(d_A\) of the weak entity \(T\). Add: a foreign key constraint: foreign key \((FK_S)\) references \(S(K_S)\) a referential integrity action: on delete cascade Relation: Create a new relation schema \(R_R\) Include as foreign key attributes in relation \(R_R\) the primary key of the entities related by relationship \(R\). Include attributes \(A_R\) of relationship \(R\) For 1:1 relationship, combine the attributes in the relation to the relation schema of either entity. For 1:N relationship, combine the attributes in the relation to the relation schema of either on the N-side. For M:N, the relation schema of relationship cannot be combined to relation schema of entity. Add: a foreign key constraint for the foreign key. a referential integrity action: optional=on delete set null; total=on delete cascade; Relation Algebra Selection \(\sigma_{Condition}(Relation)\) Projection \(\pi_{List}(Relation)\) result no duplicated. Set operations \(\cup\) union \(-\) set difference \(\cap\) intersection \(\times\) Cartesian product Join \(\Join_{Condition}\) join two relation with certain condition. left/right/full outer join can create table with value null SQL 123select A1, A2, ..., Anfrom R1, R2, ..., Rmwhere P; Ai are attributes. Ri are relations. P is a predicate (condition). Return a relation(may contain duplicates). select clause = Projection \(\pi L(R)\) from clause = Cartesian product \(A\times B\) where clause = Selection \(\sigma C(R)\), natural join \(\Join_N\) Nested Subqueries 123select *from Rwhere a &gt; (select avg(a) from R) Aggregate Functions 1234select name, count(name)from Rgroup by name, numberhaving number&gt;100; An attribute in the select clause must also appear in the group by clause. An attribute in the group by clause do not need to appear in the select clause. Any attribute present in the having clause that is not being aggregated must appear in the group by clause. Temporary Subqueries 12345select name, avgAmountfrom (select name, avg(amount) as avgAmount from R group by name)resultwhere avgAmount = (select max(amount) from result); In Oracle, it is expressed by the form 1234567with result(name, avg(amount) as select name, avg(amount) from R group by name)select name, avgAmountfrom resultwhere avgAmount = (select max(amount) from result); Manipulation Create Relations create table T(a type); Alter and Destroy Relations add attributes alter table T add b type; drop attributes alter table T drop column b; drop table drop table T; Integrity Constraints(IC) not null primary key unique foreign key...references... check Create views to hide certain data from certain users Delete tuples delete from R where name='want' Update update R set a = 0 Function Dependency superkey: any set of attributes that determines the entire tuple. candidate key: any minimal set of attributes that determines the entire tuple. primary key: if there are multiple candidate keys, the database designer choose one of them as the primary key. The set of all functional dependencies logically implied by \(F\) is called the closure of \(F\) denoted as \(F^+\) Lossless Decomposition A decomposition of \(R\) into \(R_1\) and \(R_2\) is lossless if and only if \(R_1 \cap R_2 \to R_1\) or \(R_1 \cap R_2 \to R_2\) Preserve Functional dependencies The decomposition is dependency preserving if and only if $(F_i )^+ = F^+ $. prime and non-prime attributes: An attribute is a prime attribute if it is part of any candidate key. Canonical Cover Algorithm \(F_c\) = \(F\) Repeat: Use the union rule to replace any FDs in \(F_c\) of the form \(X\to Y\) and \(X\to Z\) with \(X\to YZ\) Find an FD \(X\to Y\) in \(F_c\) with an extraneous attribute either in \(X\) or in \(Y\) Until \(F_c\) does not change 1NF A relation schema is in First Normal Form (1NF) if all attributes are atomic (single-valued). There are no multi-valued or composite attributes Relation schema are always in 1NF. 2NF A relation schema is in Second Normal Form (2NF) if all non-prime attributes are fully functionally dependent on every candidate key. Relation \(R\) is 2NF if and only if For each FD: \(X\to A\) in \(F^+\): \(A\in X\) (trivial) or \(X\) is not a proper subset of a candidate key for \(R\) or A is a prime attribute for \(R\) 3NF A relation schema R is in Third Normal Form (3NF) if it is in 2NF and every non-prime attribute of R is non-transitively dependent on every candidate key of R. Relation \(R\) is 3NF if and only if: For each FD:\(X\to A\) in \(F^+\): \(A\in X\) (trivial) or \(X\) is a superkey for \(R\) or \(A\) is a prime attribute for \(R\) Algorithm Let \(R\) be the initial relation schema with FDs \(F\) Compute the canonical cover \(F_c\) of \(F\). \(S = \phi\) For each FS \(X\to Y\) in the \(F_c\): \(S=S\cup (X,Y)\) If no schema contains a candidate key for \(R\), choose any candidate key \(K\) , \(S =S\cup K\) The algorithm always creates a lossless-join, dependency preserving, 3NF decomposition. BCNF A relation schema is in Boyce-Codd Normal Form (BCNF) if every determinant (left hand side) of its FDs is a superkey. Relation \(R\) is BCNF if and only if: For each FD:\(X\to A\) in \(F^+\): \(A\in X\) (trivial) or \(X\) is a superkey for \(R\) There is always a lossless decomposition that generates BCNF relation schema. However, all the functional dependencies may not be preserved. Algorithm Let \(R\) be the initial relation schema with set of FDs \(F\). Compute \(F^+\) \(S =R\) Until all relation schemas in S are in BCNF For each \(R\) in \(S\) For each FD \(X\to Y\) that violates BCNF for \(R\) $S = (S-R)(R-Y)(X,Y) End until Test if a FD violates BCNF EITHER test \(R_i\) for BCNF with respect to the restriction of \(F^+\) to R i (i.e., all FDs in \(F^+\) that contain only attributes from \(R_i\) ). OR use the following test. For every set of attributes \(X \subseteq R_i\), check that the attribute closure \(X^+\) either includes no attribute of \(R_i-X\) or includes all attributes of \(R_i\). If the condition is violated, the dependency \(X\to(X^+ -X) \cap R_i\) holds on \(R_i\) , and \(R_i\) violates BCNF. We use the BCNF-violating dependency to decompose \(R_i\) . Storage and File Structure seek time: Time to move the arms to position the disk head on a track. (4 to 15 ms) rotational latency: Time to wait for the page (sector) to rotate under the head. (2 to 7 ms) transfer time: Time to actually move data to/from the disk surface. (1ms/4KB) seek time and rotational delay dominate. File blocking Factor \(bf_r\) The number if records that fit in a page and is equal to \(\lfloor\) # bytes per page / # bytes per record \(\rfloor\) The number of pages needed to store a file is equal to \(\lceil\)# records / \(bf_r\rceil\) Index To reduce the cost to find a record, we build index page on certain attribute of the records. The attribute is called the search key of the index. Ordered Index An index page is also called an index node. fan-out is the number of children of an index node The height of the tree is \(\lceil log_{fan-out}(\# index\ entries)\rceil\) Clustering and Non-clustering Primary Index (Clustering Index) An index for which the file is sorted on the search key of the index. Secondary Index (Non-clustering Index) An index for which the file is not sorted on the search key of the index Sparse vs Dense Sparse Index contains an index entry for only some search key values (applicable only to primary/clustering indexes) Dense Index contains an index entry for every search key values (often secondary/non-clustering indexes) To build an index on an attribute that may be the same for several records. It is not a problem if the index is primary and sparse. When secondary and dense. we have 3 way: use variable length index entries. &lt;name,ptr1,ptr2&gt; Use multiple index entries per name.&lt;name,ptr1&gt; &lt;name,ptr2&gt; Use an extra level of indirection(most common) an index entry points to a list/bucket that contains the pointers to all the actual records with that name. B+ tree Index Automatically reorganizes itself to keep balance. Balanced tree All path from the root node to the leaf nodes are the same length Fan-out The maximum number of pointers/children in each node, denote n. B+ tree order The value \(\lceil(n-1)/2\rceil\) corresponds to the minimum number of values in a leaf node. Non-leaf nodes form a multi-level, sparse index on the leaf nodes Insertion If the leaf node \(L\) has enough space, done. Else, split the leaf node. Place first \(\lceil n/2\rceil\) values in \(L\), copy up values at \(\lceil n/2\rceil +1\) and place values left in \(L&#39;\) copy up: insert an index entry(value at \(\lceil n/2\rceil +1\)), pointer to \(L&#39;\)) into the parent of \(L\) Splits can happen recursively. To split an internal index node $N $ Place first \(\lceil n/2\rceil -1\) values in \(N\), push up value at \(\lceil n/2\rceil\) and place values left in \(N&#39;\) Push up: Insert an index entry(value at \(\lceil n/2\rceil\)) pointer t o\(N&#39;\)) into the the parent of \(R\) Deletion If the leaf node \(L\) has at least $n(-1)/2$ values, done. Else, try to re-distribute by borrowing values from a sibling node(adjacent node right or left) If re-distribution fails, merge \(L\) and its sibling. Hash index and Bitmap Index Hash indexes are always secondary indexes. Bitmap indexes are a special type of index designed for efficient querying on multiple search keys A bitmap is simply an array of bits. Query Processing Selection \(B_r\) the number of pages that contain records of relation \(r\) \(HT\) the height of the tree index File Scan linear search cost: \(B_r\) or \(B_r/2\) if selection is on a key attribute. It stops once we find one. binary search selection is equality comparison on the attribute on which the file is ordered cost: \(\lceil log_2(B_r)\rceil\) Equality Search primary index on key retrieve a single record that satisfies the equality condition cost: \(HT_i +1\) primary index on non-key retrieve multiple records that satisfy the equality condition cost: \(HT_i+\#\ pages\ contain\ records\ satisfy\ the\ condition\) secondary index retrieve a single record if the search key is a candidate key cost: \(HT_i +1\) for tree index \(1+1\) for hash index Comparisons retrieve record with condition \(A\ge V\) or \(A\le V\) by using linear file scan, binary search or indexes. primary index \(A\ge V\) use the index to find the first one, and scan from there \(A\le V\) do not use the index. Scan from the top till the first one \(A&gt;V\) secondary index \(A\le V\) scan the leaf pages of the index until the first entry \(A&gt;V\) For conjunction(AND) \(\sigma_{\theta1\and\theta2\and...\and\theta n}(r)\) using one index select a combination of \(\theta_i\) and all algorithms above that results in the least cost using composite index If available, use an appropriate composite index by intersection of record pointers If any attributes have indexes with record pointers then use them and take intersection to get the set of pointers. Then check other condition in memory. cost: \(\sum\ costs\ of\ individual\ index\ scans + cost\ of\ retrieving\ the\ tuples\) For disjunction(OR) \(\sigma_{\theta1\or\theta2\or...\or\theta n}(r)\) by union of record pointers If all attributes have indexes with record pointers then use them and take union of them cost: \(\sum costs\ of\ individual\ index\ scans+cost\ of\ retrieving\ the\ tuples\) Sorting Let \(M\) denote the memory size. N-way merge (assume\(N&lt;M\)): Use \(N\) pages of memory to buffer input, and 1 page to buffer output. If the number of input pages is greater than the memory size, several passes are needed. Number of passes: \(\lceil log_{M-1}(B_r/M)\rceil+1\) passes I/O cost: \(2*B_r*(\lceil log_{M-1}(B_r/M)\rceil+1)\) pages Join \(r, s\) the relations to be joined \(n_r,n_s\) the number of tuples( records) in \(r\) and \(s\), respectively. \(B_r,B_s\) the number of pages in \(r\) and \(s\), respectively \(M\) the available pages of memory. nested-loop join requires no indexes and can be used in any condition Worst case \(n_r*B_s+B_r\) only 1 memory page available for each relation Best case \(B_r+B_s\) block nested-loop join requires no indexes and can be used in any condition Worst case \(B_r*B_s+B_r\) only 1 memory page available for each relation Best case \(B_r+B_s\) With Optimizations \(\lceil B_r/(M-2)\rceil * B_s+B_r\) Use \(M-2\) page as the blocking unit of the outer relation. indexed nested-loop join index lookups can replace file scans if the joint is equi-join or natural join and an index is available on the inner relations joint attribute. cost \(B_r+n_r*c\) where c is the cost of traversing the index and fetching all matching \(s\) tuples for one tuple of \(r\) merge-join Sort both relations on their join attribute an merge the sorted relations to joint them. cost \(B_r+B_s+cost\ of \ sorting\) the cost of sorting is 0 if the relations are sorted. hash-join Applicable for equi-joins and natural joins A hash function is used to partition tuples of both relations into \(n\) buckets. cost \(3*(B_r+B_s)\) 1 read and 1 write to create partitions/buckets, 1 read to compute the join. complex joins Projection select distinct boatId from Reserves sorting (standard) Modify Pass 0 to eliminate unwanted attributes. Write &lt; Read Modify merge passes to eliminate duplicates. Write &lt; Read hashing Discard unwanted attributes and apply the hash function \(h1\) to hash to \(M-1\) partitions. Write &lt; Read Read each partition and build in-memory hash table using hash function \(h2\) discard duplicates Set Operations sorting sort both \(r\) and \(s\) on the same attribute eliminating duplicates when merge intersection: only if it is in both relations Union: only once if it belongs to both relations Set difference: only if its is in the first relation but not in the second one hashing partition relation \(r\) and \(s\) using hashing function \(h1\) on all attributes For each s-partition, build an in-memory hash table using \(h2\) on all attributes while discarding duplicates. Then scan the r-partition, for each tuple of r intersection: output \(t_r\)only if it is also in \(s\) union: output \(t_r\) if it is not in \(s\). also output all tuples in \(s\) set difference: output \(t_r\) only if it is not in \(s\) Relation Algebra Tree Evaluation A relational algebra (operator) tree represents a relational algebra expression (an SQL query) as a tree. The Evaluation is bottom up. materialization Evaluate and generate the results of one operation at a time. Actually store (materialize) the result on disk for subsequent use. Overall Cost: sum of costs of individual operations + cost of writing intermediate results to disk pipelining Evaluate several operations simultaneously, passing tuples to the next operation as they are generated. No need to store temporary results. Much cheaper than materialization since there is no need to store temporary relations to disk. Query Optimization Query optimization is the process of selecting the most efficient query-evaluation plan from among many strategies. Cost Based Optimization Generate logically equivalent evaluation plans. Estimate the cost of each plan. Execute the plan with the minimum expected cost. Heuristic Optimization Perform the cheap operations first. Try to utilize existing indexes. Remove unneeded attributes early. Transformation of Relation Expressions Two relational algebra expressions are equivalent if they generate the same set of tuples on every legal database instance. Cascading of projections: \(\pi_{a1}(R)=\pi_{a1}(\pi_{a2}...(\pi_{an}(R))...)\) Cascading of selections: \(\sigma_{c1\and c2\and ...\and cn}(R)=\sigma_{c1}(\sigma_{c2}...(\sigma_{cn}(R))...)\) Commutativity of selections: \(\sigma_{c1}(\sigma_{c2}(R))=\sigma_{c2}(\sigma_{c1}(R))\) Commutativity of joins/Cartesian products: \(R\Join S=S\Join R\) Associativity of joins/Cartesian products: \((R\Join S)\Join T=R\Join (S\Join T)\) Commutativity of selections with projections: \(\pi_a(\sigma_c(R))=\sigma_c(\pi_a(R))\) Commutativity of selections with joins/Cartesian products: \(\sigma_c(R \Join S)=(\sigma_cR) \Join S\) Projection Distributes Over Join: \(\pi_a(R\Join S)=(\pi_a(\pi_{a1}R)\Join (\pi_{a2}S))\) Join Order: A good ordering of join operations is important for reducing the size of intermediate results. Usually we use dynamic programming to find out the best plan with minimum cost. Interesting Sort Order: a particular sort order of tuples that could be useful for a later operation. Estimating Statistics of Express Result The DBMS system catalog stores the following statistics for each relation \(r\): \(n_r\) the number of tuples in \(r\) \(B_r\) the number of pages containing tuples of \(r\) \(I_r\) the size of a tuple of \(r\) in bytes \(bf_r\) the blocking factor of \(r\) \(V(A,r)\) the number of distinct values that appear in \(r\) for attribute \(A\). For indexes the system catalog stores the following information: \(HT_i\) the number of levels in the index \(i\) \(LB_i\) the number of pages at the leaf level of the index Selection Cardinality \(SC(\theta,r)\) the selection cardinality of predicate \(\theta\) for relation \(r\), is the average number of tuples that satisfy the predicate \(\theta\) Selectivity \(Selectivity(\theta,r)=SC(\theta,r)/n_r\) the fraction of tuples that satisfy \(\theta\) . It is between 0 and 1. Size Estimation Estimates may be quite inaccurate but provide upper bounds on the sizes. Join If \(r\cap s=A\) is a key for \(r\) The number of tuples is no greater than \(n_s\) If \(r\cap s=A\) is a (not null) foreign key for \(s\) referencing \(r\) The number of tuples is exactly \(n_s\) If \(r\cap s=A\) is not a key for \(r\) or \(s\) The number of tuples is the lower of \(n_r*n_s\over V(A,s)\) and \(n_r*n_s\over V(A,r)\) Projection: \(V(A,r)\) Aggregation: \(V(A,r)\) Set operations: Union \(r\cup s \to n_r + n_s\) intersection \(r\cap s \to min(n_r , n_s)\) set difference \(r-s\to r\) Transactions A transaction is a unit of program execution that accesses and possibly updates the database. ACID properties: Atomicity Either all work or none work Consistency Execution of a transaction in isolation preserves the consistency of the database. Isolation Concurrently executing transactions must be unaware of other concurrently executing transactions. Intermediate transaction results must be hidden from other concurrently executing transactions Durability After a transaction completes successfully, the changes it made to the database persist, even if there are system failures. Transaction State Active Partial committed Failed Aborted Committed Schedules A schedule is a sequence that indicates the chronological order in which instructions of concurrent transactions are executed. Schedules must be serializable, and recoverable, for the sake of database consistency, and preferably, cascadeless. Serial and Serializability Serial The transactions are executed one after the other. A serial schedule preserves database consistency. Serializable A (possibly concurrent) schedule is serializable if it is equivalent to a serial schedule. Conflict Instruction \(I_i\) and \(I_j\) of transaction \(T_i\) and \(T_j\) conflict if and only if at least one of them writes. Conflict Equivalent A schedule \(S\) that can be transformed into a schedule \(Sâ€²\) by a series of swaps of non-conflicting instructions. Conflict Serializable A schedule S that is conflict equivalent to a serial schedule. Recoverability A schedule is recoverable if a transaction, \(T_j\) , that reads a data item previously written by a transaction \(T_i\), commits after \(T_i\). Cascading Rollback when a single transaction failure leads to a series of transaction rollbacks. Cascadeless Schedules Schedules where cascading rollback cannot occur. A schedule is cascadeless if, for each pair of transactions \(T_i\) and \(T_j\) where \(T_j\) reads a data item previously written by \(T_i\), the read operation of \(T_j\) appears before the commit operation of \(T_i\). Every cascadeless schedule is also recoverable. Concurrency Control Lock-based Protocols Lock a mechanism to control concurrent access to a data item. A data item Q can be locked in one of two modes: shared-mode(shared lock) can only read Q. Any number of Transactions can hold lock-s. exclusive-mode (exclusive lock) can both read and write Q. Only one transaction can hold lock-x. A transaction must make a lock request to the concurrency-control manager before accessing a data item. A transaction can proceed only after a request is granted The concurrency control manager should allow only conflict-serializable schedules. starvation an lock -x waits forever because a sequence of lock-s are reading the data Two-phase locking (2PL) protocol Phase 1: Growing Phase A transaction may obtain or upgrade locks, but may not release any locks. Phase 2: Shrinking Phase A transaction may release or downgrade locks, but may not obtain any new locks Only part of the conflict serializable schedule can be executed by 2PL Strict 2PL all lock-x held until a transaction commits. Rigorous 2PL all locks held until a transaction commits. Strict and rigorous 2PL schedules are cascadeless. Deadlock Every transaction is waiting for another transaction. To handle a deadlock, some transactions must be rolled back and its locks released. Deadlock prevention Order lock requests Preemption and/or rollback wait-die scheme (non-preemptive): older wait, younger die wound-wait scheme (preemptive): older kill, younger waits time-based scheme: wait a pre-defined time and roll back Deadlock detection A wait-for graph is the opposite direction of precedence graph. If there is a cycle in the graph, a deadlock will happen. Deadlock recovery select a transaction with the minimum cost as the victim to rollback total or partial. Tree Protocol Only lock-x instructions are allowed. The first lock by \(T_i\) may be on any data item. Subsequently, a data item Q can be locked by \(T_i\) only if the parent of Q is currently locked by \(T_i\). Data items may be unlocked at anytime. A data item that has been unlocked by \(T_i\) cannot be locked again by \(T_i\). All legal schedules under the tree protocol are conflict serializable. Timestamp-Based Protocols The timestamps determine the serializability order. Two timestamps values are associated with each data item Q: write timestamp WTS(Q) and read timestamp RTS(Q) Read If $TS(T_i) &lt; WTS(Q) $ rollback If \(TS(T_i) ≥ WTS(Q)\) \(RTS(Q) = max(TS(T_i), RTS(Q))\) Write If \(TS(T_i) &lt; RTS(Q)\) rollback If \(TS(T_i) &lt; WTS(Q)\) rollback/ignore Otherwise $WTS(Q) = TS(T_i) $ Validation-based Protocols Most transactions are read-only. The system maintains three timestamps for each transaction: start(\(T_i\)) validation\((T_i\)) finish(\(T_i\)) For all transactions \(T_k\) with \(TS(T_k)&lt;TS(T_i)\) one of the following must hold: finish(\(T_k\)) &lt; start(\(T_i\)) data items read(\(T_k\)) \(\cap\) data items read(\(T_i\)) = \(\phi\) and start(\(T_i\)) &lt; finish(\(T_k\)) &lt; validation(\(T_i\)) Multi-version Timestamp Ordering Read Always succeed. set \(RTS(Q)=TS(T_i)\) Write If \(TS(T_i) &lt; RTS(Q)\) rollback If \(TS(T_i) = WTS(Q)\) overwrite contents If \(TS(T_i) &gt; WTS(Q)\) create new version. set \(R/WTS(Q′)=TS(T_i)\) Snapshot Isolation Each transaction works on its own private copy (snapshot) of the data items it reads and writes. First committer wins First updater wins Snapshot isolation does not ensure serializability! Recovery System log-based recovery A log is a sequence of log records that maintains a record of all the update activities on the database. A log kept on the stable storage. may use log record buffer to output from memory to disk when the buffer is full. deferred database modification All modifications are recorded to the log, but all the writes are deferred until after partial commit. The old value is not needed in this scheme. immediate database modification Database updates of an uncommitted transaction are allowed to be made as the writes are issued. Since undoing may be needed, update logs must have both the old value and the new value. checkpoints redo transactions commit before failure undo transactions not yet commit shadow paging Maintain two page tables during the lifetime of a transaction, The current page table is used for database item accesses during execution of the transaction. The shadow page table is stored in nonvolatile storage for recovery.]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differential Equation]]></title>
    <url>%2F2019%2F04%2F10%2FDifferential-Equation%2F</url>
    <content type="text"><![CDATA[First Order Linear first order linear equation standard form: \[{dy\over dt}+p(t)y=g(t)\] In some cases it is possible to solve a first order linear equation immediately by integrating the equation. Unfortunately, most of them should use a method called integrating factor. \[\mu(t){dy\over dt}+\mu(t)p(t)y=\mu(t)g(t)\] where \({d\mu\over dt}=p(t)\mu(t)\) so that \(\mu(t)=ce^{G(t)}\) where \(G(t) = \int g(t)\) Separable \[M(x)dx+N(y)dy=0\] We can directly integral both sides to get the solution. Second Order Linear \[{d^2y\over dt^2}=f(t,y,{dy\over dt})\] Linear if \(f(t,y,{dy\over dt})=g(t)-p(t){dy\over dt} -q(t)y\) Homogenous Equation When \(g(t)=0\) the second order linear equation is said to be homogenous \[y&#39;&#39;+p(t)y&#39;+q(t)y=0\] Constant Coefficients A second order linear homogenous equation with constant \[ay’’+by’+cy=0\] has characteristic equation \[ar^2+br+c=0\] The roots \(r_1\) and \(r_2\) correspond to \(y_1=e^{r_1}\) and \(y_2=e^{r_2}\) Any linear combination of \(y_1\) and \(y_2\) is a solution. The distribution of the roots have three conditions 1. Two distinct real roots. Just exponential function 2. Two distinct complex roots. By, Euler’s formula, exponential of complex number is Sinusoids. - Its real part \(u\) and its imaginary part \(v\) are also solutions of it. We need to express \(y=c_1u+c_2v\) 3. Two identical real roots\(r_0\). Then the solution is \(y_1=e^{r_0t}\quad y_t=te^{r_0t}\). We can get this result by assume \(y_2=v(t)y_1\) and finally we can find \(v(t)=c_1+c_2t\) Wronskian When given a initial point \(y_0\), we have the equations: \[c_1y_1(t_0)+c_2y_2(t_0)=y_0\\ c_1y_1’(t_0)+c_2y_2’(t_0)=y_0’\] The Wronskian determinant is defined as \[W=\begin{vmatrix}y_1&amp;y_2\\y_1’&amp;y_2’\end{vmatrix} = y_1y_2’+y_1’y_2\] It is a function of independent variable t. Thus, different t correspond to different W. If at a point \(t=t_0\quad W(t_0)\ne0\) the solution is unique. The constants are \[c_1={\begin{vmatrix}y_0&amp;y_2(t_0)\\y_0’&amp;y_2’(t_0)\end{vmatrix} \over W(t_0)} \quad c_2={-\begin{vmatrix}y_0&amp;y_1(t_0)\\y_0’&amp;y_1’(t_0)\end{vmatrix}\over W(t_0)}\] For the equation \(y_0=c_1y_1+c_2y_2\) If at a point \(t=t_0\quad W(t_0)=0\) the initial conditions cannot be satisfied not matter how \(c_1\) and \(c_2\) are chosen. \(y=c_1y_1(t)+c_2y_2(t)\) is called the general solution.\(y_1\) and \(y_2\) are said to form a fundamental set of solutions if Wronskian is nonzero. Abel Theorem If \(y_1\) and \(y_2\) are solutions of \[L[y]=y’’+p(t)y’+q(t)y=0\] Where p and q are continuous on an open interval \(I\), then the Wronskian is given by \[W(y_1,y_2)(t)=c e^{-\int p(t)dt}\] Therefore, \(W(y_1,y_2)(t)\) is either zero or else is never zero for all t in the interval. Nonhomogeneous Equations A nonhomogenous equation is \[ L[y]=y’’+p(t)y’+q(t)y=g(t)\ne 0\] We can solve it by thee steps: 1. Find the general solution \(c_1y_1(t) +c_2y_2(t)\) of the corresponding homogenous equation. 2. Find some single solution \(Y(t)\) of the nonhomogenous equation. Often this solution is referred to as a particular solution. 3. Form the sum of the functions found in steps 1 and 2. Method of Undetermined Coefficients Make an initial assumption about the form of the particular solution \(Y(t)\). But with the coefficients left unspecified. e.g. for \(y&#39;&#39;-3y&#39;-4y=3e^{2t}\) we guess that \(Y(t)=Ae^{2t}\) and finally obtain \(A=-{1\over 2}\) ==VERY IMPORTANT== If one of the terms of our first guess \(Y(t)\) is included in the general solution to the corresponding homogeneous equation, try \(tY,t^2Y,...\) and so on. Series Solutions of Second Order Linear Equation We now consider the second order linear equations when the coefficients are functions of the independent variable. \[P(x){d^2y \over dx^2 }+ Q(x){dy\over dx} +R(x)y =0\] A point \(x_0\) where \(Q(x_0)\over P(x_0)\) is analytic is called an ordinary point, otherwise is called singular point. Solutions near ordinary point set \(y=\sum_{n=0}^\infty a_nx^n\) Then we have the kth derivative \(y^{(k)}=\sum_{n=k}^\infty n(n-1)...(n-k+1)a_nx^{n-k}\) substitute the series for the original function to get the recurrence relation of the coefficients \(a_n\) Usually we need to shift the index of summation by replacing \(n\) by \(n-sth\) The two initial items are the arbitrary coefficients \[y=a_0y_1+a_1y_2=a_0\sum...+a_1\sum...\] where \(y_1,y_2\)are two power series solutions that are analytic at \(x_0\). The radius of convergence for each of the series solutions \(y_1\) and \(y_2\) is at least as the minimum of the radii of convergence of the series for \(Q\over P\) and \(R \over P\) Solutions near singular point We first consider a relatively simple differential equation that has a singular point, Euler equation, singular point at \(x=0\). Euler equation \[L[y]=x^2y&#39;&#39;+\alpha xy&#39; +\beta y = 0\] set \(y=x^r\) we can obtain \(r_1,r_2={-(\alpha-1)\pm\sqrt{(\alpha-1)^2-4\beta}\over 2}\) Three possibilities: Real, distinct roots \(y=c_1x^{r_1}+c_2x^{r_2}\) Equal roots \(y=(c_1+c_2lnx)x^{r_1}\) Complex roots \(y=c_1x^\lambda cos(\mu lnx)+c_2x^\lambda sin(\mu lnx)\) Regular singular point \[P(x)y&#39;&#39; +Q(x)y&#39; +R(x)y=0\] where \(x_0\) is a singular point. This means that \(P(x_0)=0\) and that at least one of \(Q\) and \(R\) is not zero at \(a_0\) weak singularities \(\lim_{x\to x_0}(x-x_0){Q(x)\over P(x)}\) is finite and \(\lim_{x\to x_0}(x-x_0)^2{R(x)\over P(x)}\) is finite This means the singularity in \(Q/P\) can be no worse than \((x-x_0)^{-1}\) and the singularity in \(R/P\) can be no worse than \((x-x_0)^{-2}\). Such a point is called a regular singular point. Otherwise, called irregular singular point. Laplace Transform Among the tools that are very useful for solving linear differential equations are integral transforms. An integral transform is a relation of the form \[F(s)=\int_\alpha^\beta K(s,t)f(t)dy\] where \(K(s,t)\) is given function, called the kernel of the transformation. When \(K(s,t)=e^{-st}\) the transform is called Laplace transforms. \[\mathcal L\{f^{(n)}(t)\}=s^n\mathcal L\{f(t)\}-s^{n-1}f(0)-...-sf^{(n-2)}(0)-f^{(n-1)}(0)\] Procedure To solve a second order linear equation with constant coefficients \[ay&#39;&#39;+by&#39;+cy=f(t)\] By Laplace transform, we have \(a[s^2Y(s)-sy(0)-y&#39;(0)]+b[sY(s)-y(0)]+cY(s)=F(x)\) where the \(F(s)\) is the transform of \(f(t)\). Then we find that \(Y(s)={(as+b)y(0)+ay&#39;(0)+F(s)\over as^2+bs+c}\) Then the \(y(t)\) whose LT is \(Y(t)\) is the solution Unit step function \[\mathcal L\{u(t-c)\}=\int_c^\infty e^{-st}dt={e^{-cs}\over s}\quad s&gt;0\] If \(F(s)=\mathcal L\{f(t)\}\) exists for \(s&gt;a\ge0\), and if \(c\) is a constant, then \[\mathcal L\{e^{ct}f(t)\}=F(s-c), \quad s&gt;a+c\] Conversely, if \(f(t)=\mathcal L^{-1}\{F(s-c\}\), then \(e^{ct}f(t)=\mathcal L^{-1}\{F(s-c)\}\) Systems of First Order Linear Equations Systems of a homogeneous linear equations with constant coefficients are of the form \(\boldsymbol{x}&#39;=A\boldsymbol x\) where \(A\) is a constant \(n \times n\) matrix. Solution By setting \(\boldsymbol x =\xi e^{rt}\), we can get \(r\xi e^{rt}=A\xi e^{rt}\). Therefore, \((A-r\boldsymbol I)\xi=0\). where \(\boldsymbol I\) is the \(n\times n\) identity matrix. Solve $det(A-rI) = 0 $ We got \(r\), the eigenvalues of \(A\). Each one correspond to a constant eigenvectors \(\xi^{(i)}\) For phase portrait, \(r&lt;0\) means toward vector \(\xi^{(i)}\), \(r&gt;0\) means outward direction. Possibilities of eigenvalues All eigenvalues are real and different from each other. \(\boldsymbol x = c_1\xi^{(1)}e^{r_1t}+...+c_n\xi^{(n)}e^{r_nt}\) Some eigenvalues occur in complex conjugate pairs. \(\boldsymbol x^{(1)}(t)=\xi^{(1)}e^{r_1t}\quad \boldsymbol x^{(2)}(t)=\bar\xi^{(1)}e^{\bar r_1t}\) if \(r_1 = \lambda +\mu \quad\xi = \boldsymbol a +i\boldsymbol b\), we can write \(\boldsymbol x^{(1)}(t)=\boldsymbol u(t) + i\boldsymbol v(t)\) \(\boldsymbol u(t) = e^{\lambda t}(\boldsymbol a cos \mu t-\boldsymbol b sin \mu t)\\\boldsymbol v(t) = e^{\lambda t}(\boldsymbol a sin\mu t+\boldsymbol b cos\mu t)\) Then the general solution is \(\boldsymbol x= c_1\boldsymbol u(t) +c_2\boldsymbol v(t) +c_3\xi^{(3)}e^{r_3t}+...+c_n\xi^{(n)}e^{r_nt}\) Some eigenvalues, either real or complex, are repeated. TBC...]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>ODE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fourier Transform]]></title>
    <url>%2F2019%2F03%2F21%2FFourier-Transform%2F</url>
    <content type="text"><![CDATA[Fourier Series CTFS any T-periodic functions can be expressed by many Sinusoids \(\omega_0 = {2\pi\over T}\) is the fundamental frequency \(f_0 = {1 \over T}\) is the fundamental frequency in ordinary frequency Synthesis Equation A weighted sum of infinite Sinusoids \[x(t) =\sum_{k=-\infty}^{\infty} a_ke^{jk\omega_0t}\] Analysis Equation CT calculate the coefficients of each Sinusoids \[a_k = {1\over T}\int_0^Tx(t)e^{-jk\omega_0t}dt\] The coefficient \(a_k\) is aperiodic. Intuition the analysis equation is a normalized inner product that computes a projection coefficient that specifies how much \(x(t)\) projecting on \(e^{jk\omega_0t}\) . The Projection is The normalization process is Tricks \(2Acos\omega t\to a_{\omega\over\omega_0}=A\quad a_{-\omega\over\omega_0}=A\) \(2Asin\omega t\to a_{\omega\over\omega_0}=-jA\quad a_{-\omega\over\omega_0}=jA\) DTFS any N-periodic signal can be expressed by N distinct harmonics. \(\omega_0 = {2\pi\over N}\) is the fundamental frequency, must contain \(\pi\) otherwise not periodic \(f_0 = {1 \over N}\) is the fundamental frequency in ordinary frequency Synthesis Equation a weighted sum of N complex sinusoids. \[ x[n] = \sum_{k=0}^{N-1} a_ke^{jk\omega_0n}\] Analysis Equation DT calculates the FS coefficients which are the weights for the harmonics. \[a_k = {1 \over N}\sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}\] The coefficient \(a_k\) is \(N\) periodic ==IMPORTANT NOTE==: for N is even, $a_{N2} $ is definitely real. It has no conjugates. i.e. \(N=4\) The DTFS of \(2cos({2\pi\over4}n)+2cos({2\pi\over2}n)\) is \(a_{-1}=1,a_0=0,a_1=1,a_2=2\) Properties 1553176610644 Fourier Transform Regarding aperiodic signals as periodic signals in the limit of period T going to infinity. CTFT Recall that \(a_k = {1\over T}\int_{t\in T}x(t)e^{-jk\omega_0t}dt\) is the projection coefficient that \(x(t)\) on \(e^{-jk\omega_0t}\). However, when \(T\to\infty\quad a_k\to0\), so we define \(X_T(j\omega)=\int_{-T/2}^{T/2}x(t)e^{-j\omega t}dt\). Therefore, \(X_T(j{k2\pi\over T}) = Ta_k\) . We rewrite the Synthesis Equation with \(\omega_0={2\pi\over T}\) : \[ x(t) = \sum_{k=-\infty}^{\infty}{\omega_0\over 2\pi}X_T(jk\omega_0) e^{jk\omega_0t}\] as \(T\to\infty\quad \omega_0\to0\), we can rewrite it in integral form. Synthesis Equation consider the signal as a superposition of complex sinusoids with density \[x(t) = {1\over 2\pi}\int_{-\infty}^\infty X(j\omega)e^{j\omega t}d\omega = \mathcal{F}^{-1}\{X(j\omega)\} \] Analysis Equation calculate the density of complex sinusoids with different frequency \[X(j\omega)=\int_{-\infty}^{\infty}x(t)e^{-j\omega t}dt = \mathcal{F}\{x(t)\}\] Duality property There is symmetry in the FT(Fourier Transform) and IFT(Inverse Fourier Transform) integrals. They are identical except for a factor \(2\pi\) and a change in sign. Examples Window \(x(t) = \text{window width }2T_1 \iff X(j\omega) = {2sin\omega T_1\over \omega}\) Complex Sinusoid \(x(t) = e^{j\omega_0t} \iff X(j\omega) = 2\pi\delta(\omega-\omega_0)\) Periodic Signal FT integral of a FS sum equals the sum of a FT integral. \[X(j\omega) = \int_{-\infty}^{\infty}(\sum_{k=-\infty}^\infty a_ke^{jk\omega_0t})e^{-j\omega t}dt \\= \sum_{k=-\infty}^\infty a_k\int_{-\infty}^{\infty}e^{jk\omega_0t}e^{-j\omega t}dt \\=2\pi \sum_{k=-\infty}^\infty a_k\delta(\omega-k\omega_0)\\=2\pi a_{\omega\over\omega_0}\] It is discrete(including \(\delta()\) ) The value is \(2\pi\) the CTFS Periodic Extension FS coefficients of the periodic extension of a signal are the sample values of the signal's FT (scaled by \(1/T\)) \(a_k={1\over T}X(jk\omega_0)\) DTFT DTFT must be \(2\pi\)-periodic Synthesis Equation consider the signal as a superposition of complex sinusoids with density \[x[n] = {1\over 2\pi}\int_{2\pi} X(e^{j\omega})e^{j\omega n}d\omega = \mathcal{F}^{-1}\{X(e^{j\omega})\} \] Analysis Equation calculate the density of complex sinusoids with different frequency \[X(e^{j\omega})=\sum_{n=-\infty}^{\infty}x[n]e^{-j\omega n}dt = \mathcal{F}\{x[n]\}\] Periodic Signal \(X(e^{j\omega})=2\pi\delta(\omega-\omega_0)\) for \(0\le\omega&lt;2\pi\) DFT DFT is the same as DTFS, except for a scaling factor DTFS \[a_k = {1 \over N}\sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}\] \[ x[n] = \sum_{k=0}^{N-1} a_ke^{jk\omega_0n}\] DFT \[X[k] = \sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}\] \[ x[n] = {1 \over N}\sum_{k=0}^{N-1} X[k]e^{jk\omega_0n}\] Property For CTFT For DTFT]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>Fourier</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Probability]]></title>
    <url>%2F2019%2F03%2F21%2FProbability%2F</url>
    <content type="text"><![CDATA[Basic Concepts random experiment an experimental procedure and one or more measurements or observations. sample space the set of all possible outcomes. discrete sample space if the number of outcomes is countable continuous sample space if not countable event certain conditions for an outcome. A subset A of the sample space S An event A occurs if the outcome is a member of A. the certain event, S, consists of all outcomes. the null event, \(\phi\), contains no outcomes. An elementary event contains only one outcome. Replacement and Ordering We can choose \(k\) objects from a set \(A\) that has \(n\) members in different ways Replacement: With replacement After selecting an object and noting its identity, the object is put back before the next selection. Without replacement The object is not put back before the next selection. Ordering: With ordering The order in which we draw the objects is recorded Without ordering Only the identity and number of times each object is drawn is important Replacement ordering number of outcomes Y N - Y Y \(n^k\) N N \({n\choose k}={n!\over (n-k)! k!}\) N Y \({n\choose k}k!={n!\over(n-k)!}\) Note: when with replacement and without ordering, outcomes are NOT equally probable. There are two ways to choose a same object. But they counts the same outcome. Conditional Probability Independence \(P[A\cap B]=P[A]P[B]\) also \(P[A|B] = P[A]\) \[P[A_1A_2...A_n]=\prod_{i=1}^n P[A_i]\] Exclusive \[P[A\cup B]=P[A]+P[B]-P[A\cap B] = P[A]+P[B]\] \[P[A_1A_2...A_n] = 0\] Conditional probability \(P[A|B]={p[A\cap B]\over p[B]}\) where we assume that \(P[B]&gt;0\) Total Probability Theorem \[P[A]=\sum_{i=1}^n P[A|B_i]P[B_i]\] Bayes' Rule \[P[B_j|A]={P[AB_j]\over P[A]}={P[A|B_j]P[B_j]\over\sum_{k=1}^n P[A|B_k]P[B_k]}\] Sequential Experiments Experiments that involve repetitions or multiple participants can often be viewed as a sequence of sub-experiments. The sub-experiments can be identical or non-identical, dependent or independent. The individual sample spaces can be identical or non-identical. If the experiments are identical, the individual sample spaces are identical but not vice versa. Tossing a coin n times: repetition independent identical sub-experiments identical individual sample spaces Checking the number of students sleeping in class: multiple participants independent? (maybe not. Others sleep then I sleep) identical individual sample spaces non-identical sub-experiments (Dalaos do not sleep but I do) Bernoulli trials an experiment once with two possible outcomes Binomial probability law n independent Bernoulli trials \[p_n(k)={n\choose k}p^k(1-p)^{n-k}\] Multinomial probability law M partition of sample space, so \(p_1...p_M\) mutual exclusive \[P[(k_1,k_2,...,k_M)]={n!\over k_1!k_2!...k_M!}p_1^{K_1}p_2^{K_2}...p_M^{K_M}\] Geometric probability law M be the time until the first success \[p_M(m)=p(1-p)^{m-1}\] N be the time before the first success \[p_N(n)=p(1-p)^n\] Sequences of dependent experiments Referred to as a Markov Chain Single Random Variables A random variable \(X\) is a function that assigns a number to every outcome \(\xi\) of an experiment. Underlying sample space \(S\) is called the domain of the RV. \(\xi\in S\) Set of all possible values of \(X\) called the range of the RV. \(X(\xi)\in S_X\) Equivalent Event \(A=\{\xi:X(\xi)\in B\}\) Characteristic PMF probability mass function \[p_X(x)=P[X=x]=P[\{\xi:X(\xi)=x\}]\] For continuous RV, \(P[X=x]=0\). \(p_X(x)&gt;0\) \(\sum_{x\in S_x}p_X(x)=1\) \(P[X\ in\ B]=\sum_{x\in B}p_X(x)\) where \(B\in S_X\) CDF cumulative distribution function The summation of PMF, the integral of PDF, is the cumulative distribution(CDF). \[F_X(x)=P[X\le x]\\=\sum_{n=-\infty}^x p_X(n)=\int_{-\infty}^xf_X(s)ds\] Discrete RV: like a series of stairs jumping up. Continuous RV: a continuous increasing function. Mixed RV: continuous function with discrete jumps where \(P[X=x_j]=c\) PDF probability density function. The value is not a probability! \[P[a\le X\le b]=\int_a^bf_X(x)dx\\f_X(x)={d\over dx}F_X(x)\] For discrete RV: PMF multiply an impulse \(\delta(x-x_k)\). Mean Expectation/Mean of a RV (if the sum/integral converges absolutely) \[m_X = E[X]=\sum_{x\in S_x}xp_X(x) \\E[X]=\int_{-\infty}^\infty xf_X(x)dx\] Expectation/Mean of a function of a RV \(Y=g(x)\) \[E[Y]=\sum_k g(x_k)p_X(x_k)\\E[Y] = \int_{-\infty}^\infty g(x)f_X(x)dx\] Linearity of Expectation Generally \(E[\sum_i a_ig_i(X)]=\sum_i a_iE[g_i(X)] \ne \sum a_ig_i(E[X])\) if \(g_i\) are all linear, the the tree above equal. Variance \[Var[X] = E[D^2]=E[(X-E[X])^2]\\Std[X]=\sqrt {Var[X]}\] Useful formula \(Var[X] = E[X^2]-(E[X])^2 \\Var[cX] = c^2Var[X]\) Moments The Mean and variance are examples of the moments of a RV. The \(n^{th}\) moment of a RV \[E[X^n] = \sum_kx_k^np_X(x_k)\\E[X^n] = \int_{-\infty}^\infty x^nf_X(x)\] The \(n^{th}\) central moment of a RV \[E[(X-E[X])^n] = \sum_k(x_k-E[X])^np_X(x_k)\\E[(X-E[X])^n] = \int_{-\infty}^\infty(x-E[X])^nf_X(x_k)\] Important moments: 1st moment, \(E[X]\) is the Mean 2nd central moment, \(E[(x-E[x])^2]\) is the variance Minimum Mean Squared Error Estimation \[MSE(c) = E[(X-c)^2]=\int_{-\infty}^\infty(x-c)^2f_X(x)dx\] Thus, the best guess is \(c=E[X]\)! Also, the variance is ten the minimum mean squared error associated with the best guess. Discrete RV A discrete random variable assumes values from a countable set. \(S_X=\{x_1,x_2,...\}\) A discrete random variable is finite if its range is finite.\(S_X=\{x_1,x_2,...,x_n\}\) Bernoulli X only take two values, either 1 or 0. \[P_X[0] = 1-p\quad P_X[1] = p\] Mean \(E[X]=p\) Variance \(Var[X] = p(1-p)\) Single coin toss Occurrence of an event of interest Binomial The number of times that an event occurs. \[P_X[k]={n\choose k} p^k(1-p)^{n-k}\quad\text{for k = 0,1,2...}\] Mean \(E[X] = np\) Variance \(Var[X]=np(1-p)\) Multiple coin flips Occurrence of a property in individuals of a population(e.g. bit errors in a transmission) When \(n\to\infty\) , turn to Poisson RV Geometric Suppose a random experiment is repeated, In each repeat, it occurs independently and with probability \(p\) Trials The number of trials until the first success occurs. \[P_X[k]=(1-p)^{k-1}p\quad\text{for k = 1,2,...}\] Mean \(E[X]={1 \over p}\) Variance \(Var[X] = {1-p\over p^2}\) Failures The number of failures before the first success, \(X’ = X-1\) is also a geometric RV. \[P_{X’}[k]=(1-p)^kp\quad\text{for k = 0,1,2...}\] Mean \(E[X‘]={1-p \over p}\) Variance \(Var[M’]={1-p\over p^2}\) Number of transmissions required until an error free transmission. Memoryless property No matter how hard you ever tried, the probability to succeed remains the same. Uniform(discrete) Values in a set of integers are with equal probability. \[S_X=\{0,1,2...,M-1\}\\ P_X[k]={1\over M}\quad \text{for k }\in S_x\] Mean \(E[X] = {M-1\over 2}={a+b\over 2}\) Variance \(Var[X]={M^2-1\over 12}={(b-a+1)^2-1\over12}\) Poisson The number of occurrences of an event in a certain interval of time or space. \[P_X[k]={\alpha^k \over k!}e^{-\alpha}\quad \text{for k=0,1,2...}\] The parameter \(\alpha\) is the average number of events in the interval. Mean \(E[N]=\alpha\) Variance \(Var[N]=\alpha\) Number of hits on a website in one hour number of particles emitted by a radioactive mass in a fixed time period Relationship with Binomial RV: For a Binomial random variable with \(p = {\alpha\over n}\): \(p_0=(1-p)^n = (1-{\alpha\over n})^n \to e^{-\alpha} \quad\text{as}\ n\to\infty\) \({p_{k+1}\over p_k} = {(n-k)p\over(k+1)(1-p)}={(1-k/n)\alpha\over(k+1)(1-\alpha/n)}\) when\(n\to\infty\quad{p_{k+1}\over p_k}={\alpha\over k+1}\) Continuous RV Uniform(continuous) Intervals of the same length on the domain have the same probability. \[S_X=[a,b]\\ f_X(x)={1\over {b-a}}\quad \text{for x }\in S_x\\ F_X(x)={x-a\over {b-a}}\quad \text{for x }\in S_x\] Mean \(E[X] = {a+b\over 2}\) Variance \(Var[X]={(b-a)^2\over 12}\) Exponential The length of time that wait next train. \(\lambda\) is the occurrence time per unit time. \[f_X(x)=\lambda e^{-\lambda x}\quad\text{when }x\gt 0\\ F_X(x)=1-e^{-\lambda x}\quad\text{when }x\gt 0\] Mean \(E[X]={1\over\lambda}\) Variance \(Var[X] = {1\over \lambda^2}\) Memoryless property No matter how hard you ever wait, the probability to meet the right one remains the same. Gaussian(Normal) Variables that tend to occur around a certain value,m , the mean. \[f_X= {1\over \sqrt{2\pi}\sigma}e^{-(x-m)^2\over 2\sigma^2}\ \] we define the CDF of the&quot;normalized&quot; Gaussian with mean m=0 and standard deviation \(\sigma=1\) as \[\Phi(x)={1\over \sqrt{2\pi}}\int_{-\infty}^x e^{-t^2 \over 2}dt\] Then the CDF of a Gaussian RV, \(X\), with mean \(m\) and standard deviation \(\sigma\) is \[F_X(x)=\Phi({x-m\over \sigma})={1\over \sqrt{2\pi}}\int_{-\infty}^{x-m\over\sigma} e^{-t^2 \over 2}dt\] Q-function \(Q(x) = 1-\Phi(x)=P[X&gt;x]\) By symmetry, \(Q(0)={1\over 2}\) and \(Q(x)=\Phi(-x)\) Conditional Conditional PMF/CDF/PDF \[p_X(x_K|C)={P[\{X=x_k\}\cap C]\over P[C]}\\F_X(x|C)={P[\{X\le x\}\cap C]\over P[C]}\\f_X(x|C)={d\over dx}F_X(x|C)\] Total probability \[p_X(x)=\sum_i p_X(x|B_i)P[B_i]\] Conditional expected value \[m_{X|B}=E[X|B]=\sum_{x\in S_X}xp_X(x|B)\] Conditional variance \[VAR[X|B]=E[(X-m_{X|B})^2|B]=E[X^2|B]-m_{X|B}^2\] Compute Everything by Conditioning \[f_X(x)=\sum_i f_X(x|b_i)P[B_i]\\E[X]=\sum_i E[X|B_i]P[B_i]\] Transformation Given a random variable \(X\) with known distribution and a real valued function \(g(x)\), such that \(Y = g(X)\) is also a random variable. Find the distribution of \(Y\). Type of Y If X is discrete, Y must be discrete. If X is continuous, Y can be all three types, depends on \(g(x)\) Key idea To find equivalent events in X for suitably defined events in Y. Approaches If Y is Discrete, find the PMF at the possible values of Y \(p_Y(k) = P[Y=k]=P[\{X:g(X)=k\}]\) If Y is continuous, there are two appsroaches: Find the CDF of Y \(F_Y(y)=P[Y\le y]=P[\{X:g(X)\le y\}]\) Find the PDF of Y \(f_Y(y_0)=\sum_{x:g(x)=y}{f_X(x_)\over |g&#39;(x)|}\) VERY USEFUL \(\uparrow\uparrow\uparrow\uparrow\uparrow\) Multiple Random Variables vector random variable A vector random variable \(\vec X\) is a function that assigns a vector of real numbers to every outcome of an experiment. One random variable can be considered as a mapping from the sample space to the real line. Two random variables can be considered as a mapping from the sample space to the plane. scattergrams visualize the joint behavior of two RVs. Pairs DT case Joint PMF \[p_{X,Y}(j,k)=P[\{X=j\}\cap \{Y=k\}]\] Marginal PMF \[p_X(j)=P[X=j,Y=anything]\] The function of \(X\) is the sum of all \(Y\) along y-axis. PMF of Function Suppose that \(Z=g(X,Y)\), is an integer valued function \[p_Z(k)=P[\{Z=k\}]=\sum_{(i,j):g(i,j)=k}p_{X,Y}(j,l)\] CT case Joint CDF \[F_{X,Y}(x,y)=P[\{X&lt;x\}\cap \{Y&lt;y\}]\] Joint PDF \[f_{X,Y}=\frac {\partial ^2 F_{X,Y}(x,y)}{\partial x\partial y}\] Marginal PDF \[f_X(x)={dF_X(x)\over dx}={dF_{X,Y}(x,\infty)\over dx}=\int_{-\infty}^\infty f_{X,Y}(x,\alpha)d\alpha\] The function of \(X\) is the integral of all \(Y\) along y-axis. PDF of Function For \(Z=g(X,Y)\). To find the \(f_Z(z)\) Pick a real number z Find the region \(D_z=\{(x,y):g(x,y)\le z\}\) Evaluate \(F_Z(z)=\int\int_{D_z}f(x,y)dxdy\) Differentiate \(f_z(z)={d\over dz}F_Z(z)\) If \(X\) and \(Y\) are independent, the PDF of \(Z=X+Y\) is \(f_Z=\int_{-\infty}^\infty f_X(x)f_Y(z-x)dx\) The convolution integral Characteristic Mean for a function \(Z=g(X,Y)\) \[E[Z]=\sum_j \sum_k g(j,k)p_{X,Y}(j,k)\\E[Z] = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y)f_{X,Y}(x,y)dxdy\] The mean of sum is the sum of mean. \[E[X_1+X_2+...+X_n]=E[X_1]+E[X_2]+...+E[X_n]\] If \(X\) and \(Y\) are independent \[E[g_1(X)g_2(Y)]=E[g_1(X)]E[g_2(Y)]\] Moments The \(j^{th},k^{th}\) moment of a pair \[E[X^jY^k]=\int_{-\infty}^\infty\int_{-\infty}^\infty x^jy^kf(x,y)dxdy\] The \(j^{th},k^{th}\) central moment of a pair \[E[(X-E[X])^j(Y-E[Y])^k]\] For \(Z=X+Y\) \(VAR[Z]=VAR[X]+2COV(X,Y)+VAR[Y]\) If \(X\) and \(Y\) are uncorrelated, then \(VAR[Z]=VAR[X]+VAR[Y]\) Covariance The covariance indicates how \(X\) and \(Y\) vary together. \[E[(X-E[X])(Y-E[Y])]=COV(X,Y)\] Positive \(X\) smaller, \(Y\) smaller Negative \(X\) larger, \(Y\) larger Magnitude is not a good measure of their relationship. Useful formula \(COV[X,Y]=E[XY]-E[X]E[Y]\) If either \(X\) or \(Y\) is zero mean, the covariance and correlation are identical \(COV[X,Y]=E[XY]\) Correlation \(X\) and \(Y\) are uncorrelated if \(COV[X,Y]=0\) thus \(E[XY]=E[X]E[Y]\) correlation coefficient \(\rho_{X,Y}={COV[X,Y]\over \sigma_X\sigma_Y}=\rho_{Y,X}\) where \(\sigma_x^2 = VAR(X)\) \(|\rho_{X,Y}|\le 1\) Positive \(X\) larger, \(Y\) larger Negative \(X\) larger, \(Y\) smaller Conditional Probability Conditional PMF/CDF/PDF \[p_{Y|X}(k|j)={p_{X,Y}(j,k)\over p_X(j)}\\F_{Y|X}(y|x)={\int_{-\infty}^yf_{X,Y}(x,\beta)d\beta \over f_X(x)}\\f_{Y|X}(y|x)={f_{X,Y}(x,y)\over f_X(x)}\] All functions above are a cross-plane parallel to y-axis of original 2D function. Total probability theorem \[f_Y(y)=\int_{-\infty}^\infty f_{Y|X}(y|x)f_X(x)dx\] Bayes theorem \[f_{X|Y}(x|y)={f_{Y|X}(y|x)f_x(x)\over\int_{-\infty}^\infty f_{Y|X}(y|x)f_X(x)dx}\] Independence \[P[\{X\in A_X\}\cap \{Y\in A_Y\}]=P[X\in A_X]P[Y\in A_Y]\] If \(X\) and \(Y\) are independent then \[p_{Y|X}(k|j) = p_Y(k)\] Conditional Mean \[E[Y|x]=\int_{-\infty}^\infty yf_{Y|X}(y|x)dy\] The conditional expected value of \(Y\) is a function of \(X\). More general, \(E[g(Y)|x]=\int_{-\infty}^\infty g(y)f_{Y|X}(y|x)dy\) Since \(E[Y|X]\) is a function of \(X\), it is also a RV. Taking the expectation over \(X\), the conditioning disappears! \[E[E[Y|X]] = E[Y]\] more generally, \(E[g(Y)]=E[E[g(Y)|X]]\) This is an expectation version of the total probability theorem More than two RVs An \(N\) dimensional random vector is a mapping from a probability space \(S\) to \(R^N\). Often we think \(n\) RVs as a single n-dimensional random vector. CDF/PDF/PMF \(F_{X_1,X_2,...X_n}(x_1,x_2,...x_n) = P[X_1\le x_1,X_2\le x_2,...X_n\le x_n]\) \(f_{X_1,X_2,...X_n}(x_1,x_2,...x_n)={\partial^n\over\partial x_1...\partial x_n}F_{X_1,X_2,...X_n}(x_1,x_2,...x_n)\) \(p_{X_1,X_2,...X_n}(x_1,x_2,...x_n)=P[X_1= x_1,X_2= x_2,...X_n= x_n]\) Multinomial Distribution A generalization of the binomial distribution. An experiment consists of \(n\) independent trials of a sub-experiment with \(m\) possible outcomes. \(\sum p_i=1\) and \(\sum X_i=n\). \[p_{X_1,X_2,...X_n}(k_1,k_2,...k_n)={n!\over k_1!k_2!...k_m!}p_1^{k_1}p_2^{k_2}...p_m^{k_m}\] if \(k_1+k_2+...+k_m=n\) and \(k_i\ge 0\) for all \(i\), and zero other wise. Marginal Statistics Eliminate variable from a PDF/PMF by integrating/summing \(f_{X_1,X_2,...X_{n-1}}(x_1,x_2,...x_{n-1})=\\\int_{-\infty}^\infty f_{X_1,X_2,...X_n}(x_1,x_2,...x_n)dx_n\) \(p_{X_1,X_2,...X_{n-1}}(x_1,x_2,...x_{n-1})=\\\sum_{-\infty}^\infty p_{X_1,X_2,...X_n}(k_1,k_2,...k_n)\) Conditional Conditional PDF \(f_{X_1X_2X_3}(x_1,x_2|x_3)={f_{X_1X_2X_3}(x_1,x_2,x_3)\over f_{X_3}(x_3)}\\f_{X_1X_2X_3}(x_1|x_2,x_3)={f_{X_1X_2X_3}(x_1,x_2,x_3)\over f_{X_2X_3}(x_2,x_3)}\) Conditional PMF \(p_{X_1X_2X_3}(k_1,k_2|k_3)={p_{X_1X_2X_3}(k_1,k_2,k_3)\over p_{X_3}(x_3)}\\p_{X_1X_2X_3}(k_1|k_2,k_3)={p_{X_1X_2X_3}(k_1,k_2,k_3)\over p_{X_2X_3}(k_2,k_3)}\) Correlation Matrix 1558864122035 Covariance Matrix 1558864243712 Relationship between Correlation and Covariance \[\boldsymbol C=\boldsymbol R-E[\vec X]E[\vec Y]\] if the mean vector is zero, then \(\boldsymbol C=\boldsymbol R\) Symmetric Diagonal if and only if the elements of the random vector are uncorrelated Sample and Stochastic Process Sample Suppose \(X_1,X_2,...,X_n\) are independent and identically distributed(i.i.d.), i.e. they all have the same distribution. Let \(\mu=E[X_i]\) and \(\sigma^2=VAR(X_i)\) and \(S=\sum_{i=1}^nX_i\) \(E[S]=n\mu\\VAR(S)=n\sigma^2\) Sample Mean \(M_n={1\over n}\sum_{j=1}^nX_j\) Then we can get imply \(E[M_n] = m\quad VAR[M_n]={\sigma^2\over n}\) The \(M_n\) is getting closer and closer to the exact value. Markov Inequality \(X\) non-negative RV \(P[X\ge a]\le {E[X]\over a}\) Based on the mean ONLY! Chebyshev Inequality \(P[|X-m|\ge a]\le{\sigma^2\over a^2}\) Laws of Large Numbers Let \(X_1,X_2,...\) be a sequence of i.i.d. RV with finite mean \(\mu\). Weak version \(\lim_{n\to\infty}P[|M_n-\mu|&lt;\epsilon]=1\) Strong version \(P[\lim_{n\to\infty}M_n=\mu]=1\) Central Limit Let \(X_i\) for \(i\in\{1,2,...,n\}\) are i.i.d. with mean and variance \(\mu, \sigma^2\). Define \(S_n\) to be the sum of \(X_i\) If we define \(Z_n={S_n-n\mu\over\sigma\sqrt n}\) \(\lim_{n\to\infty}P[Z_n&lt;z]={1\over \sqrt{2\pi}}\int_{-\infty}^ze^{-x^2\over 2}dx\) In other words, the distribution of \(Z_n\) approaches the distribution of a Gaussian with zero mean and unit variance. \(P[\{X&gt;x\}]=Q(x)\quad P[|X|&lt;\epsilon=1-2Q[(\epsilon)]\) \(\lim_{n\to\infty}P[Z_n&lt;z]={1\over \sqrt{2\pi}}\int_{-\infty}^ze^{-x^2\over 2}dx\) Stochastic Process Definition A random process or stochastic process maps a probability space \(S\) to a set of functions, \(X(t,\xi)\) It assign to every outcome \(\xi \in S\) a time function \(X(t,\xi)\) for $ tI$,where \(I\) is a discrete or continuous index set. A random process is discrete-time if the time index set \(I\) is a countable set. Otherwise it is continuous. \(t\) \(\xi\) \(X(t,\xi)\) vary vary ensemble/family of functions vary fixed realization/ sample functions fixed vary random variable fixed fixed number A random process is uniquely specified by the collection of all n-th order distribution or density functions. \(F_{X(t_1)X(t_2)...X(t_n)}(x_1,x_2,...,x_n)=P[\bigcap_{i=1}^nX(t_i)\le x_i]\) \(f_{X(t_1)X(t_2)...X(t_n)}(x_1,x_2,...,x_n)={\partial^n\over\partial x_1\partial x_2...\partial x_n}F_X\) Characteristics Mean function \(m_X(t)=E[X(t)]\) function of time mean of i.i.d. is constant mean of i.s.i. grow linearly Variance function \(VAR[X(t)]=E[(X(t)-m_X(t))^2]\) function of time mean of i.i.d. is constant mean of i.s.i. grow linearly Autocorrelation \(R_X(t_1,t_2)=E[X(t_1)X(t_2)]=\iint xyf_{X(t_1),X(t_2)}(x,y)dxdy\) Covariance \(C_X(t_1,t_2)=Cov(X(t_1),X(t_2))=R_X(t_1,t_2)-m_X(t_1)m_X(t_2)\) covariance of i.i.d. is a delta function \(C_X(n_1,n_2)=\sigma^2\delta(n_1,n_2)\) covariance of i.s.i. \(C_S(m,n)=\sigma^2min(m,n)\) Correlation coefficient \(\rho_x(t_1,t_2)={C_X(t_1,t_2)\over \sqrt{C_X(t_2,t_2)}\sqrt{C_X(t_2,t_2)}}\) correlation of i.i.d. is \(R_X(n_1,n_2)=\sigma^2\delta(n_1,n_2)+m^2\) Stationary The joint distribution of any set of samples does not depend on the placement of the time origin. i.id. random process is stationary. The mean and variance of stationary processes are constant. The autocorrelation and covariance functions of a stationary process depend only upon the time difference \(t1 - t2\). Wide Sense Stationary(WSS) \(m_X(t)=m\) for all \(t\) \(R_X(t_1,t_2)=R_X(t_1-t_2)\) for all \(t_1,t_2\) \(C_X(t_1,t_2)=C_X(t_1-t_2)\) for all \(t_1,t_2\) \(R_X(0)\) is the average power of the process, \(E[X(t)^2]\) \(R_X(\tau)\) is an even function of \(\tau\) \(R_X(\tau)\le R_X(0)\) If \(X(t)\) is Gaussian, it is also stationary Processes Independent and Identically Distributed Process A discrete-time process \(X_n\) is said to be independent and identically distributed or i.i.d. if all vectors formed by a finite number of samples of the process are i.i.d. \[F_{X_1,X_2,...,X_k}(x_1,x_2,...,x_k)=\prod_{i=1}^kF(x_i)\] Thus, an i.i.d. process is completely specified by a single marginal distribution (density or mass) function. Sum Process A sum process \(Sn\) is a discrete-time random process obtained by taking the sum of all past values of an i.i.d. random process \(X_n\). \(S_n=\sum_{i=1}^nX_i\) One-dimensional random walk: Let \(D_n = 2I_n - 1\). Then \(S_n=2k-n\) for \(0\le k\le n\) \(P[S_n=2k-n]={n\choose k}p^k(1-p)^{n-k}\) Sum Processes are i.s.i Independent Stationary Increment Process An increment of a random process is the difference between the values of the random process at two different points in time. A process X(t) has independent increments if for any \(k \ge 3\) time points, \(t_1 &lt; t_2 &lt; … &lt; t_k\), the increment random variables $X(t_2) - X(t_1), X(t_3) - X(t_2),… $and \(X(t_k) -X(t_{k-1})\) are independent. Only non-overlapping intervals are independent A process X(t) has stationary increments if the increments over any two intervals with the same length have the same distribution. The increments may overlap A process is said to be an independent stationary increment (i.s.i.) process if its non-overlapping increments are both independent and stationary. Any discrete-time i.s.i. process can be expressed as a sum process. \(p_{S_mS_n}(j,k)=p_{S_m}(j)\times p_{S_{n-m}}(k-j)\) Poisson Process Divide each unit interval of the real line into \(m\) equal sub intervals. At each sub interval, we toss a coin with probability of heads \(p=\lambda/m\).((The average number of heads in each unit interval is \(\lambda\).) If heads appears, step forward by 1. If tails appears, stay. \(N_m(t)=\sum_{i=1}^{\lfloor mt\rfloor}X_i\) where \(X_i\) is a Bernoulli random process with parameter \(p\) Denote \(n=\lfloor mt\rfloor\) The underlying discrete-time process is i.s.i. \(N_m(n)=\sum_{i=0}^nX_i\) For fixed \(t\) and \(m\), the distribution of \(N_m(t)\) is binomial with parameters \(n=\lfloor mt\rfloor\) and \(p=\lambda/m\) The distribution of \(N_m(t)\) approaches a Poisson distribution with mean \(np=\lfloor mt\rfloor{\lambda\over m}\to\lambda t\) Counting Process 1558880242820 Binomial Counting Process \(X_n\) is a sequence of i.i.d. Bernoulli RV with parameter \(p\). \(S_n\) is the number of 1's in the first \(n\) trials. \(p_{S_n}(k)={n\choose k}p^k(1-p)^{n-k}\) for \(k=0,1,...,n\) Poisson Counting Process \(N(t)\) is the continuous time non-negative integer valued i.s.i. process whose first order density is Poisson. \(P_{N(t)}(k)= {(\lambda t)^k\over k!}e^{-\lambda t}\) for \(k=1,2,...\) Interpretation The number of events that have occurred up to time \(t\), where events occur at random instants in time at an average rate of $$ per unit time. The time interval between adjacent events is exponentially distributed with parameter \(\lambda\) The inter-arrival times are independent. The time to the mth event an m-Erlang random ariable Suppose exactly one event occurs in[0,t]. the time the event occurs, \(X\), is uniformly distributed on [0,t] If exactly \(n\) events occur in an interval, the \(n\) arrival times are independent and uniformly distributed.]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>probability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Signal and System]]></title>
    <url>%2F2019%2F03%2F19%2FSignal-and-System%2F</url>
    <content type="text"><![CDATA[Signals What is signal? A signal is some measurable quantity that varies over time and/or space. What are systems? We define system to be anything that takes input signals and produces output signals. Basic Characterization and Manipulation Manipulation multiplying signal by constant adding/multiplying signals together Multiplying two straight line segments produces a quadratic curve transformation of the independent variable time shifting \(x(t)\to x(t-t_0)\) Minus = Left = Delay time reflection/reversal \(x(t)\to x(-t)\) time scaling \(x(t)\to x(at)\) Combination of shifting and time scaling Shift first and scale is easier. First scale requires we also scale the shift. Characterization Even signal \(x(-t)=-x(t)\) Odd signal \(x(-t)=x(t)\) Any signal can be viewed as the sum of an even part and an odd part \(x_{even}={x(t)+x(-t)\over 2}\quad x_{odd}={x(t)-x(-t)\over 2}\) infinite duration &amp; finite duration A finite duration DT signal is a sequence of N complex numbers. We can treat it as a complex N-vector. right-sided &amp; left-sided right-sided \(u(t-c)\) left-sided \(u(-t-c)\) Fundamental Signals CT Complex Exponential \[e^{st} = e^{(\sigma+j\omega)t} = e^{\sigma t}e^{j\omega t}\] Where \(e^{\sigma t}\) is purely real. \(\sigma&gt;0\) amplitude grows, \(\sigma&lt;0\) amplitude decays. \(e^{j\omega t}\) is purely imaginary and called Complex Sinusoids Complex Sinusoids \(e^{j\omega t} = cos\omega t +j sin\omega t\) Unit Function $ u(t) = 1$ when \(t\ge 0\) $ u(t) = 0$ when \(t\lt 0\) Impulse Signal Denoted by \(\delta(t)\) \[ u(t) = \int_{-\infty}^{t} \delta(\tau)d\tau \\ \delta(t) = {du(t)\over dt}\] Zero everywhere else, infinite at \(t=0\) Unit total area \(\int_{-\infty}^{\infty} \delta(t)dt = 1\) Integral is the unit step \(u(t) = \int_{-\infty}^{\infty} \delta(\tau)d\tau\) Scaled impulse k\(\delta(t)\) has total area k. ${-}^{} k(t)dt = k{-}^{}(t)dt = k $ Sampling property \(x(t)\delta(t-t_0) = x(t_0)\delta(t-t_0)\) a scaled delta function Sifting property \(\int_{-\infty}^{\infty}x(t)\delta(t-t_0)dt = x(t_0)\) a value at t0 Infinite energy. Therefore \(\delta(t)\) is an idealization that cannot physically exist. Time Scaling corresponds to Magnitude Scaling \(\delta(\alpha t) = {\delta(t)\over \left|{\alpha}\right|}\) DT Real Exponential $ ^n = e^{an} $ where \(\alpha = e^a\) \(\alpha&gt;1\) growing exponential \(0&lt;\alpha&lt;1\) decaying exponential \(-1&lt;\alpha&lt;0\) decaying oscillating \(\alpha&lt;-1\) growing oscillating Real Sinusoids \(cos(\omega n) = cos(2\pi fn)\) Periodic only if its ordinary frequency \(f\) is rational number \(f\) and \(f\pm m(integer)\) are the same frequency. \(\omega\) and \(\omega\pm m2\pi\) are the same frequency The fastest rate at which a DT signal can oscillate is at an ordinary frequency of \(f={1\over2}\) or angular frequency of \(\omega = \pi\) #### Complex Exponential \(z^n\) where \(z = e^s = e^{\sigma + j\omega} = e^\sigma e^{j\omega}\) \(z^n = \left|z\right|^n e^{j\omega n}\) \(\left|z\right|^n\)is real exponential $ e^{jn}$ is complex sinusoid Complex Sinusoid \(x[n] = e^{j\omega n} = cos \omega n + j sin \omega n\) Periodic iff \(\omega = {m2\pi \over N}\) \(e^{j\omega n} = e^{j(\omega +2k\pi)n}\) frequency is periodic for DT signal, and the highest angular frequency is \(\pi\) Unit Step \(u[n] = 1\) when \(n\ge 0\) \(u[n] = 0\) when \(n\lt 0\) Unit Impulse \(\delta[0] = 1\) other place 0. Property same as CT. Relations with Unit step the first difference between two unit step signals. \(\delta[n] = u[n]-u[n-1]\) unit step can be expressed as a sum of impulse by two ways \(u[n]\sum_{m=-\infty} ^n \delta[m]\) \(u[n]\sum_{k=0}^\infty \delta[n-k]\) the sampling property for \(\delta[n]\) when multiply \(x[n]\) and \(\delta[n]\), only one sampled value, not a continuous function anymore. Complex Frequency CT and DT Complex Frequency \(z = e^s = e^{\sigma+ j\omega}\) DT CT Growing \(|z|&gt;1\) \(\sigma&gt;0\) Decaying \(|z|&lt;1\) \(\sigma&lt;0\) Oscillatory frequency \(\ang z\) \(\omega\) System Basic System Characterization Memoryless: instantaneous Causality: does not depend on future Stability: BIBO(Bounded-Input Bounded Output) Invertibility: distinct inputs lead to distinct outputs Time-invariant Linearity Time Invariance Shifted input shifted output. Operation of time does not depends on time. $ y(t) = {t-3}^{t+2} x()d$ YES : 3 time before and 2 later $ y(t) = {0}^{t} x()d$ NO: has a beginning point 0 Linearity superposition property can be decomposed into two Additive property: \(x_1(t) \to x_2(t) = y_1(t) \to y_2(t)\) Homogeneity (scaling): \(ax_1(t) \to ay_1(t)\) LTI must satisfy zero-input/zero-output Elementary Linear Operation: Differentiation and integration: \(x^{(k)}(t)\) ( LTI! ) Time shifting:\(x(t-\tau)\) ( LTI! ) Multiply by a function: \(g(t)x(t)\) (not TI unless \(g(t)\) is constant) time reversal, time scaling ( not TI because there is a origin) Impulse Response Denoted by \(h(t)\) or \(h[n]\), is the output of \(\delta(t)/\delta[t]\) in a LTI system. If the output of a system is given by convolving the input with a \(h[n]\), then the system is LTI! Convolution is LTI Typical Examples: 1. \(h[n] = \delta[n]\) identity function. Do nothing. 2. \(h[n] = \delta[n-m]\) delay system. Delay the input by m. 3. \(h[n] = \delta[n]+0.3\delta[n-4]\) echo system. With echo volume 0.3. 4. \(h[n] = u[n]\) first sum system. \(y[n] = \sum_{-\infty}^n x[k]\) 5. \(h[n] = {1\over 3} (u[n] - u[n-3]) = {1\over 3}(h[n] + h[h-1] + h[n-2])\) window smoother. 6. \(h[n] = \delta[n] -\delta[n-1]\) first difference system. LTI Systems Linearity ensure that If \(\delta[n-k]\to h_k[n]\) we can imply \(x[n] = \sum_{k=-\infty}^{\infty} x[k]\delta[n-k]\to y[n] = \sum_{k=-\infty}^{\infty} x[k]h_k[n-k]\) Time Invariance ensure that In fact \(h_k[n]=h[n-k]\) \(x[n] = \sum_{k=-\infty}^{\infty} x[k]\delta[n-k]\quad y[n] = \sum_{k=-\infty}^{\infty} x[k]h[n-k]\) For CT case \(x(t) = \int_{-\infty}^{\infty}x(\tau)\delta(t-\tau)d\tau\quad y(t) = \int_{-\infty}^{\infty}x(\tau)h(t-\tau)d\tau\) we use \(*\) to denote it. \(y(t)=x(t)*h(t)\) Convolution/LTI properties: 1. commutative \(x(t) \ast h(t) = h(t) \ast x(t)\) 2. distributive \(x(t) \ast\{h_1(t) + h_2(t)\} = x(t)\ast h_1(t) +x(t) \ast h_2(t)\) 3. associative\(\{x(t)\ast h_1(t)\}\ast h_2(t) = x(t)*\{h_1(t)*h_2(t)\}\) Charactering LTI system by Impulse Responses: 1. Memoryless &lt;=&gt; $h[n] = 0 $ for \(n\ne0\) 2. Causality &lt;=&gt; \(h[n] =0\) for \(n\lt0\) 3. Stability &lt;=&gt;impulse response is absolute integrable/summable \(\int_{-\infty}^{\infty}\left|h(t)\right|dt \lt \infty\) or \(\sum_{n=-\infty}^{\infty}\left|h(t)\right|dt \lt \infty\) If the impulse response of the system is a unit step function, the system is a integral/first sum system. Conceptually computing For the DT convolution \(y[n]=x[n]*h[n]=\sum_{k=-\infty}^\infty x[k]h[n-k]\) draw the original \(x[k]\) and \(h[k]\) with variable \(k\) flip \(h[k]\) to get \(h[-k]\), the convolution kernel shift \(h[-k]\) right by \(n\) to obtain \(h[n-k]\) Multiple \(x[k]\) by \(h[n-k]\) and sum over all results System Function and Frequency Response System Function \[y[n] = \sum_{k=-\infty}^\infty h[k]x[n-k] = H(z)x[n] \\ H(z) = \sum_{k=-\infty}^{\infty} h[k]z^{-k}\] \[y(t) = \int_{\tau=-\infty}^\infty h(\tau)x(t-\tau) = H(s)x(t) \\H(s) = \int_{-\infty}^{\infty}h(\tau)e^{-s\tau}d\tau\] $z^n \(/\) e^{sn}$ is an eigenfunction of DT/CT LTI systems. \(H(z)\) or \(H(s)\) is the eigenvalue that depends on the complex frequency \(z\). They are also called system function of CT/DT LTI systems. \(H(s)\) is a Laplace Transform of the CT impulse response \(h(t)\) \(H(z)\) is a z-transform of the DT impulse response \(h[n]\) Frequency Response For LTI System, the eigenvalue at different oscillation frequency \(\omega\) is called frequency response. \(H(s)e^{st} = H(j\omega)e^{j\omega t}\) when \(s=\sigma + j\omega\) and \(\sigma=0\) where \(H(j\omega)\) is the cross-section of \(H(s)\) along \(j\omega\) axis. \(H(z)z^{n} = H(e^{j\omega})e^{j\omega n}\) when \(|z|= 1\) and \(\ang z=\omega\) where \(H(e^{j\omega})\)is the value of system function \(H(z)\) along the unit circle \(z=e^{j\omega}\) \(H(j\omega)=H(s)\) given that \(s=j\omega\) \(H(e^{j\omega})=H(z)\) given that \(|z|=1;z=e^{j\omega}\)]]></content>
      <categories>
        <category>Signal is Everything</category>
      </categories>
      <tags>
        <tag>Fourier</tag>
        <tag>signal</tag>
        <tag>system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Micro Economics]]></title>
    <url>%2F2019%2F01%2F30%2FMicro-Economics%2F</url>
    <content type="text"><![CDATA[[TOC] Ten Principles How people make decisions People Face Trade-offs The Cost of Something Is What You Give Up to Get It Rational People Think at the Margin People Respond to Incentives How people interact Trade Can Make Everyone Better Off Markets Are Usually a Good Way to Organize Economic Activity Governments Can Sometimes Improve Market Outcomes How the Economy as a whole works A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services Prices Rise When the Government Prints Too Much Money Society Faces a Short-Run Trade-off between Inflation and Unemployment Think like an Economist Scientist: Observation Theory and More Obeservation Assumption Model: Circular-Flow Diagram and Production Possibilities Frontier Police advisor: Positive vs Normative Statement Two hands Not accepted Disagree with each other Scientific Judgement Value Perception vs Reality Interdependence and Gains from Trade Absolute advantage the ability to produce a good using fewer inputs than another producer. Opportunity cost whatever must be given up to obtain some item. Comparative advantage the ability to produce a good at a lower opportunity cost than another producer. One should do what he has lower comparative advantage and export them. Price of the trade is between the two opportunity costs. Market Forces of Supply and Demand Distinguish between demand and quantity demanded, supply and quantity supplied. For both curve, increase means a shift to right, decrease means a shift to left. Shifts in the Demand Curve Determinants: 1. Income - normal good low income low demand - inferior good low income high demand. 2. Prices of Related Goods - substitutes one price increase, quantity decrease. another demand increase. Both price and quantity increase. - complements one price increase, another demand decrease. Both price and quantity decrease. 3. Tastes 4. Expectations 5. Number of Buyers: more means more demand. Shifts in the Supply Curve Determinants: 1. Input Prices Input price rise, supply decrease. 2. Technology Increase supply. 3. Expectations: 4. Number of Sellers: more means more supply. Elasticity A measure of the responsiveness of quantity demanded or quantity supplied to a change in one of its determinants. Demand The Price Elasticity of Demand A measure of how much the quantity demanded of a good responds to a change in the price. Determinants for Price Elasticity of Demand: Availability of Close Substitutes: goods with close substitutes are more elastic. Necessities vs Luxuries: Necessities tend to have inelastic demands. Definition of the Market: Narrowly defined markets tend to have more elastic demand than broadly defined markets because it is easier to find close substitutes. Time Horizon: Goods tend to have more elastic demand over longer time horizons. Computing \(E_d=\Delta Q_d/\Delta P\) Price elasticity = Percentage change in quantity demanded / Percentage change in price Percentage change are often calculated by midpoint method. Variety In Demand Curve, flatter(horizontal) means elastic, steeper (vertical) means inelastic. Linear demand curve points with a low price and high quantity(right) are inelastic, while points with a high price and low quantity(left) are elastic. Other Demand Elasticities \(E_i=\Delta Q_d/\Delta I\) The Income Elasticity of Demand = Percentage change in quantity demanded / Percentage change in income. \(E_c=\Delta Q_{d,a}/\Delta P_b\) The Cross-Price Elasticity of Demand = Percentage change in quantity demanded of good A / Percentage change in the price of good B Total Revenue Total Revenue = Price x Quantity. \(TR = P \times Q\) elasticity &lt; unit elasticity = 1: P up R up. \(E&lt;1\quad P\uparrow R\uparrow\) elasticity &gt; unity elasticity = 1: P up R down. \(E&gt;1\quad P\uparrow R\downarrow\) Supply The Price elasticity of supply A measure of how much the quantity supplied of a good responds to a change in the price. Determinants for Price Elasticity of Supply: Flexibility of sellers to change the amount of good. Time period considered.(Most markets) Computing \(E_s=\Delta Q_s/\Delta P\) Price elasticity of supply = Percentage change in quantity supplied / Percentage change in price Variety In Supply. Curve, flatter(horizontal) means elastic, steeper (vertical) means inelastic. Example and Application Good farming news may harm farmers: Supply shifts right, demand remain the same, elasticity greater than 1, total revenue decrease. OPEC fall to keep oil price high: Supply is elastic over long period. Curve becomes flatter and flatter. Drug interdiction may increase drug-related crime: Supply shifts left, demand remain the same, elasticity less than 1, total revenue increase. Instead, to avoid drug-related crime, we should spend resource on demand side, like drug education. Supply Demand and Government Policies price control price ceiling a binding price ceiling causes a shortage. Gas Pump. A long lines. Rent Control. the initial shortage caused by rent control is small. landlords use various mechanisms to ration housing. waiting list, preference, discrimination, under-the-table payments. price floor a binding price floor causes a surplus. The Minimum Wage: unemployment. great impact on teenage labor. More of them tend to work instead of study. Tax When the government levies a tax on a good, the equilibrium quantity of the good falls. That is, a tax on a market shrinks the size of the market. Taxes levied on sellers and taxes levied on buyers are equivalent. Can Congress Distribute the Burden of a Payroll Tax? Doesn't matter...Tax incidence depends on the forces of supply and demand A tax burden falls more heavily on the side of the market that is less elastic(inelastic). Who Pays the Luxury Tax? Producer! CS PS and Efficiency of Markets Consumer Willingness to Pay the maximum amount that a buyer will pay for a good Consumer Surplus the amount a buyer is willing to pay for a good minus the amount the buyer actually pays for it. The area below the demand curve and above the price measures the consumer surplus in a market lower price raises CS(consumer surplus). it measures the benefit that buyers receive from a good as the buyers themselves perceive it. Producer Cost the value of everything a seller must give up to produce a good. A measure of producer's willingness to sell. Producer Surplus the amount a seller is paid for a good minus the seller’s cost of providing it. The area below the price and above the supply curve measures the producer surplus in a market. higher price raises PS(producer surplus) Market Total Surplus CS+PS. the sum of consumer and producer surplus. Efficiency the property of a resource allocation of maximizing the total surplus received by all members of society Equality the property of distributing economic prosperity uniformly among the members of society. Market in Organs Not fair Market Failure the inability of some unregulated markets to allocate resources efficiently. The Costs of Taxation Deadweight Loss the fall in total surplus that results from a market distortion, such as a tax. the elasticities of supply and demand determine the size of the deadweight loss from a tax. the greater the elasticities of supply and demand, the greater the deadweight loss of a tax. The Deadweight Loss Debate Some economists believe deadweight loss is small because labor supply is fairly inelastic. Some economists believe deadweight loss is large because labor supply is more elastic. adjust number of hours they work second earners elderly can choose when to retire consider engaging in underground economy When Taxes Vary Deadweight loss increase as \(x^2\) Tax revenue change as \(max-(x-optimal)^2\) Laffer Curve and supply-side economics Economists debates whether tax rates had in fact reached such extreme levels. Because the cut in tax rates was intended to encourage people to increase the quantity of labor they supplied, the views of Laffer and Reagan became known as supply-side economics. The Costs of Production total revenue(TR) \(TR=P\times Q\) the amount a firm receives for the sale of its output. Cost total cost(TC) \(TC= FC+VC\) the market value of the inputs a firm uses in production. explicit costs input costs that require an outlay of money by the firm. Both economists and accountants care. implicit costs input costs that do not require an outlay of money by the firm. Only economists care. Profit total revenue minus total cost \(TR-TC\) economic profit total revenue minus total cost, including both explicit and implicit costs accounting profit total revenue minus total explicit cost. usually larger than economic profit. Production and Cost production function the relationship between quantity of inputs used to make a good and the quantity of output produced. marginal product the increase in output that arises from an additional unit of input diminishing marginal product the property whereby the marginal product of an input declines as the quantity of the input increases Various Measures of Cost fixed costs(FC) costs that do not vary with the quantity of output produced variable costs(VC) costs that vary with the quantity of output produced Average Cost average total cost(ATC) \(ATC=TC/Q\) total cost divided by the quantity of output. usually U-shaped. decrease because of AFC, increase because of AVC. average fixed cost(AFC) \(AFC=FC/Q\) fixed cost divided by the quantity of output. shape like \(1\over x\). average variable cost(AVC) \(AVC=VC/Q\) variable cost divided by the quantity of output. Usually rise as the quantity produced increases. Marginal Cost(MC) The increase in total cost(TC) that arises from an extra unit of production(Q). \[ MC = \Delta TC/\Delta Q\] eventually rise as the quantity produced increases. reflect the property of diminishing marginal product Relationship with ATC Whenever marginal cost is less than average total cost, average total cost is falling. Whenever marginal cost is greater than average total cost, average total cost is rising. The marginal-cost curve crosses the average-total-cost curve at its minimum. Long Run Short Run economies of scale the property whereby long-run average total cost (ATC) falls as the quantity of output increases diseconomies of scale the property whereby long-run average total cost (ATC) rises as the quantity of output increases constant returns to scale the property whereby long-run average total cost (ATC) stays the same as the quantity of output changes Firms in Competitive Markets Competition competitive market There are many buyers and many sellers in the market. The goods offered by the various sellers are largely the same. Firms can freely enter or exit the market.(perfectly competitive markets) Average Revenue(AR) \(AR=TR/Q\) total revenue divided by the quantity sold. For all types of firms, average revenue equals the price of the good marginal revenue(MR) \(MR=\Delta TR/\Delta Q\) the change in total revenue from an additional unit sold. For competitive firms, marginal revenue equals the price of the good. Profit Maximization supply decision If marginal revenue is greater than marginal cost, the firm should increase its output. If marginal cost is greater than marginal revenue, the firm should decrease its output. At the profit-maximizing level of output, marginal revenue and marginal cost are exactly equal. In essence, because the firm’s marginal-cost curve determines the quantity of the good the firm is willing to supply at any price, the marginal-cost curve is also the competitive firm’s supply curve,precisely, the portion lies above AVC. short run shut down temporary shut down if \[TR&lt;VC\\TR/Q&lt;VC/Q\\P&lt;AVC\] price is below the whole curve of AVC. producing any good lose money. The competitive firm’s short-run supply curve is the portion of its marginal-cost curve that lies above average variable cost. sunk costs A cost that has already been committed and cannot be recovered. The firm’s short-run supply curve is the part of the marginal-cost curve that lies above average variable cost, and the size of the fixed cost does not matter for this supply decision. Near-Empty Restaurants the rent, kitchen equipment, tables, plates, silverware, and so on are fixed cost. They are sunk cost. Even empty why not open... Off-Season Miniature Golf fixed costs are irrelevant in making this short-run decision. Only open when its revenue exceeds its variable costs. long run shut down permanent exit the market if \[TR&lt;TC\\TR/Q&lt;TC/Q\\P&lt;ATC\] enter market if \(P&gt;ATC\) The competitive firm’s long-run supply curve is the portion of its marginal-cost curve that lies above average total cost. measure profit \[Profit = TR-TC\\ Profit=(TR/Q-TC/Q)\times Q \\ Profit = (P-ATC)\times Q\] The Supply Curve in a Competitive Market short run supply curve inelastic because few firms enter or exit the market long run supply curve elastic because many firms enter or exit the market. At the end of this process of entry and exit, firms that remain in the market must be making zero economic profit. The process of entry and exit ends only when price and average total cost are driven to equality. A shift in demand curve in the short run create more profit. in the long run attract more firms to enter and supply curve shift to let every firm make zero economic profit. why long run supply cure slope upward resources limited. market of resources is inelastic. firms have different costs. Some have lower costs than others. Monopoly A firm that is the sole seller of a product without close substitutes. barriers to entry is the fundamental cause of monopoly. It has three main sources: 1. Monopoly resources: A key resource for production is owned by a single firm. 2. Government regulation: The government gives a single firm the exclusive right to produce some good or service. 3. The production process: A single firm can produce output at a lower cost than can a larger number of firms. natural monopoly: a monopoly that arises because a single firm can supply a good or service to an entire market at a smaller cost than could two or more firms. Production and Price Decision A monopolist’s marginal revenue is always less than the price of its good. Price effect on total revenue (P X Q): The output effect: Q higher, which tends to increase total revenue. The price effect: P lower, which tends to decrease total revenue. Price decision to maximize profit Choose the quantity where the Marginal Revenue = Average Total Cost \(MR=MC\). Then use demand curve to find the correspond price of that quantity.\(Q_P=Q_{MR=MC}\) Difference between competitive an monopoly firms For a competitive firm: \(P=MR=MC\) For a monopoly firm: \(P&gt;MR=MC\) A monopoly’s profit \(Profit = TR-TC=(TR/Q-TC/Q)Q=(P-ATC)Q\) Monopoly Drugs vs generic Drugs - After patent expires, other companies quickly enter and begin selling so-called generic products that are identical to original product. - Even when patent expires, consumers may not be convinced generic are identical, so the price of original product may still remain higher that market price. The Welfare Cost socially efficient quantity where the demand curve and the marginal cost(MC) curve intersect. monopoly quantity where the marginal revenue(MR) curve and the marginal cost(MC) curve intersect The monopolist produces less than the socially efficient quantity of output. Deadweight Loss The deadweight loss triangle is similar to the one caused by a tax. The difference between the two cases is that the government gets the revenue from a tax, whereas a private firm gets the monopoly profit. Welfare in monopolized market includes the welfare of both consumers and producers. While the consumers are worse off, the producers are better off. The problem in a monopolized market arises because the firm produces and sells a quantity of output below the level that maximizes total surplus. Price Discrimination The business practice of selling the same good at different prices to different customers. It is a rational strategy for a profit-maximizing monopolist. It requires the ability to separate customers according to their willingness to pay. some certain market force can prevent price discrimination, like arbitrage It can raise economic welfare. Movie Tickets - Low price for children and senior Airline Prices - Lower price for a round-trip if the traveler stays over a Saturday night, because they are likely to be leisure travelers. Discount Coupons - The poor has low willingness to purchase but high willingness to clip coupons. Financial Aid - Many college and universities give financial aid to needy students. One can view this policy as a type of price discrimination. Quantity Discount Lower price for customers who buy large quantities. Public Policy Increase Competition with Antitrust Laws some merges are to lower costs. These benefits are called synergies Regulation. price at MC have two problems ATC &gt; MC. The company lose the fixed cost. it gives monopolies no incentive to reduce costs. Public Ownership But Economists prefer private ownership. Doing nothing Some Economists argue that this is the best. Monopolistic Competition Between Monopoly and Perfect Competition In reality, many industries fall somewhere between the polar cases of perfect competition and monopoly. Economists call this situation imperfect competition. There are two types of imperfect competition: oligopoly: a market with only a few sellers, each offering a product that is similar or identical to the products offered by other sellers in the market. Economists measure a market's domination by a small number of firms with a statistic called the concentration ratio, which is the percentage of total output in the market supplied by the four largest firms. monopolistic competition: a market structure in which many firms sell products that are similar but not identical. Attributes: Many sellers: There are many firms competing for the same group of customers, Product differentiation: Each firm produces a product that is at least slightly different from those of other firms. Thus, rather than being a price taker, each firm faces a downward-sloping demand curve. Free entry and exit: Firms can enter or exit the market without restriction. Thus, the numbers of firms in the market adjusts until economic profits are driven to zero. NO. of Firms Type Example One Monopoly Tap water. Cable TV. Few Oligopoly Tennis balls. Cigarettes. Many Monopolistic Competition Novels. Movie. Many Perfect Competition Wheat. Milk. Competition with Differentiated Products To understand monopolistically competitive markets, there are four aspects. ### Firm in the Short Run A firm chooses its quantity and price just as monopoly does. The quantity is at which MR = MC and then use demand curve to find the price. Long Run Equilibrium Profit encourage entry and loss encourage exit. Finally the demand curve and ATC curve is tangent at the quantity where MR curve intersects MC. - as in a monopoly market, price exceeds MC. - as in a competitive market, price equals ATC. Monopolistic vs Perfect Competition Two noteworthy differences. 1. Excess Capacity Perfectly competitive firms produce at the efficient scale,whereas monopolistically competitive firms produce below this level, called to have excess capacity under monopolistic competition. 2. Markup over Marginal Cost The difference between the price and the MR=MC point. the Welfare of Society One source of inefficiency is the markup of price over MC. Thus, a monopolistically competitive market has the normal deadweight loss of monopoly pricing. Another way is the externalities associated with entry. - The product-variety externality: Because consumers get some consumer surplus from the introduction of a new product, entry of a new firm conveys a positive externality on consumers. - The business-stealing externality: Because other firms lose customers and profits from the entry of a new competitor, entry of a new firm imposes a negative externality on existing firms. In the end, we can conclude only that monopolistically competitive markets do not have all the desirable welfare properties of perfectly competitive markets. That is, the invisible hand does not ensure that total surplus is maximized under monopolistic competition. Yet because the inefficiencies are subtle, hard to measure, and hard to fix, there is no easy way for public policy to improve the market outcome. Advertising The debate over Advertising &gt; Advertising and the Price if Eyeglasses &gt; &gt; - Advertising reduced average prices by ire than 20 percent. Advertising is as a signal of quality. Only good goods are advertised. Brand name - provide consumers with information about the quality when quality cannot be judged before purchase. - give firms an incentive to maintain high quality. Oligopoly A market structure in which only a few sellers offer similar or identical products. Game Theory The study of how people a heave in strategic situations. Markets with Only a Few Sellers duopoly An oligopoly with only two members. Competition If the market is perfectly competitive, the price is the marginal cost(MC). If the market is monopoly , the price is the marginal revenue(MR). Collusion The agreement among firms in a market about quantities to produce or prices to charge. Cartel A group of firms acting in unison. A cartel must agree not only on the total level of production but also on the amount produced by each member. The Equilibrium for an Oligopoly Oligopolists would like to form cartels and earn monopoly profits, but that is often impossible, prohibited by antitrust laws. Nash equilibrium A situation in which economic actors interacting with one another each choose their best strategy given the strategies that all the other actors have chosen. When firms in an oligopoly individually choose production to maximize profit, they produce a quantity if output greater than the level produced by monopoly and less than the level produced bu perfect competition. The oligopoly price is less than the monopoly price but greater than the competitive price(which equals marginal cost). As the number of sellers in an oligopoly grows larger, an oligopolistic market looks more and more like a competitive market. The price approaches marginal cost, and the quantity produced approaches the socially efficient level. The Economics of Cooperation Prisoner’s dilemma A particular “game” between two captured prisoners that illustrates why cooperation is difficult to maintain even when it is mutually beneficial. Dominant strategy A strategy that is best for a player in a game regardless of the strategies chosen by the other players. Confessing the other prisoner is the dominant strategy for each of the prisoners, just like to produce more is the dominant strategy for firms in oligopoly. OPEC and the World Oil Market OPEC is a cartel trying to increase oil price. But its member cheated. So the price fell. Other examples of Prisoner’s Dilemma &gt; Arm Race &gt; - US and Soviet both arm. &gt; &gt; Common Resources &gt; &gt; - Self-interest is the essence of humanity. Welfare of Society Whether lack of cooperation is a problem depends on the circumstances Bad for both society and players: arm-race, common resources. Good for society bad for players: oligopolists trying to maintain monopoly profits. Why people sometimes cooperate Cooperation may be achieved if the situation occurs repeatedly. The Prisoner’s Dilemma Tournament The winner is a simple strategy called tit-for-tat. Start by cooperation and just do what the other do last time. Public Policy toward Oligopolies Policy makers should induce compete and discourage cooperation. 1. Restraint of trade and the antitrust laws. An illegal phone call The Sherman Antitrust Act prohibits competing executives from even talking about fixing prices. Controversies over antitrust policy Yet the antitrust laws have been used to condemn some business practices whose effects are not obvious. Here we consider three examples. Resale price maintenance Discount retailer free rides others' good service. Business practices that appear to reduce competition may in fact have legitimate purposes. Predatory pricing Economists continue to debate whether predatory pricing should be a concern for antitrust policymakers. Is it profitable? If so, when? Can courts distinguish which price is predatory? Tying It is a form of price discrimination. It is unclear whether tying has adverse effects for society as a whole. The Microsoft case Government thought typing IE browser to Windows exerted too much market power.MS thought it was natural to integrate. Finally MS accept some restrictions from the government and government agree it to integrate IE to Windows. Externalities The uncompensated impact of one person’s actions on the well-being of a bystander. If the impact on the bystander is adverse, it is called a negative externality. If it is beneficial, it is called a positive externality. Market Inefficiency Negative Externalities social cost includes private cost plus the costs to those bystanders. The social-cost curve is above the supply curve. \(Q_{optimum}&lt;Q_{market}\) internalizing the externality altering incentives so that people take account of the external effects of their actions. Positive Externalities social value is greater than the private value. The social-value curve is above the demand curve. \(Q_{optimum}&gt;Q_{market}\) subsidy give money to assist the market. Technology Spillovers, Industrial Policy, and Patent Protection - Technology Spillovers: one firm’s research benefits the whole industry, positive externality. - Industrial Policy: government intervention that aims to promote technology-enhancing industries. - Patent Protection: internalize the externality by giving the firm a proper right over its invention. Public Policies toward Externalities As a general matter, the government can respond to externalities in one of two ways. Command-and-control policies regulate behavior directly. Market-based policies provide incentives so that private decision makers will choose to solve the problem on their own. Command-and-Control Policies Regulation Environmental Protection Agency(EPA) sometime dictates a maximum level of pollution that a factory may emit. Other times the EPA requires that firms adopt a particular technology to reduce emissions. Market-Based Policy Corrective Taxes and Subsidies (Pigovian taxes) a tax designed to induce private decision makers to take account of the social costs that arise from a negative externality. Supply curve is a horizontal line, perfectly elastic. Why Is Gasoline Taxed So Heavily Congestion Accidents Pollution Tradable Pollution Permits Set up a maximum pollution quantity, but allow the quantity to be traded. Supply curve is a vertical line, perfectly inelastic. In some circumstances, selling pollution permits may be better than levying a corrective tax. Because the demand curve is often unknown, in order to achieve a max pollution goal, EPA can just simply set it to be the limit and let market force drive it to the optimal efficiency. Environments stated clean environment is priceless. Economists disagree with them. They even disagree with each other. They think people face trade-off, technology development needs pollution. Also, if people think more in economic term, considering clean environment a good, by economic tricks mentioned above, we can protect it better. Private Solution The types include Moral codes and Social sanctions People do not liter Charities &gt; Sierra Club &gt; - nonprofit organization funded with private donations to protect the environment. &gt; &gt; colleges and universities receive gifts from alumni, corporations, and foundations. Relying on the self-interest of the relevant parties. Apple grower buy neighbor beekeeper’s beehives. Enter into a contract. Apple grower and beekeeper make a contract specifying the number of trees and bees. The Coase Theorem The proposition that if private parties can bargain without cost over the allocation of resources, they can solve the problem of Externalities on their own. Legal to keep dog - The victim pays the owner for not keeping dog. Illegal to keep dog - The owner pays the victim for keeping dog. The Coase theorem says that private economic actors can potentially solve the problem of externalities among themselves. Whatever the initial distribution of rights, the interested parties can reach a bargain in which everyone is better off and the outcome is efficient. Why not always working However, the Coase theorem applies only when the invested parties have no trouble reaching an agreement. In the real world, bargaining does not always work, even when a mutually beneficial agreement is possible. Transaction costs The cost that parties incur in the process of agreeing to and following through on a bargain. It is hard for a large number of parties to make bargaining. Government sometimes can act on behalf of an interested parties to negotiate with another. Public Goods and Common Resources Different Kinds of Goods Two characteristics: 1. excludable the property of a good whereby a person can be prevented from using it. 2. rival in consumption The property oof a good whereby one person’s use diminished other people’s use. The two characteristics divide good s into four categories: 1. Private goods Both excludable and rival in consumption. &gt; Ice-cream cones &gt; Clothing &gt; Congested toll road Public goods Neither excludable nor rival in consumption. Tornado siren National defense Uncongested non-toll roads Common resources Rival in consumption but not excludable. Fish in the ocean The environment Congested non-toll roads Club goods Excludable but not rival in consumption. Fire protection Cable TV Uncongested toll roads Public Goods free rider a person who receives the benefit of a good but avoids paying for it. National Defense - necessary public good Basic Research - profit-seeking firms’ incentive is to free ride on the genera knowledge created by others. So, government should remedy the situation. However, the support may be inappropriate because the benefits are hard to measure. Fighting Poverty Advocates of antipoverty programs claim that fighting poverty is a public good. Some goods may switch between public and private good. &gt; Lighthouse &gt; - Ships free ride the benefit of lighthouse, so most lighthouse are run by government now. &gt; - On the coast of England in the 19th century, some lighthouses were privately owned. They charged the owner of nearby port. If not paid, they would turn off the light so that no ship comes. cost-benefit analysis A study that compares the costs and benefits to society of providing a public good. How much is a life worth? - Ways to look at the total amount of money a person would have earned is bizarre for Economists because disabled and retired has no value. - Ways to estimate what value people put on their own lives, concludes each life is about $10 million. Common Resources Tragedy of the Commons A Parable that illustrates why common resources are used more than is desirable from the standpoint of society as a whole. Overgrazing - Reason: people neglect the negative externalities of reducing the goods for others. - Solution: government should internalize the externality by taxing sheep or auction off a limited number of sheep-grazing permits. Some important Common Resources &gt; Clean Air and Water &gt; - Pollution is negative externalities that can be remedied with regulation or corrective taxes. &gt; &gt; Congested Roads &gt; - Congestion yields rivalry of consumption. &gt; - Toll is like a corrective tax. &gt; &gt; Fish, Whales and Other Wildlife &gt; - Government charges fishing and hunting license. &gt; - Fisherman must throw back small fish. &gt; - Hunter can kill only a limited number of animals. Case study &gt; Why the Cow Is Not Extinct? &gt; - Cows are a private good, while elephants are common resources &gt; - To protect elephants, some African country made it illegal to kill them and some made allow people to kill only those on their own property.]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>Economics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shortest Paths and Flow]]></title>
    <url>%2F2018%2F12%2F15%2FShortest-Paths-and-Flow%2F</url>
    <content type="text"><![CDATA[Algorithm Comments Graph Rep Running time Space Used Bellman-Ford Single-Source Adj List 𝑂(𝑉𝐸) 𝑂(𝑉) In DAG Out DAG Single-Source DAG Adj List 𝑂(𝑉+𝐸) 𝑂(𝑉) Dijkstra Single-SourceNon-Neg Weights Adj List 𝑂((𝑉 + 𝐸)log𝑉) 𝑂(𝑉) All-pairs 1 All-Pairs Adj Matrix 𝑂(𝑉4) 𝑂(𝑉2) All-pairs 2 All-Pairs Adj Matrix 𝑂(𝑉3log𝑉) 𝑂(𝑉2) Floyd-Warshall All-Pairs Adj Matrix 𝑂(𝑉3) 𝑂(𝑉2) Shortest Paths All algorithms, except Dijkstra, allow negative weights, but there can NOT be negative cycle. Shortest path problem: Find the shortest path from s to t. Single-source shortest path: Find the shortest path from s to every node. Lemma (Cut and Paste Argument): Let 𝑃 = (𝑠, … , 𝑢, ... , 𝑡) be the shortest 𝑠 − 𝑡 path. The n the subpaths 𝑃1 = (𝑠, … , 𝑢) and 𝑃2= (𝑢, . . 𝑡) must also be, respectively, shortest 𝑠 − 𝑢 and 𝑢 − 𝑡 paths. Relaxation: Let v.d be the shortest distance found so far from the starting node s to node v, and v.p be the last node in the current shortest path from s to node v. 1544956501161 Single Source Shortest Path Let ss(s,v) store shortest path distance from s to v Bellman-Ford Algorithm Initially, set n.d = inf for all nodes. Except starting node s.d = 0 Relax all edges once ... path length maximum 1 Relax all edges second time ... path length maximum 2 A path may have at most V-1 edges. So we it V-1 times. Finally, n.d is the actual shortest distance from s to n Basic Implementation 1234567Shortest-Path(G,s):for each node n in V do n.d &#x3D; infs.d &#x3D; 0for i &#x3D; 1 to V-1 for each edge (u,n) in E Relax(u,n) Running time: Θ(𝑉𝐸) Space: Θ(𝑉) Efficient Implementation Take advantage of dynamic programming. Recurrence: Suppose (u,n) is the last edge of shortest path of length at most i from s to n. By cut and paste argument, the subpath from s to u must be a shortest path of length at most i - 1, followed by (u,n). So, for all i &gt; 0 and n != s n.d[i] = min{ u.d[i-1] + w(u,n) } u in V, (u,n) in E Final solution: n.d[m-1] 12345678910Bellman-Ford(G,s):for each node n in V n.d &#x3D; inf, n.p &#x3D; nils.d &#x3D; 0for i &#x3D; 1 to V-1 for each node u in V if u.d changed in previous iteration then for each un in Adj(u) Relax(u,un) if no un.d changed in this iteration &#x3D;&gt; terminate Running time: Θ(𝑉𝐸) Space: Θ(𝑉) Shortest Paths in a DAG By subpath optimality, we have ss(s,n) = min { ss(s,u) + w(u,n) } u in V, (u,v) in Es Unlike Bellman-Ford, each edge will only be relaxed once. We need to ensure that when n is processed, ALL u with (u,n) in E have already been processed. We can do that by processing n (also ss(s,n)) in the topological order of the nodes. 123456789DAG-Shortest-Path(G,s)topologically sort the vertices of Gfor each vertex n in V n.d &#x3D; inf n.p &#x3D; nils.d &#x3D; 0for each vertex u in topological order for each vertex un in Adj(u) Relax(u,un) Running time: Θ(𝑉+𝐸) 1545052561597 Dijkstra Algorithm NOT allow negative weights. Maintain a set of explored nodes S for which we know u.d = ss(s,u)by variable status Initialize S = {s}, s.d = 0, v.d = inf Use a Min priority queueQ on V with d as key Key Lemma If all edges leaving S are relaxed, then v.d = ss(s,v),where v is the vertex in V-S with the minimum v.d So this v can be added to S, then repeat pseudocode 123456789101112Dijkstra(G,s):for each node n in V do n.d &#x3D; inf, n.p &#x3D; nil, n.status &#x3D; unknowns.d &#x3D; 0create a min priority queue Q onV with d as keywhile Q not empty % E times u &#x3D; Extract-Min(Q) % O(logV) time u.status &#x3D; over for each un in Adj(u) do if un.status &#x3D; unknown then Relax(u,un) Decrease-Key(Q,un,un.d) Running time: 𝑂(𝐸log𝑉) Very similar to Prim's algorithm 1545052597485 All-Pairs Shortest Paths Input: Directed graph G = (V,E) Weight w(e) = length of edge e Output: ss(u,v), for all pairs of nodes u, v A data structure from which the shortest path from 𝑢 to 𝑣 can be extracted efficiently, for any pair of nodes 𝑢, 𝑣 Note: Storing all shortest paths explicitly for all pairs requires 𝑂(𝑉3) space. Graph Representation: Assume adjacency matrix: 𝑤(𝑢, 𝑣) can be extracted in 𝑂(1) time. 𝑤(𝑢, 𝑢)= 0, 𝑤(𝑢, 𝑣)= ∞if there is no edge from 𝑢 to 𝑣. If the graph is stored in adjacency lists format, can convert to adjacency matrix in 𝑂(𝑉2) time. Using Previous Algorithms NOT negative cost edges: Dijkstra's algorithm Running time: 𝑂(𝐸 log 𝑉), totally 𝑂(𝑉𝐸 log 𝑉) Space: 𝑂 (𝑛3 log 𝑛 ) for dense graphs HAVE negative cost edges: Bellman-Ford Running time: 𝑂(𝑉𝐸), totally 𝑂(𝑉2𝐸) Space: 𝑂 (𝑛4) for dense graphs First DP Formulation Define d(i,j,m) = length of the shortest path from i to j that contains at most m edges. Use D[m] to denote the matrix [d(i,j,m)] Recurrence( essentially the same as in Bellman-Ford): d(i,j,m) = min { d(i,k,m-1) + w(k,j) } k from 1 to n initially d(i,j,1) = w(i,j) pseudocode 1234567891011Slow-All-Pairs-Shortest-Paths(G):d(i,j,1) &#x3D; w(i,j) for all 1&lt;&#x3D;i,j&lt;&#x3D;nfor m &#x3D; 2 to n-1 let D[m] be a new n*n matrix for i &#x3D; 1 to n for j &#x3D; 1 to n d(i,j,m) &#x3D; inf for l &#x3D;1 to n if d(i,k,m-1) + w(k,j) &lt; d(i,j,m) then d(i,j,m) &#x3D; d(i,k,m-1) + w(k,j)return D[n-1] Running time: 𝑂(𝑛4) Space: 𝑂(𝑛3) can be improved to 𝑂(𝑛2) Second DP Formulation Observation To compute d(i,j,m), instead of looking at the last stop before j, we look at the middle point. This cuts down the problem size by half. Thus, to calculate D[1],D[2],D[4],D[8],... Note that overshooting D[n-1] is OK. Since D[n'] , n'&gt;n -1 has the shortest paths with up to n' edges. It will not miss any shortest path with up to n-1 edges. Recurrence d(i,j,2s) = min { d(i,k,s) + d(k,j,s) } k from 1 to n initially d(i,j,1) = w(i,j) Analyze Running time: 𝑂(𝑛3) for each matrix , totally 𝑂(𝑛3log𝑛) Floyd-Warshall Define d(i,j,k) = length of the shortest path from i to j that all intermediate vertices on the path (if any) are in the set {1,2,...,k} Observation When computing d(i,j,k) there are two cases: k is not a node on the shortest path from i to j =&gt; then the path uses only vertices in {1,2,...,k-1} k is an intermediate node on the shortest path from i to j =&gt; path can be spilt into shortest subpath from i to k and a subpath from k to j Both subpaths use only vertices in {1,2,...,k-1} Recurrence 1544967760217 pseudocode 1234567891011Floyd-Warshall(G):d(i,j,0) &#x3D; w(i,j) for all 1&lt;&#x3D;i,j&lt;&#x3D;nfor k&#x3D;1 to n let D[k] be a new n*n matrix for i &#x3D; 1 to n for j &#x3D; 1 to n if d(i,k,k-1) + d(k,j,k-1) &lt; d(i,j,k-1) then d(i,j,k) &#x3D; d(i,k,k-1) + d(k,j,k-1) else d(i,j,k) &#x3D; d(i,j,k-1)return D[n] Running time: 𝑂(𝑛3) Space: 𝑂(𝑛3) but can be improved to 𝑂(𝑛2) Surprising discovery: If we just drop all third dimension. i.e. the algorithm just uses n*n array D, the algorithm still works! Maximum Flow Input: A directed connected graph 𝐺 =(𝑉, 𝐸) , where every edge 𝑒 ∈ 𝐸 has a capacity 𝑐(𝑒); a source vertex 𝑠 and a target vertex 𝑡. Output: A flow 𝑓: 𝐸 → 𝐑 from 𝑠 to 𝑡, such that For each 𝑒 ∈ 𝐸, 0 ≤ 𝑓(𝑒) ≤ 𝑐(𝑒) (capacity) For each 𝒗 ∈ 𝑽 − {𝒔, 𝒕}, sumOut( 𝒇(𝒆) ) = sumInto( 𝒇(𝒆) )(conservation)、 Define: The value of a flow is |𝑓| = sum(𝑓(𝑠, 𝑣))= sum(𝑓(𝑣, 𝑡)) where 𝑣 in V s-t Cut 1544969825169 Residual Graph 1544970376338 Ford Fulkerson Algorithm Start with f(e) = 0 for all edges e in E Construct Residual Graph Gf for current flow f(e) = 0 while there exists some s-t path P in Gf Let capacity of flow cf(p) = min { cf(e): e in P} This is the maximum amount of flow that can be pushed through residual capacity of P's edges Push c(f,p) units of flow along the edges e in P by adding cf(p) to f(e) for every e in P Construct Residual Graph Gf for new current flow f(e) When algorithm gets stuck, current flow is maximal! 12345678910111213141516Ford-Fulkerson(G,s,t):for each (u,n) in E do f(u,n) &#x3D; 0 cf(u,n) &#x3D; c(e) cf(n,u) &#x3D; 0while there exists path P in residual graph Gf do cf(p) &#x3D; min &#123;cf(e):e in P&#125; for each edge (u,n) in P do if (u,n) in E then f(u,n) &#x3D; f(u,n) + cf(p) cf(u,n) &#x3D; cf(u,n) - cf(p) cf(n,u) &#x3D; cf(n,u) + cf(p) else f(n,u) &#x3D; f(n,u) - cf(p) cf(n,u) &#x3D; cf(n,u) + cf(p) cf(u,n) &#x3D; cf(u,n) - cf(p) 1544971779763 Construct Residual Graph Gf 1544971840720 capacity of flow in 8, write back to G 1544971890428 The Gf in next iteration is 1544971927243 Until there is no s-t path in Gf. Current flow is optimally maximal. Applications he Max Flow setup can model (surprisingly) many (seemingly) unrelated problems. The idea is to express the problem as a max flow and then feed individual instances into out max flow solver. The examples below all share the property that they are integer flow problems, e.g., al capacities are integral, so running-time analyses can use FF bound for integral flows. Edge-Disjoint Paths Define: Two paths are edge-disjoint if they have no edge in common. 1544972200654 Circulations with Demands Given a number of source vertices 𝑠1, 𝑠2, …, each with a supply of 𝑠𝑢𝑝(𝑠𝑖) and a number of target vertices 𝑡1, 𝑡2, …, each with a demand of 𝑑𝑒𝑚 𝑡𝑖 ; sum of supply &gt;= sum of demand Need a flow meets all demands. Solution: Add a super source and a super target Maximum Bipartite Matching A Matching is a subset M ⊆ E such that: ∀v ∈ V at most one edge in M is incident upon v. The Size |M| is the number of edges in M. A Maximum Matching is matching M such that: every other matching Mʹ satisfies |Mʹ | ≤ M. Given bipartite graph G, find a Maximum Matching. Formulation: Create directed graph 𝐺′ = (𝑋 ∪ 𝑌 ∪ {𝑠, 𝑡}, 𝐸′ ). Direct all edges from 𝑋 to 𝑌, and assign them capacity 1. Add source 𝑠, and unit capacity edges from 𝑠 to each node in 𝑋. Add target 𝑡, and unit capacity edges from each node in 𝑌 to 𝑡. Theorem: Max cardinality matching in 𝐺 = value of max flow in 𝐺′. 1544973114900 Running time: 𝑂(𝑉𝐸) Baseball Elimination 1544973389780 Input： 𝑛 teams: 1, 2, … , 𝑛 One particular team, say 𝑛 (without loss of generality) Team 𝑖 has won 𝑤𝑖 games already Team 𝑖 and 𝑗 still need to play 𝑟𝑖𝑗 games, 𝑟𝑖𝑗 = 0 or 1. Team 𝑖 has a total of 𝑟𝑖 = sum(𝑟𝑖𝑗: 𝑗) games to play Output: “Yes”, if there is an outcome for each remaining game such that team 𝑛 finishes with the most wins (tie is OK). “No”, if no such possibilities. 1544973553493 Claim: There is a way for team n to finish in the first place iff the max flow has value of the sum of supply from source s]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Minimum Spanning Tree]]></title>
    <url>%2F2018%2F12%2F15%2FMinimum-Spanning-Tree%2F</url>
    <content type="text"><![CDATA[1544942034490 Uniqueness of MST Cut Lemma Let S be any subset of nodes let e be the min cost edge with exactly one endpoint in S. Then every MST must contain e. Proof Let T* be some MST Consider any edge e in T* Removing e from T* breaks T* into two parts Sand V-S e must be the min cost edge crossing the cut (S,S-V) Applying the cut lemma on S, every MST must contain e Apply the above arguments to every edge in T*, we have Every edge e in T* must be contained in every MST Prim's Algorithm 1544950977399 1544951001949 Idea Initialize explored set S = { any one node } Add min cost edge e = (u,n) with u in S and n in V-S to T Add n to S Repeat until S = V Feature Maintain set of explored nodes S A Min priority queue Q to keep unknown nodes. key is their cheapest edge to node inS pseudocode 12345678910111213Prim(G,r):for each n in V do n.key &#x3D; inf, n.p &#x3D; nil, n.status &#x3D; unknownr.key &#x3D; 0create a min priority queue Q on Vwhile Q not empty u &#x3D; Extract-Min(Q) % need O(logV) time u.status &#x3D; over for each n in Adj(u) do if n.status &#x3D; unknown and w(u,n)&lt;n.key then n.p &#x3D; u n.key &#x3D; w(u,n) Decrease-Key(Q,n,w(u,n)) Running time: 𝑂(𝐸log𝑉) Kruskal's Algorithm Idea Start with an empty tree T Consider edges in ascending order of weight. Case 1: If adding e to T create a cycle, discard e Case 2: Otherwise, insert e = (u,v) into T according to cut Lemma Union-Find Data Structure Key Question How to check whether adding e to T will create a cycle? DFS take 𝑂(𝐸⋅𝑉) time in total. Can we do better in 𝑂(log𝑉) time? After an edge e is added, two sets union together Need such a &quot;union-find&quot;data structure: Find-Set(u): For a given node u, find which set this node belongs to. Union(u,v): For two given nodes u and v, merge the two sets containing u and v together. Set as A Tree The trees in the union-find data structure are NOT the same as the partial MST trees! The root of the tree is the representative node of all nodes in that tree (i.e., use the root’s ID as the unique ID of the set). Every node (except the root), has a pointer pointing to its parent. The root has a parent pointer to itself. No child pointers (unlike BST), so a node can have many children. 12Make-Set(x):x.parent &#x3D; x, x.hight &#x3D; 0 123456Find-Set(x):x.height &#x3D; 0while x!&#x3D; x.parent do x.height &#x3D; x.height + 1 x&#x3D; x.parentreturn x 12345678Union(x,y):a &#x3D; Find-Set(x)b &#x3D; Find-Set(y)if a.height &lt;&#x3D; b.height then if a.height &#x3D; b.height then b.height &#x3D; b.height + 1 a.parent &#x3D; belse b.parent &#x3D; a Path Compression while Find-Set(x) we can update its parent pointer to compress the path. 1544954929318 pseudocode 12345678MST-Kruskal(G):for each node n in V Make-Set(n)sort the edges of G into increasing order by weight % O(ElogE)for each edge (u,n) in E taken in the above order if Find-Set(u) !&#x3D; Find-Set(v) then % O(E) output edge (u,n) Union(u,n) Running time: 𝑂(𝐸log𝐸+𝐸log𝑉)=𝑂(𝐸log𝑉) Note: If edges are already sorted(𝑂(𝐸log𝐸)) and we use path compression, then the running time is close to 𝑂(𝐸)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph Basic and Algorithm]]></title>
    <url>%2F2018%2F12%2F15%2FGraph-Basic-and-Algorithm%2F</url>
    <content type="text"><![CDATA[Basic node: vertex edge: connection between nodes Undirected Edges have no direction (or both directions) deg (𝑣) = # edges at 𝑣 sum of deg(𝑣) = 2𝐸 Directed Edges have directions If an edge has both directions, we will use two edges with opposite directions degout (𝑣) = # edge leaving 𝑣; degin (𝑣 ) = # edge entering 𝑣. sum of deg𝑜𝑢𝑡(𝑣) = sum of deg𝑖𝑛(𝑣) = 𝐸 Path A path in a (directed or undirected) graph 𝐺 = (𝑉, 𝐸) is a sequence 𝑃 of nodes 𝑣1, 𝑣2, … , 𝑣𝑘−1, 𝑣𝑘 such that (𝑣𝑖, 𝑣𝑖+1) is an edge. The length of the path is 𝑘 − 1 (i.e., # edges in the path). A path is simple if all nodes are distinct. Connectivity An undirected graph is connected if for every pair of nodes 𝑢 and 𝑣, there is a path between 𝑢 and 𝑣. Theorem: For a connected graph, 𝐸 ≥ 𝑉 − 1. Cycle A cycle is a path v1, v2, … , vk-1, vk in which v1 = vk , k &gt; 2 A cycle is simple if the first 𝑘 − 1 nodes are all distinct. Data structure Adjacency Nodes Adjacency List A node-indexed array of lists. Given node 𝑢, retrieving all neighbors in Θ(deg(𝑢)) time Given 𝑢, 𝑣, checking if (𝑢, 𝑣) is an edge takes Θ(deg(𝑢)) time. Space: Θ(𝑉 + 𝐸). Adjacency Matrix A 𝑉 × 𝑉 matrix. Given node 𝑢, retrieving all neighbors in Θ (𝑉) time Given 𝑢, 𝑣, checking if (𝑢, 𝑣) is an edge takes 𝑂(1) time. Space: Θ(𝑉2). Trees connected &amp;&amp; no cycle =&gt; tree no cycle =&gt; forest(several trees) After we have run BFS or DFS on an undirected graph, the edges can be classified into 3 types: Tree edges: traversed by the BFS/DFS Back edges: connecting a node with one of its ancestors(other than its parent) Cross edges: connecting two nodes with no ancestor/descendent relationship. Algorithm BFS span a tree with NO back edges pseudo-code 1234567891011initialize an empty queue QEnqueue(Q,r)while Q not empty do n &#x3D; Dequeue(Q) for each v in Adj(n) if v.status &#x3D; unknown v.status &#x3D; processing v.d &#x3D; n.d + 1 v.p &#x3D; n Enqueue(Q,v) n.status &#x3D; over Running time: Θ(𝑉+𝐸), which is Θ𝐸 if the graph is connected Application Find connected components. DFS span a tree with NO cross edges pseudo-code 123456789101112DFS(G)for each vertex n in V do if n.status &#x3D; unknown then DFS-Visit(n)DFS-Visit(n):n.status &#x3D; processingfor each v in Adj(n) do if v.status &#x3D; unknown the v.p &#x3D; n DFS-Vist(v)n.status &#x3D; over Running time: Θ(𝑉+𝐸) Application Cycle detection Topological Sort A topological ordering of a graph is a linear ordering of the vertices of a DAG(Directed Acyclic Graph) such that if (u,v) is in the graph, u appears before v in the linear ordering. idea Output a vertex u with in-degree zero in current graph Remove u and all edges (u,v) from current graph If the graph is not empty, go to step 1 pseudo-code 1234567891011121314Initialize Q to be an empty queuefor each u in V do If inDegree(u) &#x3D; 0 then % find all starting vertices Enqueue(Q,u)while Q not empty u &#x3D; Dequeue(Q); Output u for each v in Adj(u) % remove u&#39;s outgoing edges inDegree(v) &#x3D; inDegree(v) - 1 if inDegree(v) &#x3D; 0 then Enqueque(Q,v)return Running time: 𝑂(𝑉+𝐸)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming]]></title>
    <url>%2F2018%2F12%2F15%2FDynamic-Programming%2F</url>
    <content type="text"><![CDATA[Main idea of DP: Analyze the structure of an optimal solution Recursively define the value of an optimal solution Compute the value of an optimal solution (usually bottom-up) @[TOC] One-Dimension Stairs Climbing 1D You can climb 1 or 2 stairs with one step. How many different ways can you climb n stairs? Recurrence F(n) = F(n-1) + F(n-2) Base case F(1) = 1, F(2) = 2 pseudo-code 1234567F(n):allocate an array A of size nA[1] &#x3D; 1A[2] &#x3D; 2for i&#x3D;3 to n A[i] &#x3D; A[i-1] + A[i-2]return A[n] Analyze Running time: Θ(𝑛) Space: Θ(𝑛) but can be improved to Θ(1) by freeing array. Rod Cutting Problem Given a rod of length n and prices pi for i = 1,...,n, where pi is the price of a rod of length i . Find a way to cut the rod to maximize total revenue. length i 1 2 3 4 5 6 7 8 9 10 price pi 1 5 8 9 10 17 17 20 24 30 Recurrence total optimal revenue is price of cut rod and optimal revenue of remaining rood. rn = max{pn, p1 + rn-1, p2 + rn-2, ..., pn-1+r1} Base case Define rn be the maximum revenue obtainable from cutting a rod of length n. r1 = p1 pseudo-code 123456789101112131415cutRod(n):let r[0...n] and s[0...n] be new arraysr[0] &#x3D; 0 for j&#x3D;1 to n % every optimal length q &#x3D; -inf for i &#x3D;1 to j % every cut length if q &lt; p[i] + r[j-i] then q &#x3D; p[i]+r[j-i] % keep track the optimal cut length s[j] &#x3D; i r[j] &#x3D; qj &#x3D; n while j&gt;0 do print s[j] j &#x3D; j-s[j] i 0 1 2 3 4 5 6 7 8 9 10 p[i] 0 1 5 8 9 10 17 17 20 24 30 r[i] 0 1 5 8 9 13 17 18 22 25 30 s[i] 0 1 2 3 2 2 6 1 2 3 10 Analysize Running time: Θ(𝑛2) Space: Θ(𝑛) Weighted Interval Scheduling 1D Jobs j starts at sj, finish at fj and has weight(value) vj. Two jobs compatible if they don't overlap. Goal: find maximum-weight subset if mutually compatible jobs. Recurrence Firstly, label all jobs by finishing time. Maximum subset is either DO take this job and maximum subset of compatible jobs set NOT take this job and maximum subset of remaining jobs V[j] = max{vj + V[c(j)], V[j-1]} function c(j) return the largest index i&lt;j such that job i is compatible job j. Base case The goal is to find a subset of a set. We start from a empty set. pseudo-code 12345678910111213141516schedule():sort all jobs by finish timeV[0] &#x3D; 0for i &#x3D; 1 to n % DO take job j if V[i] + V[c(i)] &gt; V[j-1] then V[i] &#x3D; value[i] + V[c(i)] keep[i] &#x3D; 1 % NOT take job j else V[i] &#x3D; V[i-1] keep[i] &#x3D; 0 i &#x3D; nwhile i &gt; 0 do if keep[i] &#x3D;1 then print i, i &#x3D; c(i) else i &#x3D; i-1 Analyze Running time: Θ(𝑛log𝑛) Space: Θ(𝑛) Two-Dimension Sometimes sub-problem also need to use 1D-DP to solve. Sometimes there are two variables that requires a 2D array. The 0/1 Knapsack Problem A set of n items, where item i has weight wi and value vi ,and a knapsack with capacity W. Find x1,... ,xn (either 0 or 1) such that sum(xiwi) &lt;= W and V=sum(xivi) is maximum Recurrence Start to put items into the knapsack.--1D Maximum V of capacity W is the max among whether take each item. vi + maximum XV of capacity j-wi We found it necessary to consider capacity of knapsack.--2D V[j] = max{v(i) + V[j-w(i)]} i from 1 to n WRONG: This may pick the same item more than once! Thus, both in vi and V[j-w(i)]! New def: let V[i,j] be the largest obtained value with capacity j, choosing ONLY from the first i items. Below formula truly reflect whether. Left is NOT, right is YES. V[i,j] = max{V[i-1,j], vi + V[i-1,j-w(i)]} i from i to n Base case We start from take nothing: i= 0, V[0,j] = 0 and empty knapsack j =0, V[i,0] = 0 pseudo-code 1234567891011121314let V[0...n,0...W] and keep[0...n,0...W] be new arrays of all 0 for i &#x3D; 1 to n do % put things for j &#x3D; 1 to W do % each capacity if w[i]&lt;&#x3D;j and v[i]+V[i-1,j-w[i]] &gt; V[i-1,j] then V[i,j] &#x3D; v[i]+V[i-1,j] keep[i,j] &#x3D; 1 else V[i,j] &#x3D; V[i-1,j] keelp[i,j] &#x3D; 0K &#x3D; Wfor i &#x3D; n downto 1 do if keep[i,K] &#x3D; 1 then print i K &#x3D; K-w[i] input: i 1 2 3 4 vi 10 40 30 50 wi 5 4 6 3 running: V[i,j] 0 1 2 3 4 5 6 7 8 9 10 i=0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 10 10 10 10 10 10 2 0 0 0 0 40 40 40 40 40 40 40 3 0 0 0 0 40 40 40 40 40 50 70 4 0 0 0 50 50 50 50 90 90 90 90 Analyze Running time: Θ(𝑛𝑊) Space: Θ(𝑛𝑊) without keep array can be improved to Θ(𝑛+𝑊). Longest Common Subsequence LCS Given two sequences X = (x1,x2,...,xm) and Y = (y1,y2,...,yn), we say that Z is a common subsequence of X and Y if Z has a strictly increasing sequence of indices i and j of both X and Y such that we have xip= yjp = zp for all p = 1, 2, … , k. The goal is to find the longest common subsequence of X and Y. 1544897951063 Recurrence Let c[i,j] be the length of the longest common subsequence of X[1...i] and Y[1...j]. So we need 2D that i for X and j for Y. Firstly, go through i, and then go through j If X[i] = Y[j], we match. c[i,j] = 1 + the longest common subsequence of X[1...i-1] and Y[1...j-1]. else , it's max of the longest common subsequence of X[1...i-1] and Y[1...j] and the longest common subsequence of X[1...i] and Y[1...j-1] If X[i] = Y[j] , c[i,j] = max{ c[i,i-1], c[i-1,j] } else c[i,j]= c[i-1,j-1] +1 Base case c[0,j] =0 c[i,0] = 0 pseudo-code 1234567891011121314151617181920212223LSC(X,Y):let c[0...m,0...n] and b[0...m,0...n] be new arrys of all 0for i&#x3D;1 to m for j&#x3D;1 to n if X(i) &#x3D; Y(i) then c[i,j] &#x3D; c[i-1,j-1]+1 b[i,j] &#x3D; &quot;↖&quot; else if c[i-1,j] &gt;&#x3D; c[i,j-1] then c[i,j] &#x3D; c[i-1,j] b[i,j] &#x3D; &quot;↑&quot; else c[i,j] &#x3D; c[i,j-1] b[i,j] &#x3D; &quot;←&quot;Print-LCS(b,m,n)Print-LCS(b,i,j): if i&#x3D;0 or j&#x3D;0 then return if b[i,j]&#x3D;&quot;↖&quot; then Print-LCS (b,i−1,j−1) print X(i) else if 𝑏[𝑖,𝑗]&#x3D;&quot;↑&quot; Print-LCS( b,i−1,j) else Print-LCS( b,i,j−1) 1544897610939 Analyze Running time: Θ(𝑚𝑛) Space: Θ(𝑚𝑛) without b array can be improved to Θ(𝑚+𝑛). Longest Common Substring Given two strings X = x1x2...xm and Y = y1y2...ym, we wish to find their longest common substring Z, that is, the largest k for which there are indices 𝑖 and 𝑗 with xixi+1...xi+k-1 = yjyj+1...yj-k-1 1544898031881 Recurrence Let d[i,j] keep track of k, the longest string length of X[1...i] Y[1...j]. Firstly go through X and then Y so we need 2D If X(i) = Y(j), d[i,j] = 1 + the longest string length of X[1...i-1] Y[1...j-1]. else it's 0! If X(i) = Y(j), d[i,j] = 1 + d[i-1.j-1] else d[i,j] = 0 Base case d[0,j] = 0, d[i,0]= 0 pseudo-code 1234567891011let d[0...m,0...n] be a new array of all 0length &#x3D; 0, endIndex &#x3D; 0for i &#x3D; 1 to m for j &#x3D; 1 to n if X(i) &#x3D; Y(i) then d[i,j] &#x3D; d[i-1,j-1] + 1 if d[i,j] &gt; length then length &#x3D; d[i,j] endIndex &#x3D; ifor i &#x3D; endIndex - length + to endIndex print X(i) Analyze Running time: Θ(𝑚𝑛) Space: Θ(𝑚𝑛) but can be improved to Θ(𝑚+𝑛). Over Intervals Goal is to find optimal (min or max ) solution on problem with Problem of size n Ordered input of items 1,2,...n Define substructures as Ordered input of items i..j Problem of size j-i+1 Recurrence gives optimal solution of subproblem as function of optimal solution of smaller subproblems Algorithm fills in DP table from smallest to largest problem size Often, final subproblem filled is solution for original problem Sometimes, solution of original problem is min/max over table values Longest Palindromic Substring A palindrome is a string that reads the same backward or forward. Given a string X = x1x2...xn, find the longest palindromic substring. Ex: X =ACCABA Palindromic substrings: CC, ACCA ABA Longest palindromic substrings: ACCA Recurrence Let p[i,j] be true if and only if X[i...j] is a palindrome. Obviously, we need 2D, though i &lt;= j. It's like a half plane. If X[i] = X[j], p[i,j] is true iff p[i+1,j-1] is true. If X(i) = X(j), p[i,j] = p[i+1,j-1] Order: from (i,j) to (i+1,j-1) it's a diagonal path. Base If i = j, p[i,j] = true pseudo-code 12345678910111213141516let p[0...n,0...n] be a new array of all falsemax &#x3D; 1 for i &#x3D; 1 to n-1 do p[i,i] &#x3D; true if X(i) &#x3D; X(i+1) then p[i,i+1] &#x3D; true max &#x3D; 2% updating along diagonal% started from the third diagonalfor k &#x3D; 3 to n do for i &#x3D; 1 to n-k+1 do j &#x3D; i+k-1 if p[i+1,j-1] &#x3D; true and X(i) &#x3D; X(j) then p[i,j] &#x3D; true max &#x3D; kreturn max Analyze Running time: 𝑂(𝑛2) Space: 𝑂(𝑛2) but can be improved to 𝑂(𝑛) Optimal BST Given n keys a1 &lt; a2 &lt; ... &lt; an, with weights f(a1), … , f(an), find a binary search tree T on these n keys such that B(T) = sum{ f(ai)*(d(ai)+1) } i from 1 to n is minimized, where d(ai) is the depth of ai. Recurrence Let Ti,j be some tree on the subset of nodes ai &lt; ai+1 &lt; ... &lt; aj Define w[i,j] = f(ai) + ... + f(aj) The cost is defined as B(Ti,j) = sum{ f(ai)*(d(ai)+1) } from i to j Define e[i,j] = optimal value of B(Ti,j) The optimal cost of Ti,j is The optimal cost of left subtree + The optimal cost of right subtree + root's weight e[i,j] = e[i,k-1 + e[k+1,j] +w[i,j] To find k, try every value between i and j to figure out the min. e[i,j] = min{ e[i,k-1 + e[k+1,j] +w[i,j] } for i&lt;=k&lt;=j Order: (i,j) (i,k-1) (k+1,j) Base e[i,j]= 0 for i&gt;j e[i,i] = f(ai) for all i pseudo-code 1234567891011121314151617181920212223242526Optimal-BST(a,n):let e[1...n,1...n],w[1...n,1...n],root[1...n,1...n] be new arrays of all 0for i &#x3D; 1 to n w[i,i] &#x3D; f(a(i)) for j &#x3D; i + 1 to n % complete the w[] table w[i,j] &#x3D; w[i,j-1] + f(a(i))for l &#x3D; 1 to n for i &#x3D; 1 to n-l+1 j &#x3D; i+l-1 e[i,j] &#x3D; inf % find k minimizes e[] for k &#x3D; i to j t &#x3D; e[i,k-1]+e[k+1,j]+w[i,j] if t &lt; e[i,j] then e[i,j] &#x3D; t root[i,j] &#x3D; kreturn Construct-BST(root,1,n)Construct-BST(root,i,j):if i &gt; j then return nilcreate a node zz.key &#x3D; a[root[i,j]]z.left &#x3D; Construct-BST(root,i,root[i,j]-1)z.right &#x3D; COnstruct-BST(root,root[i,j]+1,j)return z Analyze Running time: Optimal 𝑂(𝑛3) Construct 𝑂(𝑛2) Space 𝑂(𝑛2)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Finite State Machine]]></title>
    <url>%2F2018%2F12%2F08%2FFinite-State-Machine%2F</url>
    <content type="text"><![CDATA[State Machine State: A set of particular condition that the machine is in at a specific time FSM is a generic synchronous sequential circuit FSM has finite state memory, inputs &amp; outputs State memory is implemented using flip-flops State transition logic implemented using combinational circuit Output logic implemented using combinational circuit Output depends on state only - Moore Output depends on state and input - Mealy State equations Describe the behavior of a clocked sequential circuit algebraically. e.g. A(t+1) = DA(t) = A(t)x(t) + B(t)x(t) Simple form: A(t+1) = Ax + Bx or A+ = Ax + Bx State Diagram and Transition Tables 1544317606823 The diagram is a Mealy Machine Moore Machine vs. Mealy Machine Moore Machine The output value is inside the state bubbles. Outputs are function solely of the current states. Outputs change synchronously with states. Mealy Machine The output value is on the transition edge. Outputs depend on state AND inputs Change of inputs causes an immediate change of outputs. Example 1544320429228 Basic Design Steps of FSM Understand the Specifications with a block diagram Obtain an abstract specification of the FSM in state-transition diagram or table Perform state reduction equivalent states can be merged Perform state assignment assign binary values to the state in a way that next-state logic can be simplified Choose FF to implement the FSM state register D-FF : Q+ = D T-FF : Q+ = TQ' + T'Q T = 0 hold ;T = 1 toggle; JK-FF: Q+ = JQ' + K'Q | JK | Data | function | | :--: | :--: | :------: | | 0/0 | 1 | hold | | 0/1 | 0 | reset | | 1/0 | Q | set | | 1/1 | Q' | toggle | Implement the FSM design of next-state and output logic Happy DEBUGGING!! FSM Implementation Procedure Start with a state diagram (bubble diagram) Describes the Finite State Machine according to the specification Reduce the number of states if necessary/possible (state reduction) equivalent-states Two internal states are said to be equivalent if for each input combination they give exactly the same output AND send the circuit to the same(equivalent) state. Decide on the State Encoding (how many flip-flops to use and what should be the FF outputs be for each state) Produce the binary-code state table. Decode what kind of FFs to use. D-type FFs are normally used (JK or T are more complicated and may give you smaller circuit if you play with TTLs) Determine the FF input equations (use FF excitation rules for JK and T type FFs) and general output equations from the transition tables Implement the next state logic, output logic using combinational circuit techniques Draw the complete logic diagram]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>FSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VHDL]]></title>
    <url>%2F2018%2F12%2F08%2FVHDL%2F</url>
    <content type="text"><![CDATA[Entity Identify a distinct logic block Consists of the input and output ports (I/O pins) Each port can have several variables 1234entity nand2 isport (a,b : in std_logic; y : out std_logic);end nand2; Ports can be in (read), out (write), inout (bi-directional) Std_logic is the type of signal being carried at the ports Std_logic can have values such as ‘0’, ‘1’, ‘X’, ‘Z’, ‘-’ and others. (' z ' means high impedance (tristate)) Another signal type is “bit” which only assumes binary values of ‘0’ and ‘1’. Architecture Follow immediately after the entity declaration, to define the internal operation (function) of the logic module Defines the relationship between the input and output signals May have internal signals in the definition AN entity can have several different architecture specifications to describe different views or different levels 12345678entity inverter_gate is port (A: in std_logic; Z: out std_logic);end inverter_gate;architecture DATAFLOW of inverter_gate isbegin z &lt;= not A after 10 ns;end DATAFLOW; 1234567Entity OR3 isport (I1,I2,I3: in std_logic; O: out std_logic);end OR3;architecture BEHAVIOR of OR3 isbegin O &lt;= I1 or I2 or I3;end BEHAVIOR; Internal signals used to simplify the model or define connections between sub-components inside the architecture 1234architecture example of special_mus is signal control, tmp : std_logic;beginend example; Concurrent Statements Signal Assignments Component Instantiations for structural description) Processes IN ORDER Signal Assignments 1C &lt;= '0'; Delayed 1c &lt;= A and B after 10ns; --Only for simulation. Selected (outside PROCESS) connect target to many cases of one source. 1234with sel select DAta_out &lt;= a when '0', b when '1', 'X' when others; Conditional (outside PROCESS) connect target to many cases of several sources. 123Y &lt;= a when en = ‘1’ else ‘Z’ when en = ‘0’ else ‘X’; Component Instantiations for structural description 123COMPONENT xor_gate PORT ( A, B: IN STD_LOGIC; C: OUT STD_LOGIC);END COMPONENT; Processes IN ORDER If-else Branch many signals many cases. 1234567if sth = '0' then A = '1'; B = '0'; -- IN-ORDER processeselsif sth = '1' then -- optional B = '1'; C = '0'; -- optionalelse -- IMPORTANT! C = '1';end if； case-when Branch one signal many cases. 12345case sth is when '0' =&gt; A = '1'; B = '0'; when '1' =&gt; B = '1'; C = '0'; when others =&gt; C = '1'; -- IMPORTANT!end case; Variable and Signal The variable is updated without any delay as soon as the statement is executed. Variables must be declared inside a process (and are local to the process). Assignment := executed sequentially! 123456789101112131415architecture VAR of EXAMPLE is signal TRIGGER, RESULT: integer :=0;begin process variable variable1: integer := 1; variable variable2: integer := 2; variable variable3: integer := 3; begin wait on TRIGGER; variable1 := variable2; --2 variable2 := variable1 + variable3; --2+3=5 variable3 := variable2; --5 RESULT &lt;= variable1 + variable2 + variable3; --2+5+5 end process;end VAR; The signal is updated after a time delay (delta delay if no delay is specified). Signals must be declared outside the process. Assignment &lt;= executed concurrently! 123456789101112131415architecture SIGN of EXAMPLE is signal TRIGGER, RESULT: integer :=0; signal signal1: integer := 1; signal signal2: integer := 2; signal signal3: integer := 3;begin process begin wait on TRIGGER; signal1 &lt;= signal2; --2 signal2 &lt;= signal1 + signal3; --1+3 signal3 &lt;= signal2; --2 RESULT &lt;= signal1 + signal2 + signal3; --1+2+3 end process;end VAR; Dataflow architecture Specify input/output relations in Boolean expression or conditional statements 1234567891011ENTITY 2_to_4_decoder IS PORT (A, B, E: IN STD_LOGIC; D: OUT STD_LOGIC_VECTOR (3 downto 0);END 2_to_4_decoder;ARCHITECTURE dataflow OF 2_to_4_decoder ISBEGIN D(0) &lt;=‘0’ WHEN (A=‘0’ AND B=‘0’ AND E=‘0’) ELSE ‘1’; D(1) &lt;=‘0’ WHEN (A=‘0’ AND B=‘1’ AND E=‘0’) ELSE ‘1’; D(2) &lt;=‘0’ WHEN (A=‘1’ AND B=‘0’ AND E=‘0’) ELSE ‘1’; D(3) &lt;=‘0’ WHEN (A=‘1’ AND B=‘1’ AND E=‘0’) ELSE ‘1’;END dataflow; Structural architecture Describes a set of interconnected components. 1234567891011121314151617ARCHITECTURE structural OF four_bit_adder_sub IS COMPONENT four_bit_adder PORT ( A: IN STD_LOGIC_VECTOR (3 downto 0); B: IN STD_LOGIC_VECTOR (3 downto 0); C0: IN STD_LOGIC; C4: OUT STD_LOGIC; S: OUT STD_LOGIC_VECTOR (3 downto 0)); END COMPONENT; COMPONENT xor_arrays PORT ( X: IN STD_LOGIC_VECTOR (3 downto 0); Z: IN STD_LOGIC; Y: OUT STD_LOGIC_VECTOR (3 downto 0)); END COMPONENT; signal Y: STD_LOGIC_VECTOR (3 downto 0);BEGIN Block1: xor_arrays PORT MAP (B, M, Y); Block2: four_bit_adder PORT MAP (A, Y, M, C, S);END structural; Behavioral architecture Any change in the values of Sensitivity List will cause immediate execution of the process 1234567891011121314151617ENTITY mux4 ISPORT ( S: IN STD_LOGIC_VECTOR (1 downto 0); I: IN STD_LOGIC_VECTOR (3 downto 0); Y: OUT STD_LOGIC);END mux4;ARCHITECTURE behavioral OF mux4 ISBEGIN PROCESS (S, I) -- Sensitivity List BEGIN CASE S IS WHEN “00” =&gt; Y &lt;= I(0); WHEN “01” =&gt; Y &lt;= I(1); WHEN “10” =&gt; Y &lt;= I(2); WHEN “11” =&gt; Y &lt;= I(3); END CASE; END PROCESS;END behavioral;]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>VHDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[COMP2611 Final Review]]></title>
    <url>%2F2018%2F12%2F07%2FCOMP2611-Final-Review%2F</url>
    <content type="text"><![CDATA[@[TOC] Arithmetic for Computers Overflow Detection Operation Sign bit of X Sign Bit of Y Sign Bit of Result X+Y 0 0 1 X+Y 1 1 0 X-Y 0 1 1 X-Y 1 0 0 Multiplication 1544252231121 1544252074288 Division 1544252200005 1544252118812 Pipeline Stages: IF : Fetch the instructions from meomry ID : Instruction decode &amp; register read EX : Perform ALU operation MEM : Memory access (if necessary) WB : Write result back to register Registers: located between the stages: ​ IF/ID, ID/EX, EX/MEM, MEM/WB Control: IF : no control signals ID : no control signals EX : RegDst, ALUOP, ALUSrc MEM : Branch, MemRead, MemWrite WB : MenToReg, RegWrite Hazards: cause: Data dependence and Control dependence. types: Structural hazards, Data hazards, Control hazards Structural hazards Conflict for use of memory(MEM/IF) and registers(WB/ID). Solution: separate instruction and data memories Fact: Register very fast. Registers can Write during 1st half of clock cycle and Read during 2nd half. Data hazards add $s0, $t0, $t1 sub $t2, $s0, $t3 Solution: Forwarding (partial, can't solve below code) lw \(s0, 20\)(t1) sub $t2, $s0, $t3 Scheduling (assume forwarding is used) Control hazards When we need to beq , bne operations Solution: Fetch instruction after branch. If wrong, flush them away. Add comparator to compare earlier Datapath with Hazards Data forwarding Two types forwarding: EX/MEM -&gt; ID/EX and MEM/WB -&gt; ID/EX Using two multiplexers to decide what is the input of operands A and B of the ALU. note: A is for Rs. B is for Rt. EX forwarding: 123456if (EX&#x2F;MEM.RegWrite and (EX&#x2F;MEM.RegisterRd ≠ 0) and (EX&#x2F;MEM.RegisterRd &#x3D; ID&#x2F;EX.RegisterRs )) ForwardA &#x3D; 10 if (EX&#x2F;MEM.RegWrite and (EX&#x2F;MEM.RegisterRd ≠ 0) and (EX&#x2F;MEM.RegisterRd &#x3D; ID&#x2F;EX.RegisterRt )) ForwardB &#x3D; 10 MEM forwarding (only when NOT EX forwarding ): 12345678910if (MEM&#x2F;WB.RegWrite and (MEM&#x2F;WB.RegisterRd ≠ 0) and not (EX&#x2F;MEM.RegWrite and (EX&#x2F;MEM.RegisterRd ≠ 0) and (EX&#x2F;MEM.RegisterRd &#x3D; ID&#x2F;EX.RegisterRs ))and (MEM&#x2F;WB.RegisterRd &#x3D; ID&#x2F;EX.RegisterRs )) ForwardA &#x3D; 01 if (MEM&#x2F;WB.RegWrite and (MEM&#x2F;WB.RegisterRd ≠ 0) and not (EX&#x2F;MEM.RegWrite and (EX&#x2F;MEM.RegisterRd ≠ 0) and (EX&#x2F;MEM.RegisterRd &#x3D; ID&#x2F;EX.RegisterRt ))and (MEM&#x2F;WB.RegisterRd &#x3D; ID&#x2F;EX.RegisterRt )) ForwardB &#x3D; 01 Load-use detection When Load-use hazard occur, we can do nothing but stall a clock cycle. To detect it: 123Load-use hazard &#x3D; ID&#x2F;EX.MemRead and ((ID&#x2F;EX.RegisterRt &#x3D; IF&#x2F;ID.RegisterRs ) or (ID&#x2F;EX.RegisterRt &#x3D; IF&#x2F;ID.RegisterRt )) How to Stall Force control values in ID/EX register to 0 : ​ EX, MEM and WB do EX, MEM and WB do nop (no -operation) Prevent update of PC and IF/ID register Using instruction is decoded again Following instruction is fetched again 1-cycle stall allows MEM to read data for lw (with forwarding) Data Hazard when branch In pipeline datapath, branch target address calculation is executed in another ALU NOT in EXE stage. It is executed during ID staged. ID Stage: Target address calculator, Register file, Register comparator. add $1, $2, $3 add $4, $5, $6 add $7, $8, $9 # another instruction beq $1, $4, target Can resolve using forwarding. From MEM/WB and EXE/MEM to ID add $1, $2, $3 add $4, $5, $6 beq stalled... beq $1, $4, target Need one cycle stalled. lw $1,addr beq stalled... beq stalled... beq $1, $0, target Need two cycles stalled. Memory RAM(Random Access Memory) technology includes two types: 1. Static RAM -- mostly used for cache 0.5ns-2.5ns, $2000 -$5000 per GB consists only Transistors 2. Dynamic RAM(DRAM) -- mostly used for main memory 50ns - 70ns, $20-$75 per GB consists of Transistor and Capacitor Magnetic disk: 5ms - 20ms, $0.2-$2 per GB Disk Sectors and Accesses Each sector records: Sector ID Data (512 bytes, 4096 bytes proposed) Error correcting code (ECC) Synchronization fields and gaps Access to a sector involves: Queuing delay if other accesses are pending Seek: move the heads Rotational latency Data transfer Controller overhead Principle of Locality Programs access a small proportion of their address space at any time Temporal locality Items accessed recently are likely to be accessed again soon Spatial locality Items near those accessed recently are likely to be accessed soon This property is the KEY to memory hierarchy! Memory Hierachy Initially, Instructions and data are loaded into DRAM from disk. Upon first access, A copy of the referenced instruction or data item is kept in cache In subsequent accesses, First, look for the requested item in the cache ​ If the item is in the cache, return the item to the CPU ​ If NOT in the cache, look it up in the memory We can say: If not found in this level, look it up at next level until found Keep (cache) a copy of the found item at this level after use Cache Each memory locations is mapped to ONE location in the cache. average latency = hit + miss = r*t1 + ((1-r)t1 + rt2) = t1 + (1-r)t2 NOT r*t1 + (1-r)t2 Three basic organizations: Direct-mapped( one memory block to one possible cache block) Set-mapped( one memory block to one set of possible cache blocks) Fully-mapped( one memory block to all possible cache blocks) Direct Mapped Cache If the number of cache blocks (N) is a power of 2; N=2^m cache_location = the low-order m bits of block address e.g. map the value1200 to a 8 blocks, 32 byte/block cache. Block address = 1200/32 = 37 Block number = 37 % 8 = 5 Disadvantage: block access sequence: 100011, 001011, 100011 later arrival will kill out previous one. called cache conflict. Tags and Valid Bits Tags: decide which particular block is stored in a cache location. Store block address as well as the data. Only need the high-order bits. tags size= original address size- block size- blocks number(binary) - 1(valid bit) Valid Bits: decide whether the data exists. Initially 0. Consider a direct-mapped cache with 8 cache frames and a block size of 32 bytes Memory (byte) address generated by CPU|Block address|Hit or miss|Assigned cache block ---|---|--|--- 0010 1100 0010|0010 110|Miss|110 0011 0100 0000|0011 010|Miss|010 0010 1100 0100|0010 110|Hit|110 0010 1100 0010|0010 110|Hit|110 0010 0000 1000|0010 000|Miss|000 0000 0110 0000|0000 011|Miss|011 0010 0001 0000|0010 000|Hit|000 0010 0100 0001|0010 010|Miss|010 Associate Cache Fully associative(FA): Each block can be placed anywhere in the cache. pros: No cache conflict. But still have misses due to size. cons: Costly (hardware and time) N-way Set associative(SA): Each block can be placed in a certain number(N) of cache locations. A good compromise between DM &amp; FA Block Replacement Random Least recently used(LRU): Upon miss: Replace the LRU with missed address -&gt; Move the miss address to MRU position Upon hit: Move the hit address to MRU position -&gt; Pack the rest Deal with Dirty Data Write-back CPU only write to cache. CPU can write individual words at the rate of the cache. Multiple writes to a block are merged into one write to main memory. more and more caches use this strategy. Write-through CPU write both on cache and memory.(memory always up-to-date)]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>MIPS</tag>
        <tag>assemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashing]]></title>
    <url>%2F2018%2F12%2F07%2FHashing%2F</url>
    <content type="text"><![CDATA[Hash Table A hash table is an array of some fixed size, containing all the data items. Each item has a key search is performed based on the keys. Each key is mapped into some position in the array in the range 0 to m − 1, where m is the array size. The mapping is called hash function. Hashing table is a data structure that supports: 1. searching 2. insertion 3. deletion The implementation of hash tables is called hashing. Hashing is a technique which allows the executions of above operations in constant time on average. Unlike other data structures such as linked list or binary trees, data items are generally not ordered in hash tables. As a consequence, hash tables don't support the following operations: find_min and find_max finding successor and predecessor reporting data with a given range listing out the data in order Hashing Function Design A simple and reasonable strategy: h(k) = k mod m. Its a good practice to set the table size m to a prime number. &gt; h(key) =(key[0] + 37*key[1] + 37^2*key[2] + ... ) mod m Handling collision Make hash table an array of linked lists Seperate Chaining Insertion To insert a key k: &gt; Compute h(k) &gt; If T(h(k)) contains nullptr , initial this table entry to point to linked list node of kk . &gt; If T(h(k)) contains non-nullptr, add k to the beginning of the list. Deletion To delete a key k: &gt; Compute h(k) to determine which list to traverse. &gt; Search for the key k, in the list that T(h(k)) points to. &gt; Delete the item with key k if it is found. Open addressing Instead of putting keys of the same hash table into a chain, open addressing will relocate the key k to be inserted if it collides with an existing key. #### Linear probing &gt; f(i) = i &gt; hi(k) = (hash(k) + i) mod m Disadvantage: fall into cluster #### quadratic probing f(i) = i2 hi(k) = (hash(k) + i2) mod m Disadvantage: second cluster... same key same probe steps. #### double hashing f (i) = i × hash2(k) hi(k) = (hash(k) + i × hash2(k)) mod m hash2(k) must be relatively prime to the table size m. Otherwise, we will only be able to examine a fraction of the table entries.]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>hashing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AVL Trees]]></title>
    <url>%2F2018%2F12%2F06%2FAVL-Trees%2F</url>
    <content type="text"><![CDATA[(All codes below are written by Dr. Desmond) # AVL(Adelson-Velsky and Landis) Trees Every sub-tree of an AVL tree is itself an AVL tree.(An empty tree is an AVL tree too.) 123456struct AVLNode&#123; T data; int height; // Important! AVLNode* left; AVLNode* right; &#125; AVL Tree Searching same as BST searching ## AVL Tree Insertion Insertion may violate the AVL tree property in 4 cases: 1. Insertion into the right sub-tree of the right child Left(anti-clockwise) rotation [single rotation] 2. Insertion into the left sub-tree of the left child Right(clockwise) rotation [single rotation] 3. Insertion into the right sub-tree of the left child Left-right rotation [double rotation] 4. Insertion into the left sub-tree of the right child Right-left rotation [double rotation] !!! Distinguish L/R of subtree or L/R of child !!! First insert the node to right place, then balance it by rotation. 123456789AVLNode&lt;T&gt;* AVL&lt;T&gt;::insert(AVLNode&lt;T&gt;* p, const T&amp; d)&#123; if( !p ) return new AVLNode&lt;T&gt;(d); // Empty AVL tree if( d &lt; p-&gt;data ) p-&gt;left = insert(p-&gt;left, d); // Recursion on the left sub-tree else p-&gt;right = insert(p-&gt;right, d); // Recursion on the right sub-tree return balance(p);&#125; ### Rotation Single rotation operate on the node, replace original node by its child Double rotation first operate on the child, then on the node, replace original node by its grandchild. 1234567891011121314151617181920212223242526272829303132AVLNode&lt;T&gt;* AVL&lt;T&gt;::rotateRight(AVLNode&lt;T&gt;* p)&#123; AVLNode&lt;T&gt;* pl = p-&gt;left; p-&gt;left = pl-&gt;right; pl-&gt;right = p; updateHeight(p); updateHeight(pl); return pl;&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::rotateRightLeft(AVLNode&lt;T&gt;* p)&#123; rotateRight(p-&gt;right); return rotateLeft(p);&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::balance(AVLNode&lt;T&gt;* p)&#123; h-&gt;height = max(h-&gt;right-&gt;height,h-&gt;left-&gt;height); // heightDiff() return (right subtree height) - (left subtree height) if( heightDiff(p) == 2 )&#123; if( heightDiff(p-&gt;right) &lt; 0 ) // if need double rotation p-&gt;right = rotateRight(p-&gt;right); return rotateLeft(p); // single rotation &#125; if( heightDiff(p) == -2 )&#123; if( heightDiff(p-&gt;left) &gt; 0 ) // if need double rotation p-&gt;left = rotateLeft(p-&gt;left); return rotateRight(p); // single rotation &#125; return p;&#125; AVL Tree Deletion The idea is same as BST. When delete node with two children, the following code delete the node and rise the min node up. Then balance the whole tree. 123456789101112131415161718192021222324252627282930313233AVLNode&lt;T&gt;* AVL&lt;T&gt;::findMin(AVLNode&lt;T&gt;* p) const&#123; return p-&gt;left ? findMin(p-&gt;left) : p;&#125;// NOT delete the Min-node!!AVLNode&lt;T&gt;* AVL&lt;T&gt;::removeMin(AVLNode&lt;T&gt;* p)&#123; if(p-&gt;left == 0) // If it's the min node, not return itself return p-&gt;right; // Return the second min means removeMin! p-&gt;left = removeMin(p-&gt;left); // Recursion on the left sub-tree return balance(p);&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::remove(AVLNode&lt;T&gt;* p, const T&amp; d) &#123; if( !p ) return 0; // Item is not found; do nothing if( d &lt; p-&gt;data ) p-&gt;left = remove(p-&gt;left,d); // Recursion on the left sub-tree else if( d &gt; p-&gt;data ) p-&gt;right = remove(p-&gt;right,d); // Recursion on the right sub-tree else &#123; // Item is found AVLNode&lt;T&gt;* pl = p-&gt;left; AVLNode&lt;T&gt;* pr = p-&gt;right; delete p; // Remove the node with value d if( !pr ) return pl; // Return left sub-tree if no right sub-tree AVLNode&lt;T&gt;* min = findMin(pr); // Find min. node of the right sub-tree min-&gt;right = removeMin(pr); // Did NOT delete the min node on right sub-tree min-&gt;left = pl; return balance(min); // Balance this node &#125; return balance(p); // Balance this node&#125; ##### Explanation p-&gt;left = remove(p-&gt;left,d); Connect between parent and child. So no need to store parent pointer. delete p; Truly delete node with required data. But store its left and right nodes. Later return min of right sub-tree to its parent node. No memory leak. min-&gt;right = removeMin(pr); Return from the second min node and balanced from bottom up to the deleted node's right child. Did not truly delete the min node. It should already be stored by findMin() function. Also refer here]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
        <tag>data structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trees, Binary Trees, Binary Search Trees]]></title>
    <url>%2F2018%2F12%2F05%2FTrees-Binary-Trees-Binary-Search-Trees%2F</url>
    <content type="text"><![CDATA[Binary Tree Inorder traversal preorder traversal postorder traversal Binary Search Tree BST Insertion Always insert element at bottom as leaf. Remember to check if the element already exists in the tree. Recursion 12345678910bool insert(const T&amp; x, Node*&amp; n ) // pass by referenceNode* insert(const T&amp; x, Node* n ) // pass by value need return pointer&#123; // base case 1: reach the node beyond leaf if(!n) &#123;n-&gt;x = new T(x);return true / new T(x);&#125; // base case 2: the data already exists if(n-&gt;x == x) return false / nullptr ; if(x &lt; n-&gt;x) return insert(x,n-&gt;left); else return insert(x,n-&gt;right);&#125; The insert function parameter Node*&amp; is a reference. We always pass reference to it, thought the value of the reference may be nullptr. e.g. insert(x,t-&gt;left) .In this case, t always refers to a Node*. Iteration 12345678910111213141516bool insertIterative(const T&amp; x, Node* p ) // pass by value&#123; // A new pointer to refer to the leaf Node* pp = nullptr; for(;p!=nullptr;) &#123; // already exists if(p-&gt;x == x) &#123;return false;&#125; pp = p; // save parent for reference if(p &lt; p-&gt;x) p = p-&gt; left; else p = p-&gt;right; &#125; // root is nullptr if(!pp) root = new T(x); else pp = new T(x);&#125; We only need to know the value of the root of the tree. But we need to record the reference of the parent of the insertion node. BST Search public interface: 1234567891011bool contain(const T&amp; x) const&#123; Node* p = root; Node* pp = nullptr; // Non-recursion method COMMON return search(x,p,pp); // Recursion method return searchRecur(x,p) != nullptr; &#125; The implementation of search(x,p) and searchIterative(x,p,pp) is as below. Recursion can NOT keep track of the parent node pointer. 123456789Node* searchRecur(const T&amp; x, Node* n ) // pass by value&#123; // base case 1: reach the node beyond leaf if(!n) &#123;return nullptr;&#125; // base case 2: find the data if(n-&gt;x == x) return n; if(x &lt; n-&gt;x) return search(x,n-&gt;left); else return search(x,n-&gt;right);&#125; Iteration 1234567891011bool search(const T&amp; x, Node*&amp; n, Node*&amp; pn )// pass by reference&#123; for(;n!=nullptr;) &#123; if(n-&gt;x == x) &#123;return true;&#125; pn = n; if(x &lt; n-&gt;x) n = n-&gt; left; else n = n-&gt;right; &#125; return false;&#125; Can keep track of parent node pointer. Note the Node*&amp; n and Node*&amp; pn is pas by reference. They must be created before calling the function BST deletion need findMin() function to help. 123456Node* findMin(Node* p)&#123; if(!p)return nullptr; while(p-&gt;left)p=p-&gt;left; return p;&#125; Delete a node with no child : pointer: current parent Simply delete it and make its parent's original pointer to it nullptr Delete a node with one child : pointer: current parent child Pass its pointer to the only child to its parent's pointer to it. Then delete it. Delete a node with two child : pointer: current min/max parent of min/max Override its data to the max of left or min of right Actually delete the min/max node of the BST, which has no child Recursion 1234567891011121314151617181920212223242526272829303132bool remove(const T&amp; x,Node*&amp; n) // pass by reference&#123; // Base case: NOT found if(!n) return false; // Recursive steps if(x&lt;n-&gt;x) return remove(x,n-&gt;left); if(x&gt;n-&gt;x) return remove(x,n-&gt;right); // node with no child if(!n-&gt;left &amp;&amp; !n-&gt;right) &#123; delete n; n = nullptr; &#125; // node with two children else if(n-&gt;left &amp;&amp; n-&lt;right) &#123; Node* rightMin = findMin(n-&gt;right); n-&gt;x = rightMin-&gt;x; // always true remove(n-&gt;x,rightMin); &#125; // node with only one child else &#123; Node* child = root-&gt;left?root-&gt;left:root-&gt;right; // save &amp;n to curr, later delete Node* curr = n; n = n-&gt;child; delete curr; &#125; return true;&#125; Iteration Need the parent node pointer of Min node. Hard to implement.]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
        <tag>data structure</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Static Variables]]></title>
    <url>%2F2018%2F12%2F05%2FStatic-Variables%2F</url>
    <content type="text"><![CDATA[Static variables Global Scope created only once in a program. reside on the static data region of the loaded program. have a lifetime across the entire run of a program. still may have limited scope: file, function, class. Function Scope initialized only once regardless how many times the function is called. retain their values across the function calls. can be accessed only inside the function. Class Scope variables in different objects are actually one variable. initialize variable must be done in global scope cannot initialize it inside the class. NOT in any function! Not in main(), either! NO keyword static e.g. 123class A &#123;static int a;&#125;;int A::a = 10;int main()&#123;&#125; variable exists even if no class object is created. Static Functions Global Functions almost same as static variables. Member Functions do not have the implicit this pointer like regular non-static member functions. may be used even when there are no objects of the class! can only make use of static data members of the class. cannot be const nor virtual functions. can be defined outside the class declaration NO keyword static e.g. 123class A &#123;static void f();&#125;;void A::f()&#123;&#125;int main()&#123;&#125;]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
        <tag>static variable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inheritance and Polymorphism]]></title>
    <url>%2F2018%2F12%2F05%2FInheritance-and-Polymorphism%2F</url>
    <content type="text"><![CDATA[Inheritance Inheritance: describe &quot;is-a relationship&quot; Polymorphism: derived class can perform like base class(object,pointer,reference) Construction Order: from inner to outer Destruction Order: from outer to inner member access control public: member functions of the class(inner) any member funtions of other classes(other class) any global functions(outside) protected: member functions and friends of the class member functions and friends of its derived classes NOT for outside functions private: member functions and friends of the class Without inheritance, private and protected have exactly the same meaning. public protected private inheritance inheritance public protected private public inheritance public protected private protected inheritance protected protected private private inheritance private private private Public inheritance implements the &quot;is-a&quot; relationship Private inheritance is similar to &quot;has-a&quot; relationship Friend friend of Base is not friend of Derived Polymorphism Dynamic Binding&amp;Virtual Function Once a method is declared virtual in the base class, it is automatically virtual in all directly and indirectly derived classes. Even if derived classed do not announce it's virtual functions. Virtual destructor can make deleting a base pointer to derived object operate correctly. Do not rely on the virtual function mechanism during the execution of a constructor. Similarly, if a virtual function is called inside the base class destructor, it represents base class’ virtual function: when a derived class is being deleted, the derived-specific portion has already been deleted before the base class destructor is called. Abstract Base Class(ABC): NO object of ABC can be created Its derived classes must implement the pure virtual functions. ABC is just an interface.]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ Memory Leak]]></title>
    <url>%2F2018%2F11%2F26%2FC-Memory-Leak%2F</url>
    <content type="text"><![CDATA[Memory detection In Windows using Visual Studio here ##### Open source tools here]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>memory</tag>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI Chap.3 Searching]]></title>
    <url>%2F2018%2F06%2F28%2FAI-Chap-3-Searching%2F</url>
    <content type="text"><![CDATA[Describe problem formally state: initial state: actions: each state corresponds to a set of available actions transition model: return the result state of an action from the previous state goal test: path cost: the essence of a solution is a sequence of actions that lead from the initial state to the goal state. Problem formulation incremental formulation: e.g. 8-queens puzzle. Initial state: no queen; Action: add a queen. complete-state formulation: e.g. 8-queens puzzle. Initial state: 8 queens; Action: move a queen. Search search algorithms all use a search tree data structure with nodes. ### node n.state: correspond state in state space; n.parent: the node in the search tree that generated this node; n.action: the action that was applied to the parent to generate this node; n.path-cost: succeeded from parent; other concepts expanding: apply each legal action to the current state, thereby generating a new set of states. parent node ; child node ; leaf node: a node with no child node frontier(open list): the set of all leaf nodes avaiable for expansion explored set(close list): the set of expanded nodes the frontier separates the state space into explored region and unexplored region repeated state ; loopy path: a special case of the more general concept of redundant path Uninformed(blind) search tree search not remember history causes loopy paths which is a special case of redundant path graph search add explored set(close list) to remember breadth-first search FIFO queue; expand shallowest node; time complexity: O(b^d) (test while generating) O(b^(d+1)) (test while selected) space complexity: all nodes uniform-cost search priority queue; expand cheapest node; test while selected because while generating it may not be the optimal one. time complexity: e be the minimum step-cost,C* be the optimal cost. O(b^floor(1+(C*/c))) space complexity: near nodes depth-first search LIFO stack; first to deepest node that has no successors; common to implement with a recursive function, recursive depth-first search(RDFS) time complexity: O(bm) space complexity: m is the maximum depth. O(bm) or O(m)( backtracking search in which expanded node remembers which successor to generate next) depth-limited search infinite state space cause depth-first search fall. It can be alleviated by a depth limit l. iterative deepening depth-first search often used in combination with depth-first search to find the best depth limit. gradually increase the depth limit till the goal is found. upper levels generate multiple times: N(IDS) = db+(d-1)b2+...+(1)bd bidirectional search b(d/2) + b(d/2) &lt; bd but action step must be reversible Summary uninformed_search Informed(Heuristic) search greedy best-first search evaluates nodes by using just the heuristic function &gt; f(n) = h(n) time complexity: depends on heuristic function, worst O(bm), m is the maximum depth. space complexity: depends on heuristic function, worst O(bm), m is the maximum depth. A* search f(n) = g(n) + h(n) admissible heuristic: never overestimates the cost to reach the goal consistent heuristic: the estimated cost of n is no greater than n' tree-search optimal if h(n) is admissible; graph-search optimal if h(n) is consistent absolute error: E=h*-h; relative error: &gt; e=(h*-h)/h time complexity: O(bE) or O(bed)(constant step costs) space complexity: all nodes that f(n) &lt; C* Memory-bounded heuristic search iterative-deepening A*(IDA*) cutoff = f(n) - g(n) - h(n) each iteration cutoff value is the smallest (f-g-h) of any node exceeded the cutoff on the previous iteration too little memory: between iteration, only retains cutoff = f-g-h recursive best-first search(RBFS) use f_limit variable to keep track of f-value use alternative variable to record the second-lowest f-value among successors (backed-up value) too little memory: each time change mind to alternative path, forget what it have done and regenerate nodes MA* and SMA* MA*(memory-bounded A*) and SMA*(simplified MA*) SMA* expands the best leaf until memory is full, drops the worst leaf node(highest h), like RBFS, SMA* back up the value of the forgotten node to its parent learning to search better metalevel state space: the internal(computational) state of a program that is searching in an object-level state space: the real world problem Heuristic functions From relaxed problems ignore some restriction rules and estimate the cost relaxed problem must be able to solved without search we can use more than one heuristic function like below: h(n) = max{h1(n), h2(n), ..., hm(n)} #### From subproblems: Pattern databases subproblems: just solve part of the problem like going just half way pattern database: store these exact solution costs for every possible subproblem instance disjoint pattern database: only consider partial cost of the subproblem that matters learning heuristics from experience Each solution is an example for study. From these examples, a learning algorithm can be used to construct a function h(n) to predict solution costs for other states that arise during search. Inductive learning works best if supplied with features of a state relevant to predicting state's value the feature is &gt; x1(n), x2(n), ..., xm(n) after solved the problem, we found that &gt; h1(n), h2(n), ..., hm(n) then we approach is to use a linear combination &gt; h(n) = c1*x1(n) + c2*x2(n) + ... + cm*xm(n) Summary AI-Ch3-summary-1 [AI-Ch3-summary-2]/assets/AI-Chap-3-Searching/AI-Ch3-summary-2.png)]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>AI</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dancing Video]]></title>
    <url>%2F2018%2F06%2F18%2FDancing-Video%2F</url>
    <content type="text"><![CDATA[Locking Judge Show here Poppin Semi-final (win) here Poppin Final (lose) here]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>dance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Notes]]></title>
    <url>%2F2018%2F06%2F08%2FLinux-notes%2F</url>
    <content type="text"><![CDATA[Keyboard here About finding files here]]></content>
      <categories>
        <category>Linux Is Not UniX</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cpp Notes]]></title>
    <url>%2F2018%2F05%2F27%2FCpp-Notes%2F</url>
    <content type="text"><![CDATA[About private member 1 在C+的类的成员函数中，允许直接访问该类的对象的私有成员变量 2 在类的成员函数中可以访问同类型实例的私有变量量 3 拷贝构造函数里，可以直接访问另外一个同类对象（引用）的私有成员 4 类的成员函数可以直接访问作为其参数的同类型对象的私有成员 Below codes work!! 123456789101112private: int data;public: void lookOtherConst(const A&amp; a)&#123; cout&lt;&lt;"other's private data is "&lt;&lt; a.data &lt;&lt;endl;&#125; void lookOtherReference(A&amp; a)&#123; cout&lt;&lt;"other's private data is " &lt;&lt;a.data&lt;&lt;endl;&#125; void lookOther(A a)&#123; cout&lt;&lt;"other's private data is " &lt;&lt;a.data&lt;&lt;endl;&#125; About f*cking const here 前缀 与 后缀 运算符 前缀：在表达式计算中使用值之前递增或递减，表达式的值与操作数的值不同 后缀：在表达式使用值之后递增或递减， 表达式的值与操作数相同 e.g. 1234567int a=5,b =5;cout&lt;&lt;"a&lt;b++ = "&lt;&lt;bool(a&lt;b++)&lt;&lt;endl;cout&lt;&lt;"a&lt;++b = "&lt;&lt;bool(a&lt;++b)&lt;&lt;endl;output:a&lt;b++ = 0 a&lt;++b = 1]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim Notes]]></title>
    <url>%2F2018%2F05%2F25%2FVim-notes%2F</url>
    <content type="text"><![CDATA[Neovim and vim-plugin A good blog link for both introduction of neovim and vim-plugin. search and substitution here fold and unfold here keyboard remap here]]></content>
      <categories>
        <category>VIM is so hard to use</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Origin]]></title>
    <url>%2F2018%2F05%2F23%2FOrigin%2F</url>
    <content type="text"><![CDATA[Trust me, you would not like to read the shix inside, nor do I. a12b3f37607e43d047895421a584e526bc286c6795e869c9fb7804f0e061842e3caaabdbf930ef9583a4d5e4d8fcfc25516b614a89c2d5d1b903c2d935ed329070839c82a9054c11e1e403d7c97f8aed451ec81b78fb24cc4fbc37211c6513f732aedc3cfc2b695541be387ba1c50d659a3061ee3cd3d7fffbba74296f70e41bd83970b7ac634b6bd96aeb73b1d7f8c648c4c7bb3cfb04dcd00686370a46c2d0d79d320d3b3dd59a9e8d719d9146c0d843222b275787448552998aff069c2efdbbcddece426d0250d87defa62707b9d28edcbdfb8dde26214557d4e7dff176acaddf8eb199bfba2664070a18d96c86adb9225c054bee6fa6fca4718405366c88a09f01b802eec3a6170b5b94e2f0e3cbb4440c25b832786ff1d67bb8cdd36cb97463746741744e04d96e505299cf60dc7ab3e34248b3b533b143f7e2c9a8777c9c77140689616814d40e57eb53b87abde4948b5cce4714f138602286a8d6478daf2b32d9c7acec24d3ed4ea40aec7d8744a55dec0eb1eb57cbc31ab9a3d1249b7865c002d6f8a0d7696ddd72f8864d2350521804ecfed67cb4a8fe029ca94c26d15f275def037aaff114ebcc19f44614a912a356f6a1a25171f83bbb08e2995c0e15df3da7956f47955862a1d590b81ed797c3ac5af5b35be09d487af94075bc8f6d5d7386ddf554f1a46df5a48112a1982d38ba4a5f8b7160d832154392d9f0c1084cd2fda60e185c01ae4e9a0549156de8e1e2a8d45966f69150b2f25cd93a88bc641efe1a674e645a46d1cbf5a3e654a78a7334295b5712fcf95a3d2fcfdc5a8abb3ff0b311ed972e14d35a8487c9ef0ebdbe9feb7f9ac10662e0214f0f220db55884477721088f6f4ac541a9f310e0cb2b2eadeecc74d31e26c5bdc70f64dbe5c0f5cb0ac7c6543e8c29ff8d670289d345ff128ced5f100c4cc34ed37a08568085f34bc8f3c2957782518beba95df2bee7fde5e5a6e1afa347773edcee11222b1d6e818a77208ab9bc06cd0b1815681b0aa09e5fbcc54d2e0d807a8e11c5fd6f58508a9d411a3be43a8b7c02d6e092573ef344e97f3b7773af1354e4ef940ac44b491c85b3b07c42fe6b27313200277c89f44d72498ef20940f37c54faabfef715f3ad0ae11c6155db6eb36d2787ef89c9a79fb541d62069b1dce58ee9231789304180004c47152ba32e68264e79918db37a251fbae5f299c7cfd53ea4f9660395977515cb937848b24d1d91d743dfd9c34b2a933349a4dcfdb5734b119b0f075386e05020f51a52351d69640f34a1a97a24669fa684ad19efe04f3d73ea]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
</search>
