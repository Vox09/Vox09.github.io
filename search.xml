<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Machine Learning]]></title>
    <url>%2F2019%2F10%2F09%2FMachine-Learning%2F</url>
    <content type="text"><![CDATA[[TOC] BasicNaive Bayes ClassifierBayes Rule$P(A|B)={P(A\and B)\over P(B)}={P(B|A)P(A)\over P(B)}$ When $h$ stands for hypothesis, $D$ stands for data $P(h|D) = {P(D|h)P(h)\over P(D)}={P(D|h)P(h)\over \sum_hP(D|h)P(h)}$ $P(h)$ prior probability of $h$. Initial probability before observing. $P(h|D)$ posterior probability of $h$ after observing the data $D$ $P(D|h)$ likelihood of observing the data $D$ given hypothesis $h$ $P(D)$ observed probability that training data $D$ will be observed Independence and Conditional IndependenceTwo random variables $X$ and $Y$ are independent if $P(X|Y)=P(X)$ or $P(Y|X)=P(Y)$ Two random variables $X$ and $Y$ are conditionally independent given $Z$ if $P(X|Y,Z)=P(X,Z)$ Naive Bayes ClassifierAssumption: Attributes are conditionally independent of each other given the class variableObjective: Maximum the posterior probability given the data. $v_{NB} = arg\ max_{v_j\in V}P(v_j)\Pi^n_{i=1}P(a_i|v_j)$ where $a$ is the attribute, $v$ is the class variable. What if $P(a_i|v_j)$ is zero? Laplace correctionAdd a virtual count of 1 to each attribute value. change ${n_c\over n}\to{n_c+1\over n+|a|}$ Overfithypothesis $h\in H$ over fits the training data if there is an alternative hypothesis $h_0\in H$ such that$h$ has a smaller error than $h_0$ over the training examples,$h$ has a larger error than $h_0$ over the test examples (entire distribution of instances) k-fold cross-validationDivide m examples to k fold. Each time use one fold for validation, others for training. Repeat k times. k-fold stratifi ed cross-validationpartition the m examples into k folds such that each class is uniformly distributed among the k folds. Artificial Neural NetworkPerceptron inputs $x_1,x_2…$ or $\vec x$ weights $w_1,w_2….$ or $\vec w$ bias $b$ or threshold $t$ activation function $f(x)$ output $o = f(\vec x\vec w -b)$ AdalineA feed-forward network with one layer of adjustable weights connected to one or more linear units (as output units) $o = \sum^n_{i=0}w_ix_i = \vec x\vec w$ target output for training data $d: t_d$output for the training data $d: o_d$squared training error : $E(\vec w) = {1\over 2}\sum_{d\in D}(t_d-o_D)^2$ Gradient Descent$\Delta E(\vec w) = [{\part E\over\part w_0},{\part E\over\part w_1},…{\part E\over\part w_n}]$ we update the $\vec w$ with learning rate $\eta$ by $w\leftarrow w+\Delta w$where $\Delta w = -\eta\nabla E(\vec w)=\eta\sum(t_d-o_d)x_d$ Hidden LayerMLP Multi-layer perceptron If hidden units were linear, then multi-layer is no better than a single layer. We need nonlinearity, to make the network perform better. Common activation function and derivative: Sigmoid $f = {1\over 1+e^{-x}}$ derivative $f’ = f(1-f)$ Radial basis function network(RBF) $f =exp(-{(x-w_j)^T(x-w_j)\over2\sigma_j^2})$ Gaussian distribution. ReLU $f = max(0,x)$ derivative $f’ = (x&gt;0)?1:0$ tanh $f=tanh(x)$ derivative $f’=1-tanh^2(x)$ Back-Propagation${\part E\over \part w_i} = {\part\over\part w_i}{1\over 2}(t-o)^2 = -(t-o)({\part o\over \part w_i}) = -(t-o){\part o\over\part f_a}{\part f_a\over\part w_i}$ where $f_a$ is the input of the activation function, and ${\part f_a\over\part w_i} = x_i$ ${\part E\over \part w_i} =-(t-o){\part o\over \part f_a}x_i = -\delta x_i$where $\delta = (t-o){\part o \over \part f_a}$ Weight update $w_{ji}\leftarrow w_{ji}+\Delta w_{ji}=w+\eta\delta_jx_i$ batch gradient descent: use all examples in each iteration stochastic gradient descent: use 1 example in each iteration mini-batch gradient descent: use b examples in each iteration To speed up the BP algorithm, we can use the momentum term $\Delta w(t+1)=-\eta{\part E\over\part w}+\alpha\Delta w(t)$. To avoid get in stuck in local minima, we can train multiple networks using the same data, but initialize each with different random weights. CNN Nonlinearity is from ReLU. Zero padding each layer to avoid data shrinking. Max pooling to extract features and approximately locate their location. 1x1 Convolution$D_K$ is the dimension of kernel, $D_F$ is the dimension of features,$M$ is the input data channel, $N$ is the output feature channel original cost of convolution $D_K^2MND_F^2$ depth-wise convolution $D_K^2M$ One filter per channel. Combine information from channels.1 x 1 convolution: $M$ atotal $D_K^2MD_F^2+MND_F^2$ speed up $D_K^2N/(D_K^2+N)$Empirically, 9 times less work with the same accuracy. RNNHidden layers and output depend from previous states of the hidden layers and the current inputs. The same weights are used for different instances of the artificial neurons at different time stamps. Problems: vanishing gradientsgradient signal get so small that learning stops. exploding gradientsgradients signal is so large that it can cause learning to diverge. LSTMLong Short-Term Memory $\odot$ stands for element wise multificationforget gate $f_t=\sigma(W_f[h_{t-1},x_t]+b_f)$input gate $i_t=\sigma(W_i[h_{t-1},x_t]+b_i)$Internal State$\bar C_t=tanh(W_C[h_{t-1},x_t]+b_C)\quad C_t= f_t\odot C_{t-1}+i_t\odot\bar C_t$Output gate $o_t=\sigma(W_o[h_{t-1},x_t]+b_o)\quad h_t=o_t\odot tanh(C_t)$ Reinforcement LearningLearning Paradigms Supervised earningthe learner is provided with a set of inputs together with the corresponding desired outputs. Unsupervised learningtraining examples as inputs patterns, with no associated output patterns Reinforcement learningOnly input and evaluative out are given Concept Goal States $s_i$ Actions $a_i$ Rewards $r_i$ Markov Assumption:$s_{t+1}$ and $r_t$ depend only on current state and action Non-Deterministic:Action may have uncertain outcomes.$P(s,s’,a)$ probability of transition from $s$ to $s’$ given action $a$$R(s,s’,a)$ expected reward on transition $s$ to $s’$ given action $a$ Discounted Rewards:A reward in the future is not worth quite as much as a reward now.Introduce a discount factor for future rewarddiscounted rewards = $r_t+\gamma r_{t+1}+\gamma^2 r_{t+2}+…$where $0\le\gamma\le1$. shortsighted $0\leftarrow\gamma\to1$ farsighted PolicyPolicy is denoted by $\pi$ Learn action policy $\pi$ that maximizes the expected future reward$V^\pi(s)\equiv E[r_t+\gamma r_{t=1}+\gamma^2r_{t+2}+…]$ Deterministic$V^\pi(s)\equiv E[r_t+\gamma r_{t=1}+\gamma^2r_{t+2}+…] = r_t+\gamma r_{t=1}+\gamma^2r_{t+2}+…$ At state $s$, take action $a$immediate reward $r(s,a)$value of the immediate successor state $V^\pi(\delta(s,a))$ $V^\pi(s) = r(s,a) +\gamma V^\pi(\delta(s,a))$ (Bellman equation) NondeterministicAt state $s$, take action $a$ with probability $\pi(s,a)$from $s$, take action $a$, probability of transition to $s’:\ P(s,s’,s)$expected reward on transition given action $a:\ R(s,s’,a))$ $V^\pi(s) = \sum_a\pi(s,a)\sum_{s’}P(s,s’,a)[R(s,s’,a)+\gamma V^\pi(s’)]$ Iterative EvaluationInstead of solving the linear system, we can also use an iterative method! Initialize $V_0$ $V_0\to V_1\to V_2\to …V_k\to V_{k+1}…\to V^\pi$ Update Rule:$V_{k+1}(s) = \sum_a\pi(s,a)\sum_{s’}P(s,s’,a)[R(s,s’,a)+\gamma V_k(s’)]$]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>AI</tag>
        <tag>ML</tag>
        <tag>CNN</tag>
        <tag>RNN</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[PHYS1003]]></title>
    <url>%2F2019%2F10%2F09%2FPHYS1003%2F</url>
    <content type="text"><![CDATA[EnergyEnergy Consumption and Population Growth I = P x A x T Human Impact = Population x Affluence x Technology 7.2 billion people now. 140 million born/year, 57 million die/year. 108 billion estimated to have ever lived. doubling time 2 = e^0.7 Fossil fuels1 manpower = 75W 1kWh = 3.6 x 1e6 J1 hp = 746 J1 toe = 42 x 1e9 J1 Cal = 4.19 x 1e3 J1BTU = 1055 J ThermodynamicsTemperature and heatKelvin = Celsius + 273.15Fahrenheit = 9/5 Celsius + 32 Internal Energy = Kinetic Energy(KE) + Potential Energy(PE)For gas, PE is almost zero. The internal energy of gas is the sum of kinetic energy of all molecules. ${1\over2}mv_{avg}^2={1\over2}jKT$ where $j$ is the degree of freedom of motion. For ideal gas $pV=nRT=NkT$where $R=8.315J/mol/K$ and $k=1.38\times10^{-23}J/K$ and $N_A=6.02\times10^{23}$ Laws of thermodynamics The first law of thermodynamics $\Delta U=Q-W$change in internal energy = Heat added - work done 2nd law: Isolated systems naturally move towards configurations of increasing probability. Ideal GasWork done in isothermal process $W=nRT\ ln(V_f/V_i)$Work done in adiabatic expansion $W = \int PdV = 1/(\gamma-1)$ where $\gamma=C_p/C_v$ Thermodynamic processes in a cycle $a\to b$ P is constant $\ W=P\Delta V,\Delta U=Q-W$$b\to c$ T is constant $\ W=Q,\Delta U=0$$c\to d$ V is constant $\ W=0,\Delta U=-W$$d\to a$ adiabatic compression $Q=0, \Delta U=-W$For an ideal gas $\Delta U=nC_V\Delta T$ , where $C_V$ is the molar specific heat capacity.At constant volume $C_V=(j/2)R$. At constant pressure $C_P=C_V+R$ Heat EngineHeat input to the engine $Q_h$ equals to heat output from the engine $Q_c$ plus the work done $W$$Q_h=Q_c+W$ The thermal efficiency $e={W\over Q_h}=1-{Q_c\over Q_h}$ Heat PumpHeat input to the engine $Q_c$ plus the work done $W$ equals to heat output from the engine $Q_h$$Q_c+W=Q_h$The effectiveness is described by COP, the ratio of useful heat movement per work input.Cooling mode $COP={Q_c\over W}\le {T_c\over T_h-T_c}$Heating mode $COP={Q_h\over W}\le {T_h\over T_h-T_c}$COP becomes most “efficient” for small temperature differences Carnot EngineIf operates in an ideal, reversible Carnot cycle between two reservoirs and is the most efficient engine possible. Isothermal expansion $A\to B$The gas absorbs $Q_h$ from high temperature reservoir $T_h$ and do work $W_{AB}$ Adiabatic expansion $B\to C$Temperature drops from $T_h$ to $T_c$. The gas does work $W_{BC}\gt 0$ Isothermal compression $C\to D$The gas expels $Q_c$ to low temperature reservoir $T_c$ and work $W_{CD}$is done on gas Adiabatic expansion $D\to A$Temperature rises from $T_c$ to $T_h$. The gas does work $W_{DA}\lt 0$ The net work $W_{eng}=|Q_h|-|Q_c|$.The efficiency $e_C=1-{T_c\over T_h}$ Entropy$\Delta S=\Delta Q_{rev}/T \to S = \int dQ_{rev}/T$ The effect of all naturally occurring processes is always to increase the total entropy of the universe. $e_{any\ engine}\le 1-{T_c\over T_h}=e_{Carnot}$ Transport EngineOtto Gasoline Engine The efficiency of the Otto cycle is $e_0=1-{1\over (V_1/V_2)^{\gamma-1}}$. Generally about 20% to 30% Energy dissipation = Energy for brake + Air drag${1\over 2}v^3(m_c/d+\rho A)$ where $A$ is the cross-sectional area of the car. Diesel Engine Jet EnginePower required to overcome drag $P_{drag}={1\over 2}\rho c_dA_pV^3$ where $A_p$ is the frontal area of the plane.Power required to lift $P_{lift}={(mg)^2\over 2\rho vA_s}$ where $A_s$ is the cross-sectional area of the air cylinder.Total Power $P_{total}=P_{drag}+P_{lift}$Thrust can be optimized $F={P\over v}={1\over 2}\rho c_dA_pV^2+{(mg)^2\over 2\rho v^2A_s}\ge c_d\rho A_pv_{opt}^2$where $\rho v_{opt}^2={mg\over\sqrt c_dA_pA_s}$ so that $F_{opt}=\sqrt{c_df_A}mg$ where the $f_A=A_p/A_s$ is the filling factor Transport cost = ${1\over\epsilon}g\sqrt{c_df_a}\quad(N/kg)$Transport efficiency = $N_{passengers}\times\epsilon\times E/V \over thrust$(passenger-km per litre)Range = ${energy\times \epsilon\over force}$]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
  </entry>
  <entry>
    <title><![CDATA[Database]]></title>
    <url>%2F2019%2F07%2F07%2FDatabase%2F</url>
    <content type="text"><![CDATA[ER ModelKeys If the values of some attributes can uniquely identify an entity instance then these attributes are said to be a key of the entity. An entity may have more than one key. A candidate key is a minimal set of attributes (i.e., all attributes are needed) that uniquely identifies an entity instance. One candidate key is selected to be the primary key. A candidate/primary key can be composed of a set of attributes =&gt; composite key. Strong and Weak EntityStrong entity: An entity that has a primary key Weak entity: An entity that does not have a primary key. A weak entity must be associated with a strong entity, called the identifying entity, to be meaningful. The relationship associating the weak entity to the strong entity is called the identifying relationship. A discriminator, if present, uniquely identifies a weak entity instance within its identifying relationship. A surrogate key is a new attribute introduced into an entity to be the primary key of the entity. It may be useful to make weak entities strong or to replace a strong entity’s key, if it consists of many attributes. (usually sequentially assigned numbers.) Relation Model and AlgebraReduce from ER Model Generalization, two options: Reduce all entities to relation schema Add: the primary key, $K$, of the superclass a foreign key constraint, reference superclass a referential integrity action: on delete cascade Reduce only subclass entities to relation schema. (Only for total, disjoint generalizations)Add: all the attributes of the super class entity. Composite attributes, two options: Create a single attribute and pack components in it. Create a separate attribute for each component. Multivalued attribute $M$ , entity $S$: Create a relation schema $SM(K_S,A)$ where $K_S$ is the primary key of $S$, $A$ is the $M$. The primary key of relation $SM$ is the union of all its attributes. Add: a foreign key constraint: foreign key $(FK_S)$ reference $S(K_S)$ a referential integrity action: on delete cascade Strong entity:Create a relation with all the attributes. Weak entity $T$ that depends on strong entity $S$: Create a relation schema $R_T$ with attributes of $T$. Include attributes $A_R$ of relationship $R$ in $R_T$ Include as foreign key attributes $FK_S$ in relation $R_T$ The primary key of relation $R_T$ is the union of the foreign key attributes $FK_S$ and the discriminator $d_A$ of the weak entity $T$. Add: a foreign key constraint: foreign key $(FK_S)$ references $S(K_S)$ a referential integrity action: on delete cascade Relation: Create a new relation schema $R_R$ Include as foreign key attributes in relation $R_R$ the primary key of the entities related by relationship $R$. Include attributes $A_R$ of relationship $R$ For 1:1 relationship, combine the attributes in the relation to the relation schema of either entity. For 1:N relationship, combine the attributes in the relation to the relation schema of either on the N-side. For M:N, the relation schema of relationship cannot be combined to relation schema of entity. Add: a foreign key constraint for the foreign key. a referential integrity action: optional=on delete set null; total=on delete cascade; Relation Algebra Selection$\sigma_{Condition}(Relation)$ Projection$\pi_{List}(Relation)$ result no duplicated. Set operations$\cup$ union $-$ set difference $\cap$ intersection $\times$ Cartesian product Join$\Join_{Condition}$ join two relation with certain condition.left/right/full outer join can create table with value null SQL123select A1, A2, ..., Anfrom R1, R2, ..., Rmwhere P; Ai are attributes. Ri are relations. P is a predicate (condition). Return a relation(may contain duplicates). select clause = Projection $\pi L(R)$from clause = Cartesian product $A\times B$where clause = Selection $\sigma C(R)$, natural join $\Join_N$ Nested Subqueries123select *from Rwhere a &gt; (select avg(a) from R) Aggregate Functions1234select name, count(name)from Rgroup by name, numberhaving number&gt;100; An attribute in the select clause must also appear in the group by clause. An attribute in the group by clause do not need to appear in the select clause. Any attribute present in the having clause that is not being aggregated must appear in the group by clause. Temporary Subqueries12345select name, avgAmountfrom (select name, avg(amount) as avgAmount from R group by name)resultwhere avgAmount = (select max(amount) from result); In Oracle, it is expressed by the form 1234567with result(name, avg(amount) as select name, avg(amount) from R group by name)select name, avgAmountfrom resultwhere avgAmount = (select max(amount) from result); Manipulation Create Relations create table T(a type); Alter and Destroy Relationsadd attributes alter table T add b type;drop attributes alter table T drop column b;drop table drop table T; Integrity Constraints(IC)not null primary key unique foreign key...references... check Create views to hide certain data from certain users Delete tuples delete from R where name=&#39;want&#39; Update update R set a = 0 Function Dependencysuperkey: any set of attributes that determines the entire tuple. candidate key: any minimal set of attributes that determines the entire tuple. primary key: if there are multiple candidate keys, the database designer choose one of them as the primary key. The set of all functional dependencies logically implied by $F$ is called the closure of $F$ denoted as $F^+$ Lossless DecompositionA decomposition of $R$ into $R_1$ and $R_2$ is lossless if and only if $R_1 \cap R_2 \to R_1$ or $R_1 \cap R_2 \to R_2$ Preserve Functional dependenciesThe decomposition is dependency preserving if and only if $(\cup F_i )^+ = F^+ $. prime and non-prime attributes:An attribute is a prime attribute if it is part of any candidate key. Canonical Cover Algorithm $F_c$ = $F$Repeat: Use the union rule to replace any FDs in $F_c$ of the form $X\to Y$ and $X\to Z$ with $X\to YZ$ Find an FD $X\to Y$ in $F_c$ with an extraneous attribute either in $X$ or in $Y$Until $F_c$ does not change 1NFA relation schema is in First Normal Form (1NF) if all attributes are atomic (single-valued). There are no multi-valued or composite attributes Relation schema are always in 1NF. 2NFA relation schema is in Second Normal Form (2NF) if all non-prime attributes are fully functionally dependent on every candidate key. Relation $R$ is 2NF if and only ifFor each FD: $X\to A$ in $F^+$: $A\in X$ (trivial) or $X$ is not a proper subset of a candidate key for $R$ or A is a prime attribute for $R$ 3NFA relation schema R is in Third Normal Form (3NF) if it is in 2NF and every non-prime attribute of R is non-transitively dependent on every candidate key of R. Relation $R$ is 3NF if and only if:For each FD:$X\to A$ in $F^+$: $A\in X$ (trivial) or $X$ is a superkey for $R$ or $A$ is a prime attribute for $R$ Algorithm Let $R$ be the initial relation schema with FDs $F$Compute the canonical cover $F_c$ of $F$.$S = \phi$For each FS $X\to Y$ in the $F_c$: $S=S\cup (X,Y)$If no schema contains a candidate key for $R$, choose any candidate key $K$ , $S =S\cup K$ The algorithm always creates a lossless-join, dependency preserving, 3NF decomposition. BCNFA relation schema is in Boyce-Codd Normal Form (BCNF) ifevery determinant (left hand side) of its FDs is a superkey. Relation $R$ is BCNF if and only if:For each FD:$X\to A$ in $F^+$: $A\in X$ (trivial) or $X$ is a superkey for $R$ There is always a lossless decomposition that generates BCNF relation schema. However, all the functional dependencies may not be preserved. Algorithm Let $R$ be the initial relation schema with set of FDs $F$.Compute $F^+$$S =R$Until all relation schemas in S are in BCNF For each $R$ in $S$ For each FD $X\to Y$ that violates BCNF for $R$ $S = (S-R)\cup (R-Y)\cup (X,Y)End until Test if a FD violates BCNFEITHER test $R_i$ for BCNF with respect to the restriction of $F^+$ to R i (i.e., all FDsin $F^+$ that contain only attributes from $R_i$ ). OR use the following test. For every set of attributes $X \subseteq R_i$, check that the attribute closure $X^+$ eitherincludes no attribute of $R_i-X$ or includes all attributes of $R_i$. If the condition is violated, the dependency $X\to(X^+ -X) \cap R_i$ holds on $R_i$ ,and $R_i$ violates BCNF. We use the BCNF-violating dependency to decompose $R_i$ . Storage and File Structureseek time: Time to move the arms to position the disk head on a track. (4 to 15 ms)rotational latency: Time to wait for the page (sector) to rotate under the head. (2 to 7 ms)transfer time: Time to actually move data to/from the disk surface. (1ms/4KB) seek time and rotational delay dominate. File blocking Factor $bf_r$The number if records that fit in a page and is equal to$\lfloor$ # bytes per page / # bytes per record $\rfloor$The number of pages needed to store a file is equal to$\lceil$# records / $bf_r\rceil$ IndexTo reduce the cost to find a record, we build index page on certain attribute of the records. The attribute is called the search key of the index. Ordered Index An index page is also called an index node. fan-out is the number of children of an index node The height of the tree is $\lceil log_{fan-out}(# index\ entries)\rceil$ Clustering and Non-clustering Primary Index (Clustering Index)An index for which the file is sorted on the search key of the index. Secondary Index (Non-clustering Index)An index for which the file is not sorted on the search key of the index Sparse vs Dense Sparse Indexcontains an index entry for only some search key values (applicable only to primary/clustering indexes) Dense Indexcontains an index entry for every search key values (often secondary/non-clustering indexes) To build an index on an attribute that may be the same for several records.It is not a problem if the index is primary and sparse.When secondary and dense. we have 3 way: use variable length index entries. Use multiple index entries per name. Use an extra level of indirection(most common)an index entry points to a list/bucket that contains the pointers to all the actual records with that name. B+ tree IndexAutomatically reorganizes itself to keep balance. Balanced treeAll path from the root node to the leaf nodes are the same length Fan-outThe maximum number of pointers/children in each node, denote n. B+ tree orderThe value $\lceil(n-1)/2\rceil$ corresponds to the minimum number of values in a leaf node. Non-leaf nodes form a multi-level, sparse index on the leaf nodes InsertionIf the leaf node $L$ has enough space, done.Else, split the leaf node. Place first $\lceil n/2\rceil$ values in $L$, copy up values at $\lceil n/2\rceil +1$ and place values left in $L’$ copy up: insert an index entry(value at $\lceil n/2\rceil +1$), pointer to $L’$) into the parent of $L$ Splits can happen recursively. To split an internal index node $N $ Place first $\lceil n/2\rceil -1$ values in $N$, push up value at $\lceil n/2\rceil$ and place values left in $N’$ Push up: Insert an index entry(value at $\lceil n/2\rceil$) pointer t o$N’$) into the the parent of $R$ DeletionIf the leaf node $L$ has at least $\lceil n(-1)/2\rceil $ values, done.Else, try to re-distribute by borrowing values from a sibling node(adjacent node right or left)If re-distribution fails, merge $L$ and its sibling. Hash index and Bitmap IndexHash indexes are always secondary indexes. Bitmap indexes are a special type of index designed for efficient querying on multiple search keysA bitmap is simply an array of bits. Query ProcessingSelection$B_r$ the number of pages that contain records of relation $r$$HT$ the height of the tree index File Scan linear search cost: $B_r$ or $B_r/2$ if selection is on a key attribute. It stops once we find one. binary search selection is equality comparison on the attribute on which the file is ordered cost: $\lceil log_2(B_r)\rceil$ Equality Search primary index on key retrieve a single record that satisfies the equality condition cost: $HT_i +1$ primary index on non-key retrieve multiple records that satisfy the equality condition cost: $HT_i+#\ pages\ contain\ records\ satisfy\ the\ condition$ secondary index retrieve a single record if the search key is a candidate key cost: $HT_i +1$ for tree index $1+1$ for hash index Comparisonsretrieve record with condition $A\ge V$ or $A\le V$ by using linear file scan, binary search or indexes. primary index $A\ge V$ use the index to find the first one, and scan from there $A\le V$ do not use the index. Scan from the top till the first one $A&gt;V$ secondary index $A\le V$ scan the leaf pages of the index until the first entry $A&gt;V$ For conjunction(AND) $\sigma_{\theta1\and\theta2\and…\and\theta n}(r)$ using one index select a combination of $\theta_i$ and all algorithms above that results in the least cost using composite index If available, use an appropriate composite index by intersection of record pointers If any attributes have indexes with record pointers then use them and take intersection to get the set of pointers. Then check other condition in memory. cost: $\sum\ costs\ of\ individual\ index\ scans + cost\ of\ retrieving\ the\ tuples$ For disjunction(OR) $\sigma_{\theta1\or\theta2\or…\or\theta n}(r)$ by union of record pointers If all attributes have indexes with record pointers then use them and take union of them cost: $\sum costs\ of\ individual\ index\ scans+cost\ of\ retrieving\ the\ tuples$ SortingLet $M$ denote the memory size. N-way merge (assume$N&lt;M$):Use $N$ pages of memory to buffer input, and 1 page to buffer output. If the number of input pages is greater than the memory size, several passes are needed. Number of passes: $\lceil log_{M-1}(B_r/M)\rceil+1$ passesI/O cost: $2B_r(\lceil log_{M-1}(B_r/M)\rceil+1)$ pages Join$r, s$ the relations to be joined$n_r,n_s$ the number of tuples( records) in $r$ and $s$, respectively.$B_r,B_s$ the number of pages in $r$ and $s$, respectively$M$ the available pages of memory. nested-loop join requires no indexes and can be used in any condition Worst case $n_r*B_s+B_r$ only 1 memory page available for each relation Best case $B_r+B_s$ block nested-loop join requires no indexes and can be used in any condition Worst case $B_r*B_s+B_r$ only 1 memory page available for each relation Best case $B_r+B_s$ With Optimizations $\lceil B_r/(M-2)\rceil * B_s+B_r$Use $M-2$ page as the blocking unit of the outer relation. indexed nested-loop join index lookups can replace file scans if the joint is equi-join or natural join andan index is available on the inner relations joint attribute. cost $B_r+n_r*c$where c is the cost of traversing the index and fetching all matching $s$ tuples for one tuple of $r$ merge-join Sort both relations on their join attribute an merge the sorted relations to joint them. cost $B_r+B_s+cost\ of \ sorting$the cost of sorting is 0 if the relations are sorted. hash-join Applicable for equi-joins and natural joins A hash function is used to partition tuples of both relations into $n$ buckets. cost $3*(B_r+B_s)$1 read and 1 write to create partitions/buckets, 1 read to compute the join. complex joins Projectionselect distinct boatId from Reserves sorting (standard) Modify Pass 0 to eliminate unwanted attributes. Write &lt; Read Modify merge passes to eliminate duplicates. Write &lt; Read hashing Discard unwanted attributes and apply the hash function $h1$ to hash to $M-1$ partitions. Write &lt; Read Read each partition and build in-memory hash table using hash function $h2$ discard duplicates Set Operations sorting sort both $r$ and $s$ on the same attribute eliminating duplicates when merge intersection: only if it is in both relations Union: only once if it belongs to both relations Set difference: only if its is in the first relation but not in the second one hashing partition relation $r$ and $s$ using hashing function $h1$ on all attributes For each s-partition, build an in-memory hash table using $h2$ on all attributes while discarding duplicates. Then scan the r-partition, for each tuple of r intersection: output $t_r$only if it is also in $s$ union: output $t_r$ if it is not in $s$. also output all tuples in $s$ set difference: output $t_r$ only if it is not in $s$ Relation Algebra Tree EvaluationA relational algebra (operator) tree represents a relational algebra expression (an SQL query) as a tree.The Evaluation is bottom up. materialization Evaluate and generate the results of one operation at a time. Actually store (materialize) the result on disk for subsequent use. Overall Cost: sum of costs of individual operations + cost of writing intermediate results to disk pipelining Evaluate several operations simultaneously, passing tuples to the next operation as they are generated. No need to store temporary results. Much cheaper than materialization since there is no need to store temporary relations to disk. Query OptimizationQuery optimization is the process of selecting the most efficient query-evaluation plan from among many strategies. Cost Based Optimization Generate logically equivalent evaluation plans. Estimate the cost of each plan. Execute the plan with the minimum expected cost. Heuristic Optimization Perform the cheap operations first. Try to utilize existing indexes. Remove unneeded attributes early. Transformation of Relation ExpressionsTwo relational algebra expressions are equivalent if they generate the same set of tuples on every legal database instance. Cascading of projections: $\pi_{a1}(R)=\pi_{a1}(\pi_{a2}…(\pi_{an}(R))…)$ Cascading of selections: $\sigma_{c1\and c2\and …\and cn}(R)=\sigma_{c1}(\sigma_{c2}…(\sigma_{cn}(R))…)$ Commutativity of selections: $\sigma_{c1}(\sigma_{c2}(R))=\sigma_{c2}(\sigma_{c1}(R))$ Commutativity of joins/Cartesian products: $R\Join S=S\Join R$ Associativity of joins/Cartesian products: $(R\Join S)\Join T=R\Join (S\Join T)$ Commutativity of selections with projections: $\pi_a(\sigma_c(R))=\sigma_c(\pi_a(R))$ Commutativity of selections with joins/Cartesian products: $\sigma_c(R \Join S)=(\sigma_cR) \Join S$ Projection Distributes Over Join: $\pi_a(R\Join S)=(\pi_a(\pi_{a1}R)\Join (\pi_{a2}S))$ Join Order: A good ordering of join operations is important for reducing the size of intermediate results. Usually we use dynamic programming to find out the best plan with minimum cost. Interesting Sort Order: a particular sort order of tuples that could be useful for a later operation. Estimating Statistics of Express ResultThe DBMS system catalog stores the following statistics for each relation $r$: $n_r$ the number of tuples in $r$ $B_r$ the number of pages containing tuples of $r$ $I_r$ the size of a tuple of $r$ in bytes $bf_r$ the blocking factor of $r$ $V(A,r)$ the number of distinct values that appear in $r$ for attribute $A$. For indexes the system catalog stores the following information: $HT_i$ the number of levels in the index $i$ $LB_i$ the number of pages at the leaf level of the index Selection Cardinality$SC(\theta,r)$ the selection cardinality of predicate $\theta$ for relation $r$, is the average number of tuples that satisfy the predicate $\theta$ Selectivity$Selectivity(\theta,r)=SC(\theta,r)/n_r$ the fraction of tuples that satisfy $\theta$ . It is between 0 and 1. Size EstimationEstimates may be quite inaccurate but provide upper bounds on the sizes. Join If $r\cap s=A$ is a key for $r$The number of tuples is no greater than $n_s$ If $r\cap s=A$ is a (not null) foreign key for $s$ referencing $r$ The number of tuples is exactly $n_s$ If $r\cap s=A$ is not a key for $r$ or $s$The number of tuples is the lower of $n_rn_s\over V(A,s)$ and $n_rn_s\over V(A,r)$ Projection: $V(A,r)$ Aggregation: $V(A,r)$ Set operations: Union $r\cup s \to n_r + n_s$ intersection $r\cap s \to min(n_r , n_s)$ set difference $r-s\to r$ TransactionsA transaction is a unit of program execution that accesses and possibly updates the database. ACID properties: AtomicityEither all work or none work ConsistencyExecution of a transaction in isolation preserves the consistency of the database. IsolationConcurrently executing transactions must be unaware of other concurrently executing transactions. Intermediate transaction results must be hidden from other concurrently executing transactions DurabilityAfter a transaction completes successfully, the changes it made to the database persist, even if there are system failures. Transaction State Active Partial committed Failed Aborted Committed SchedulesA schedule is a sequence that indicates the chronological order in which instructions of concurrent transactions are executed. Schedules must be serializable, and recoverable, for the sake of database consistency, and preferably, cascadeless. Serial and Serializability SerialThe transactions are executed one after the other. A serial schedule preserves database consistency. SerializableA (possibly concurrent) schedule is serializable if it is equivalentto a serial schedule. ConflictInstruction $I_i$ and $I_j$ of transaction $T_i$ and $T_j$ conflict if and only if at least one of them writes. Conflict EquivalentA schedule $S$ that can be transformed into a schedule $Sâ€²$ by a series of swaps of non-conflicting instructions. Conflict SerializableA schedule S that is conflict equivalent to a serial schedule. RecoverabilityA schedule is recoverable if a transaction, $T_j$ , that reads a data item previously written by a transaction $T_i$, commits after $T_i$. Cascading Rollbackwhen a single transaction failure leads to a series of transaction rollbacks. Cascadeless Schedules Schedules where cascading rollback cannot occur. A schedule is cascadeless if, for each pair of transactions $T_i$ and $T_j$ where $T_j$ reads a data item previously written by $T_i$, the read operation of $T_j$ appears before the commit operation of $T_i$. Every cascadeless schedule is also recoverable. Concurrency ControlLock-based ProtocolsLock a mechanism to control concurrent access to a data item. A data item Q can be locked in one of two modes: shared-mode(shared lock) can only read Q. Any number of Transactions can hold lock-s. exclusive-mode (exclusive lock) can both read and write Q. Only one transaction can hold lock-x. A transaction must make a lock request to the concurrency-control manager before accessing a data item.A transaction can proceed only after a request is granted The concurrency control manager should allow only conflict-serializable schedules. starvationan lock -x waits forever because a sequence of lock-s are reading the data Two-phase locking (2PL) protocolPhase 1: Growing PhaseA transaction may obtain or upgrade locks, but may not release any locks. Phase 2: Shrinking PhaseA transaction may release or downgrade locks, but may not obtain any new locks Only part of the conflict serializable schedule can be executed by 2PL Strict 2PLall lock-x held until a transaction commits. Rigorous 2PLall locks held until a transaction commits. Strict and rigorous 2PL schedules are cascadeless. DeadlockEvery transaction is waiting for another transaction. To handle a deadlock, some transactions must be rolled back and its locks released. Deadlock prevention Order lock requests Preemption and/or rollback wait-die scheme (non-preemptive): older wait, younger die wound-wait scheme (preemptive): older kill, younger waits time-based scheme: wait a pre-defined time and roll back Deadlock detectionA wait-for graph is the opposite direction of precedence graph.If there is a cycle in the graph, a deadlock will happen. Deadlock recoveryselect a transaction with the minimum cost as the victim to rollback total or partial. Tree Protocol Only lock-x instructions are allowed. The first lock by $T_i$ may be on any data item. Subsequently, a data item Q can be locked by $T_i$ only if the parent of Q is currently locked by $T_i$. Data items may be unlocked at anytime. A data item that has been unlocked by $T_i$ cannot be locked again by $T_i$. All legal schedules under the tree protocol are conflict serializable. Timestamp-Based ProtocolsThe timestamps determine the serializability order.Two timestamps values are associated with each data item Q:write timestamp WTS(Q) and read timestamp RTS(Q) ReadIf $TS(T_i) &lt; WTS(Q) $ rollbackIf $TS(T_i) ≥ WTS(Q)$$RTS(Q) = max(TS(T_i), RTS(Q))$ WriteIf $TS(T_i) &lt; RTS(Q)$ rollbackIf $TS(T_i) &lt; WTS(Q)$ rollback/ignoreOtherwise $WTS(Q) = TS(T_i) $ Validation-based ProtocolsMost transactions are read-only. The system maintains three timestamps for each transaction: start($T_i$) validation$(T_i$) finish($T_i$) For all transactions $T_k$ with $TS(T_k)&lt;TS(T_i)$ one of the following must hold: finish($T_k$) &lt; start($T_i$) data items read($T_k$) $\cap$ data items read($T_i$) = $\phi$ andstart($T_i$) &lt; finish($T_k$) &lt; validation($T_i$) Multi-version Timestamp Ordering ReadAlways succeed. set $RTS(Q)=TS(T_i)$ WriteIf $TS(T_i) &lt; RTS(Q)$ rollbackIf $TS(T_i) = WTS(Q)$ overwrite contentsIf $TS(T_i) &gt; WTS(Q)$ create new version. set $R/WTS(Q′)=TS(T_i)$ Snapshot IsolationEach transaction works on its own private copy (snapshot) of the data items it reads and writes. First committer wins First updater wins Snapshot isolation does not ensure serializability! Recovery System log-based recoveryA log is a sequence of log records that maintains a record of all the update activities on the database.A log kept on the stable storage.may use log record buffer to output from memory to disk when the buffer is full. deferred database modificationAll modifications are recorded to the log, but all the writes are deferred until after partial commit.The old value is not needed in this scheme. immediate database modificationDatabase updates of an uncommitted transaction are allowed to be made as the writes are issued.Since undoing may be needed, update logs must have both the old value and the new value. checkpointsredo transactions commit before failureundo transactions not yet commit shadow pagingMaintain two page tables during the lifetime of a transaction,The current page table is used for database item accesses during execution of the transaction.The shadow page table is stored in nonvolatile storage for recovery.]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>database</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Differential Equation]]></title>
    <url>%2F2019%2F04%2F10%2FDifferential-Equation%2F</url>
    <content type="text"><![CDATA[First OrderLinearfirst order linear equation standard form: {dy\over dt}+p(t)y=g(t)In some cases it is possible to solve a first order linear equation immediately byintegrating the equation.Unfortunately, most of them should use a method called integrating factor. \mu(t){dy\over dt}+\mu(t)p(t)y=\mu(t)g(t)where ${d\mu\over dt}=p(t)\mu(t)$ so that $\mu(t)=ce^{G(t)}$ where $G(t) = \int g(t)$ SeparableM(x)dx+N(y)dy=0We can directly integral both sides to get the solution. Second Order Linear{d^2y\over dt^2}=f(t,y,{dy\over dt})Linear if $f(t,y,{dy\over dt})=g(t)-p(t){dy\over dt} -q(t)y$ Homogenous EquationWhen $g(t)=0$ the second order linear equation is said to be homogenous y''+p(t)y'+q(t)y=0Constant CoefficientsA second order linear homogenous equation with constant ay’’+by’+cy=0has characteristic equation ar^2+br+c=0The roots $r_1$ and $r_2$ correspond to $y_1=e^{r_1}$ and $y_2=e^{r_2}$Any linear combination of $y_1$ and $y_2$ is a solution. The distribution of the roots have three conditions Two distinct real roots. Just exponential function Two distinct complex roots. By, Euler’s formula, exponential of complex number is Sinusoids. Its real part $u$ and its imaginary part $v$ are also solutions of it. We need to express$y=c_1u+c_2v$ Two identical real roots$r_0$. Then the solution is $y_1=e^{r_0t}\quad y_t=te^{r_0t}$. We can get this result by assume $y_2=v(t)y_1$ and finally we can find $v(t)=c_1+c_2t$ WronskianWhen given a initial point $y_0$, we have the equations: c_1y_1(t_0)+c_2y_2(t_0)=y_0\\ c_1y_1’(t_0)+c_2y_2’(t_0)=y_0’The Wronskian determinant is defined as W=\begin{vmatrix}y_1&y_2\\y_1’&y_2’\end{vmatrix} = y_1y_2’+y_1’y_2It is a function of independent variable t. Thus, different t correspond to different W. If at a point $t=t_0\quad W(t_0)\ne0$ the solution is unique. The constants are c_1={\begin{vmatrix}y_0&y_2(t_0)\\y_0’&y_2’(t_0)\end{vmatrix} \over W(t_0)} \quad c_2={-\begin{vmatrix}y_0&y_1(t_0)\\y_0’&y_1’(t_0)\end{vmatrix}\over W(t_0)}For the equation $y_0=c_1y_1+c_2y_2$ If at a point $t=t_0\quad W(t_0)=0$ the initial conditions cannot be satisfied not matter how $c_1$ and $c_2$ are chosen. $y=c_1y_1(t)+c_2y_2(t)$ is called the general solution.$y_1$ and $y_2$ are said to form a fundamental set of solutions if Wronskian is nonzero. Abel TheoremIf $y_1$ and $y_2$ are solutions of L[y]=y’’+p(t)y’+q(t)y=0Where p and q are continuous on an open interval $I$, then the Wronskian is given by W(y_1,y_2)(t)=c e^{-\int p(t)dt}Therefore, $W(y_1,y_2)(t)$ is either zero or else is never zero for all t in the interval. Nonhomogeneous EquationsA nonhomogenous equation is L[y]=y’’+p(t)y’+q(t)y=g(t)\ne 0We can solve it by thee steps: Find the general solution $c_1y_1(t) +c_2y_2(t)$ of the corresponding homogenous equation. Find some single solution $Y(t)$ of the nonhomogenous equation. Often this solution is referred to as a particular solution. Form the sum of the functions found in steps 1 and 2. Method of Undetermined CoefficientsMake an initial assumption about the form of the particular solution $Y(t)$. But with the coefficients left unspecified.e.g. for $y’’-3y’-4y=3e^{2t}$ we guess that $Y(t)=Ae^{2t}$ and finally obtain $A=-{1\over 2}$ ==VERY IMPORTANT==If one of the terms of our first guess $Y(t)$ is included in the general solution to the corresponding homogeneous equation, try $tY,t^2Y,…$ and so on. Series Solutions of Second Order Linear EquationWe now consider the second order linear equations when the coefficients are functions of the independent variable. P(x){d^2y \over dx^2 }+ Q(x){dy\over dx} +R(x)y =0A point $x_0$ where $Q(x_0)\over P(x_0)$ is analytic is called an ordinary point, otherwise is called singular point. Solutions near ordinary point set $y=\sum_{n=0}^\infty a_nx^n$ Then we have the kth derivative $y^{(k)}=\sum_{n=k}^\infty n(n-1)…(n-k+1)a_nx^{n-k}$ substitute the series for the original function to get the recurrence relation of the coefficients $a_n$ Usually we need to shift the index of summation by replacing $n$ by $n-sth$ The two initial items are the arbitrary coefficientsy=a_0y_1+a_1y_2=a_0\sum...+a_1\sum...where $y_1,y_2$are two power series solutions that are analytic at $x_0$. The radius of convergence for each of the series solutions $y_1$ and $y_2$ is at least as the minimum of the radii of convergence of the series for $Q\over P$ and $R \over P$ Solutions near singular pointWe first consider a relatively simple differential equation that has a singular point, Euler equation, singular point at $x=0$. Euler equationL[y]=x^2y''+\alpha xy' +\beta y = 0set $y=x^r$ we can obtain $r_1,r_2={-(\alpha-1)\pm\sqrt{(\alpha-1)^2-4\beta}\over 2}$Three possibilities: Real, distinct roots $y=c_1x^{r_1}+c_2x^{r_2}$ Equal roots $y=(c_1+c_2lnx)x^{r_1}$ Complex roots $y=c_1x^\lambda cos(\mu lnx)+c_2x^\lambda sin(\mu lnx)$ Regular singular pointP(x)y'' +Q(x)y' +R(x)y=0where $x_0$ is a singular point. This means that $P(x_0)=0$ and that at least one of $Q$ and $R$ is not zero at $a_0$ weak singularities$\lim_{x\to x_0}(x-x_0){Q(x)\over P(x)}$ is finite and $\lim_{x\to x_0}(x-x_0)^2{R(x)\over P(x)}$ is finite This means the singularity in $Q/P$ can be no worse than $(x-x_0)^{-1}$ and the singularity in $R/P$ can be no worse than $(x-x_0)^{-2}$. Such a point is called a regular singular point. Otherwise, called irregular singular point. Laplace TransformAmong the tools that are very useful for solving linear differential equations are integral transforms.An integral transform is a relation of the form F(s)=\int_\alpha^\beta K(s,t)f(t)dywhere $K(s,t)$ is given function, called the kernel of the transformation. When $K(s,t)=e^{-st}$ the transform is called Laplace transforms. \mathcal L\{f^{(n)}(t)\}=s^n\mathcal L\{f(t)\}-s^{n-1}f(0)-...-sf^{(n-2)}(0)-f^{(n-1)}(0)ProcedureTo solve a second order linear equation with constant coefficients ay''+by'+cy=f(t)By Laplace transform, we have $a[s^2Y(s)-sy(0)-y’(0)]+b[sY(s)-y(0)]+cY(s)=F(x)$where the $F(s)$ is the transform of $f(t)$. Then we find that $Y(s)={(as+b)y(0)+ay’(0)+F(s)\over as^2+bs+c}$ Then the $y(t)$ whose LT is $Y(t)$ is the solution Unit step function\mathcal L\{u(t-c)\}=\int_c^\infty e^{-st}dt={e^{-cs}\over s}\quad s>0If $F(s)=\mathcal L\{f(t)\}$ exists for $s&gt;a\ge0$, and if $c$ is a constant, then \mathcal L\{e^{ct}f(t)\}=F(s-c), \quad s>a+cConversely, if $f(t)=\mathcal L^{-1}\{F(s-c\}$, then $e^{ct}f(t)=\mathcal L^{-1}\{F(s-c)\}$ Systems of First Order Linear EquationsSystems of a homogeneous linear equations with constant coefficients are of the form$\boldsymbol{x}’=A\boldsymbol x$ where $A$ is a constant $n \times n$ matrix. Solution By setting $\boldsymbol x =\xi e^{rt}$, we can get $r\xi e^{rt}=A\xi e^{rt}$. Therefore, $(A-r\boldsymbol I)\xi=0$.where $\boldsymbol I$ is the $n\times n$ identity matrix. Solve $det(A-r\boldsymbol I) = 0 $We got $r$, the eigenvalues of $A$. Each one correspond to a constant eigenvectors $\xi^{(i)}$ For phase portrait, $r0$ means outward direction. Possibilities of eigenvalues All eigenvalues are real and different from each other.$\boldsymbol x = c_1\xi^{(1)}e^{r_1t}+…+c_n\xi^{(n)}e^{r_nt}$ Some eigenvalues occur in complex conjugate pairs.$\boldsymbol x^{(1)}(t)=\xi^{(1)}e^{r_1t}\quad \boldsymbol x^{(2)}(t)=\bar\xi^{(1)}e^{\bar r_1t}$ if $r_1 = \lambda +\mu \quad\xi = \boldsymbol a +i\boldsymbol b$, we can write $\boldsymbol x^{(1)}(t)=\boldsymbol u(t) + i\boldsymbol v(t)$$\boldsymbol u(t) = e^{\lambda t}(\boldsymbol a cos \mu t-\boldsymbol b sin \mu t)\\\boldsymbol v(t) = e^{\lambda t}(\boldsymbol a sin\mu t+\boldsymbol b cos\mu t)$ Then the general solution is$\boldsymbol x= c_1\boldsymbol u(t) +c_2\boldsymbol v(t) +c_3\xi^{(3)}e^{r_3t}+…+c_n\xi^{(n)}e^{r_nt}$ Some eigenvalues, either real or complex, are repeated.TBC…]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>ODE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Probability]]></title>
    <url>%2F2019%2F03%2F21%2FProbability%2F</url>
    <content type="text"><![CDATA[Basic Concepts random experimentan experimental procedure and one or more measurements or observations. sample spacethe set of all possible outcomes. discrete sample space if the number of outcomes is countable continuous sample space if not countable eventcertain conditions for an outcome. A subset A of the sample space S An event A occurs if the outcome is a member of A. the certain event, S, consists of all outcomes. the null event, $\phi$, contains no outcomes. An elementary event contains only one outcome. Replacement and OrderingWe can choose $k$ objects from a set $A$ that has $n$ members indifferent ways Replacement: With replacementAfter selecting an object and noting its identity, the object is put back before the next selection. Without replacementThe object is not put back before the next selection. Ordering: With orderingThe order in which we draw the objects is recorded Without orderingOnly the identity and number of times each object is drawn is important Replacement ordering number of outcomes Y N - Y Y $n^k$ N N ${n\choose k}={n!\over (n-k)! k!}$ N Y ${n\choose k}k!={n!\over(n-k)!}$ Note: when with replacement and without ordering, outcomes are NOT equally probable. There are two ways to choose a same object. But they counts the same outcome. Conditional Probability Independence$P[A\cap B]=P[A]P[B]$ also $P[A|B] = P[A]$ P[A_1A_2...A_n]=\prod_{i=1}^n P[A_i] Exclusive P[A\cup B]=P[A]+P[B]-P[A\cap B] = P[A]+P[B]P[A_1A_2...A_n] = 0 Conditional probability$P[A|B]={p[A\cap B]\over p[B]}$ where we assume that $P[B]&gt;0$ Total Probability Theorem P[A]=\sum_{i=1}^n P[A|B_i]P[B_i] Bayes’ Rule P[B_j|A]={P[AB_j]\over P[A]}={P[A|B_j]P[B_j]\over\sum_{k=1}^n P[A|B_k]P[B_k]} Sequential ExperimentsExperiments that involve repetitions or multiple participants can often be viewed as a sequence of sub-experiments. The sub-experiments can be identical or non-identical, dependent or independent. The individual sample spaces can be identical or non-identical. If the experiments are identical, the individual sample spaces are identical but not vice versa. Tossing a coin n times: repetition independent identical sub-experiments identical individual sample spaces Checking the number of students sleeping in class: multiple participants independent? (maybe not. Others sleep then I sleep) identical individual sample spaces non-identical sub-experiments (Dalaos do not sleep but I do) Bernoulli trialsan experiment once with two possible outcomes Binomial probability lawn independent Bernoulli trials p_n(k)={n\choose k}p^k(1-p)^{n-k} Multinomial probability lawM partition of sample space, so $p_1…p_M$ mutual exclusive P[(k_1,k_2,...,k_M)]={n!\over k_1!k_2!...k_M!}p_1^{K_1}p_2^{K_2}...p_M^{K_M} Geometric probability law M be the time until the first successp_M(m)=p(1-p)^{m-1} N be the time before the first successp_N(n)=p(1-p)^n Sequences of dependent experimentsReferred to as a Markov Chain Single Random VariablesA random variable $X$ is a function that assigns a number to everyoutcome $\xi$ of an experiment. Underlying sample space $S$ is called the domain of the RV. $\xi\in S$Set of all possible values of $X$ called the range of the RV. $X(\xi)\in S_X$ Equivalent Event$A=\{\xi:X(\xi)\in B\}$ CharacteristicPMFprobability mass function p_X(x)=P[X=x]=P[\{\xi:X(\xi)=x\}]For continuous RV, $P[X=x]=0$. $p_X(x)&gt;0$ $\sum_{x\in S_x}p_X(x)=1$ $P[X\ in\ B]=\sum_{x\in B}p_X(x)$ where $B\in S_X$ CDFcumulative distribution functionThe summation of PMF, the integral of PDF, is the cumulative distribution(CDF). F_X(x)=P[X\le x]\\=\sum_{n=-\infty}^x p_X(n)=\int_{-\infty}^xf_X(s)ds Discrete RV: like a series of stairs jumping up. Continuous RV: a continuous increasing function. Mixed RV: continuous function with discrete jumps where $P[X=x_j]=c$ PDF probability density function. The value is not a probability! P[a\le X\le b]=\int_a^bf_X(x)dx\\f_X(x)={d\over dx}F_X(x)For discrete RV: PMF multiply an impulse $\delta(x-x_k)$. MeanExpectation/Mean of a RV (if the sum/integral converges absolutely) m_X = E[X]=\sum_{x\in S_x}xp_X(x) \\E[X]=\int_{-\infty}^\infty xf_X(x)dxExpectation/Mean of a function of a RV $Y=g(x)$ E[Y]=\sum_k g(x_k)p_X(x_k)\\E[Y] = \int_{-\infty}^\infty g(x)f_X(x)dxLinearity of ExpectationGenerally $E[\sum_i a_ig_i(X)]=\sum_i a_iE[g_i(X)] \ne \sum a_ig_i(E[X])$if $g_i$ are all linear, the the tree above equal. VarianceVar[X] = E[D^2]=E[(X-E[X])^2]\\Std[X]=\sqrt {Var[X]}Useful formula$Var[X] = E[X^2]-(E[X])^2 \\Var[cX] = c^2Var[X]$ MomentsThe Mean and variance are examples of the moments of a RV. The $n^{th}$ moment of a RV E[X^n] = \sum_kx_k^np_X(x_k)\\E[X^n] = \int_{-\infty}^\infty x^nf_X(x)The $n^{th}$ central moment of a RV E[(X-E[X])^n] = \sum_k(x_k-E[X])^np_X(x_k)\\E[(X-E[X])^n] = \int_{-\infty}^\infty(x-E[X])^nf_X(x_k)Important moments:1st moment, $E[X]$ is the Mean2nd central moment, $E[(x-E[x])^2]$ is the variance Minimum Mean Squared Error EstimationMSE(c) = E[(X-c)^2]=\int_{-\infty}^\infty(x-c)^2f_X(x)dxThus, the best guess is $c=E[X]$!Also, the variance is ten the minimum mean squared error associated with the best guess. Discrete RVA discrete random variable assumes values from a countable set. $S_X=\{x_1,x_2,…\}$ A discrete random variable is finite if its range is finite.$S_X=\{x_1,x_2,…,x_n\}$ BernoulliX only take two values, either 1 or 0. P_X[0] = 1-p\quad P_X[1] = pMean $E[X]=p$Variance $Var[X] = p(1-p)$ Single coin toss Occurrence of an event of interest BinomialThe number of times that an event occurs. P_X[k]={n\choose k} p^k(1-p)^{n-k}\quad\text{for k = 0,1,2...}Mean $E[X] = np$Variance $Var[X]=np(1-p)$ Multiple coin flips Occurrence of a property in individuals of a population(e.g. bit errors in a transmission) When $n\to\infty$ , turn to Poisson RV GeometricSuppose a random experiment is repeated, In each repeat, it occurs independently and with probability $p$ TrialsThe number of trials until the first success occurs. P_X[k]=(1-p)^{k-1}p\quad\text{for k = 1,2,...}Mean $E[X]={1 \over p}$Variance $Var[X] = {1-p\over p^2}$ FailuresThe number of failures before the first success, $X’ = X-1$ is also a geometric RV. P_{X’}[k]=(1-p)^kp\quad\text{for k = 0,1,2...}Mean $E[X‘]={1-p \over p}$Variance $Var[M’]={1-p\over p^2}$ Number of transmissions required until an error free transmission. Memoryless propertyNo matter how hard you ever tried, the probability to succeed remains the same. Uniform(discrete)Values in a set of integers are with equal probability. S_X=\{0,1,2...,M-1\}\\ P_X[k]={1\over M}\quad \text{for k }\in S_xMean $E[X] = {M-1\over 2}={a+b\over 2}$Variance $Var[X]={M^2-1\over 12}={(b-a+1)^2-1\over12}$ PoissonThe number of occurrences of an event in a certain interval of time or space. P_X[k]={\alpha^k \over k!}e^{-\alpha}\quad \text{for k=0,1,2...}The parameter $\alpha$ is the average number of events in the interval. Mean $E[N]=\alpha$Variance $Var[N]=\alpha$ Number of hits on a website in one hour number of particles emitted by a radioactive mass in a fixed time period Relationship with Binomial RV:For a Binomial random variable with $p = {\alpha\over n}$:$p_0=(1-p)^n = (1-{\alpha\over n})^n \to e^{-\alpha} \quad\text{as}\ n\to\infty$${p_{k+1}\over p_k} = {(n-k)p\over(k+1)(1-p)}={(1-k/n)\alpha\over(k+1)(1-\alpha/n)}$when$n\to\infty\quad{p_{k+1}\over p_k}={\alpha\over k+1}$ Continuous RVUniform(continuous)Intervals of the same length on the domain have the same probability. S_X=[a,b]\\ f_X(x)={1\over {b-a}}\quad \text{for x }\in S_x\\ F_X(x)={x-a\over {b-a}}\quad \text{for x }\in S_xMean $E[X] = {a+b\over 2}$Variance $Var[X]={(b-a)^2\over 12}$ ExponentialThe length of time that wait next train. $\lambda$ is the occurrence time per unit time. f_X(x)=\lambda e^{-\lambda x}\quad\text{when }x\gt 0\\ F_X(x)=1-e^{-\lambda x}\quad\text{when }x\gt 0Mean $E[X]={1\over\lambda}$Variance $Var[X] = {1\over \lambda^2}$ Memoryless propertyNo matter how hard you ever wait, the probability to meet the right one remains the same. Gaussian(Normal)Variables that tend to occur around a certain value,m , the mean. f_X= {1\over \sqrt{2\pi}\sigma}e^{-(x-m)^2\over 2\sigma^2}\we define the CDF of the”normalized” Gaussian with mean m=0 and standard deviation $\sigma=1$ as \Phi(x)={1\over \sqrt{2\pi}}\int_{-\infty}^x e^{-t^2 \over 2}dtThen the CDF of a Gaussian RV, $X$, with mean $m$ and standard deviation $\sigma$ is F_X(x)=\Phi({x-m\over \sigma})={1\over \sqrt{2\pi}}\int_{-\infty}^{x-m\over\sigma} e^{-t^2 \over 2}dt Q-function$Q(x) = 1-\Phi(x)=P[X&gt;x]$By symmetry, $Q(0)={1\over 2}$ and $Q(x)=\Phi(-x)$ Conditional Conditional PMF/CDF/PDF p_X(x_K|C)={P[\{X=x_k\}\cap C]\over P[C]}\\F_X(x|C)={P[\{X\le x\}\cap C]\over P[C]}\\f_X(x|C)={d\over dx}F_X(x|C) Total probability p_X(x)=\sum_i p_X(x|B_i)P[B_i] Conditional expected value m_{X|B}=E[X|B]=\sum_{x\in S_X}xp_X(x|B) Conditional variance VAR[X|B]=E[(X-m_{X|B})^2|B]=E[X^2|B]-m_{X|B}^2 Compute Everything by Conditioning f_X(x)=\sum_i f_X(x|b_i)P[B_i]\\E[X]=\sum_i E[X|B_i]P[B_i] TransformationGiven a random variable $X$ with known distribution and a real valued function $g(x)$, such that $Y = g(X)$ is also a random variable. Find the distribution of $Y$. Type of Y If X is discrete, Y must be discrete. If X is continuous, Y can be all three types, depends on $g(x)$ Key ideaTo find equivalent events in X for suitably defined events in Y. Approaches If Y is Discrete, find the PMF at the possible values of Y$p_Y(k) = P[Y=k]=P[\{X:g(X)=k\}]$ If Y is continuous, there are two appsroaches: Find the CDF of Y$F_Y(y)=P[Y\le y]=P[\{X:g(X)\le y\}]$ Find the PDF of Y$f_Y(y_0)=\sum_{x:g(x)=y}{f_X(x_)\over |g’(x)|}$VERY USEFUL $\uparrow\uparrow\uparrow\uparrow\uparrow$ Multiple Random Variables vector random variableA vector random variable $\vec X$ is a function that assigns a vector of real numbers to every outcome of an experiment. One random variable can be considered as a mapping from the sample space to the real line.Two random variables can be considered as a mapping from the sample space to the plane. scattergramsvisualize the joint behavior of two RVs. PairsDT case Joint PMF p_{X,Y}(j,k)=P[\{X=j\}\cap \{Y=k\}] Marginal PMF p_X(j)=P[X=j,Y=anything]The function of $X$ is the sum of all $Y$ along y-axis. PMF of FunctionSuppose that $Z=g(X,Y)$, is an integer valued function p_Z(k)=P[\{Z=k\}]=\sum_{(i,j):g(i,j)=k}p_{X,Y}(j,l) CT case Joint CDF F_{X,Y}(x,y)=P[\{X]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>probability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fourier Transform]]></title>
    <url>%2F2019%2F03%2F21%2FFourier-Transform%2F</url>
    <content type="text"><![CDATA[Fourier SeriesCTFSany T-periodic functions can be expressed by many Sinusoids $\omega_0 = {2\pi\over T}$ is the fundamental frequency$f_0 = {1 \over T}$ is the fundamental frequency in ordinary frequency Synthesis EquationA weighted sum of infinite Sinusoidsx(t) =\sum_{k=-\infty}^{\infty} a_ke^{jk\omega_0t} Analysis Equation CTcalculate the coefficients of each Sinusoidsa_k = {1\over T}\int_0^Tx(t)e^{-jk\omega_0t}dtThe coefficient $a_k$ is aperiodic. Intuitionthe analysis equation is a normalized inner product that computes a projection coefficient that specifies how much $x(t)$ projecting on $e^{jk\omega_0t}$ . The Projection is The normalization process is Tricks $2Acos\omega t\to a_{\omega\over\omega_0}=A\quad a_{-\omega\over\omega_0}=A$$2Asin\omega t\to a_{\omega\over\omega_0}=-jA\quad a_{-\omega\over\omega_0}=jA$ DTFSany N-periodic signal can be expressed by N distinct harmonics. $\omega_0 = {2\pi\over N}$ is the fundamental frequency, must contain $\pi$ otherwise not periodic$f_0 = {1 \over N}$ is the fundamental frequency in ordinary frequency Synthesis Equationa weighted sum of N complex sinusoids. x[n] = \sum_{k=0}^{N-1} a_ke^{jk\omega_0n} Analysis Equation DTcalculates the FS coefficients which are the weights for the harmonics. a_k = {1 \over N}\sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}The coefficient $a_k$ is $N$ periodic ==IMPORTANT NOTE==:for N is even, $a_{N\over 2} $ is definitely real. It has no conjugates. i.e. $N=4$ The DTFS of $2cos({2\pi\over4}n)+2cos({2\pi\over2}n)$ is $a_{-1}=1,a_0=0,a_1=1,a_2=2$ Properties Fourier TransformRegarding aperiodic signals as periodic signals in the limit of period T going to infinity. CTFTRecall that $a_k = {1\over T}\int_{t\in T}x(t)e^{-jk\omega_0t}dt$ is the projection coefficient that $x(t)$ on $e^{-jk\omega_0t}$.However, when $T\to\infty\quad a_k\to0$, so we define $X_T(j\omega)=\int_{-T/2}^{T/2}x(t)e^{-j\omega t}dt$.Therefore, $X_T(j{k2\pi\over T}) = Ta_k$ . We rewrite the Synthesis Equation with $\omega_0={2\pi\over T}$ : x(t) = \sum_{k=-\infty}^{\infty}{\omega_0\over 2\pi}X_T(jk\omega_0) e^{jk\omega_0t}as $T\to\infty\quad \omega_0\to0$, we can rewrite it in integral form. Synthesis Equationconsider the signal as a superposition of complex sinusoids with density x(t) = {1\over 2\pi}\int_{-\infty}^\infty X(j\omega)e^{j\omega t}d\omega = \mathcal{F}^{-1}\{X(j\omega)\} Analysis Equationcalculate the density of complex sinusoids with different frequency X(j\omega)=\int_{-\infty}^{\infty}x(t)e^{-j\omega t}dt = \mathcal{F}\{x(t)\} Duality propertyThere is symmetry in the FT(Fourier Transform) and IFT(Inverse Fourier Transform) integrals.They are identical except for a factor $2\pi$ and a change in sign. Examples Window$x(t) = \text{window width }2T_1 \iff X(j\omega) = {2sin\omega T_1\over \omega}$ Complex Sinusoid$x(t) = e^{j\omega_0t} \iff X(j\omega) = 2\pi\delta(\omega-\omega_0)$ Periodic SignalFT integral of a FS sum equals the sum of a FT integral. X(j\omega) = \int_{-\infty}^{\infty}(\sum_{k=-\infty}^\infty a_ke^{jk\omega_0t})e^{-j\omega t}dt \\= \sum_{k=-\infty}^\infty a_k\int_{-\infty}^{\infty}e^{jk\omega_0t}e^{-j\omega t}dt \\=2\pi \sum_{k=-\infty}^\infty a_k\delta(\omega-k\omega_0)\\=2\pi a_{\omega\over\omega_0} It is discrete(including $\delta()$ ) The value is $2\pi$ the CTFS Periodic Extension FS coefficients of the periodic extension of a signal are the sample values of the signal’s FT (scaled by $1/T$) $a_k={1\over T}X(jk\omega_0)$ DTFTDTFT must be $2\pi$-periodic Synthesis Equationconsider the signal as a superposition of complex sinusoids with densityx[n] = {1\over 2\pi}\int_{2\pi} X(e^{j\omega})e^{j\omega n}d\omega = \mathcal{F}^{-1}\{X(e^{j\omega})\} Analysis Equationcalculate the density of complex sinusoids with different frequencyX(e^{j\omega})=\sum_{n=-\infty}^{\infty}x[n]e^{-j\omega n}dt = \mathcal{F}\{x[n]\} Periodic Signal$X(e^{j\omega})=2\pi\delta(\omega-\omega_0)$ for $0\le\omega&lt;2\pi$ DFTDFT is the same as DTFS, except for a scaling factor DTFS a_k = {1 \over N}\sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}x[n] = \sum_{k=0}^{N-1} a_ke^{jk\omega_0n}DFT X[k] = \sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}x[n] = {1 \over N}\sum_{k=0}^{N-1} X[k]e^{jk\omega_0n}PropertyFor CTFT For DTFT]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>Fourier</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Signal and System]]></title>
    <url>%2F2019%2F03%2F19%2FSignal-and-System%2F</url>
    <content type="text"><![CDATA[Signals What is signal?A signal is some measurable quantity that varies over time and/or space. What are systems?We define system to be anything that takes input signals and produces output signals. Basic Characterization and ManipulationManipulation multiplying signal by constant adding/multiplying signals together Multiplying two straight line segments produces a quadratic curve transformation of the independent variable time shifting $x(t)\to x(t-t_0)$Minus = Left = Delay time reflection/reversal $x(t)\to x(-t)$ time scaling $x(t)\to x(at)$ Combination of shifting and time scalingShift first and scale is easier. First scale requires we also scale the shift. Characterization Even signal $x(-t)=-x(t)$Odd signal $x(-t)=x(t)$Any signal can be viewed as the sum of an even part and an odd part $x_{even}={x(t)+x(-t)\over 2}\quad x_{odd}={x(t)-x(-t)\over 2}$ infinite duration &amp; finite duration A finite duration DT signal is a sequence of N complex numbers. We can treat it as a complex N-vector. right-sided &amp; left-sidedright-sided $u(t-c)$ left-sided $u(-t-c)$ Fundamental SignalsCTComplex Exponentiale^{st} = e^{(\sigma+j\omega)t} = e^{\sigma t}e^{j\omega t}Where $e^{\sigma t}$ is purely real. $\sigma&gt;0$ amplitude grows, $\sigma&lt;0$ amplitude decays.$e^{j\omega t}$ is purely imaginary and called Complex Sinusoids Complex Sinusoids$e^{j\omega t} = cos\omega t +j sin\omega t$ Unit Function$ u(t) = 1$ when $t\ge 0$$ u(t) = 0$ when $t\lt 0$ Impulse SignalDenoted by $\delta(t)$ u(t) = \int_{-\infty}^{t} \delta(\tau)d\tau \\ \delta(t) = {du(t)\over dt} Zero everywhere else, infinite at $t=0$ Unit total area $\int_{-\infty}^{\infty} \delta(t)dt = 1$ Integral is the unit step $u(t) = \int_{-\infty}^{\infty} \delta(\tau)d\tau$ Scaled impulse k$\delta(t)$ has total area k. $\int_{-\infty}^{\infty} k\delta(t)dt = k\int_{-\infty}^{\infty}\delta(t)dt = k $ Sampling property$x(t)\delta(t-t_0) = x(t_0)\delta(t-t_0)$ a scaled delta function Sifting property $\int_{-\infty}^{\infty}x(t)\delta(t-t_0)dt = x(t_0)$ a value at t~0~ Infinite energy. Therefore $\delta(t)$ is an idealization that cannot physically exist. Time Scaling corresponds to Magnitude Scaling $\delta(\alpha t) = {\delta(t)\over \left|{\alpha}\right|}$ DTReal Exponential$ \alpha^n = e^{an} $ where $\alpha = e^a$ $\alpha&gt;1$ growing exponential$0&lt;\alpha&lt;1$ decaying exponential$-1&lt;\alpha&lt;0$ decaying oscillating$\alpha&lt;-1$ growing oscillating Real Sinusoids$cos(\omega n) = cos(2\pi fn)$ Periodic only if its ordinary frequency $f$ is rational number $f$ and $f\pm m(integer)$ are the same frequency. $\omega$ and $\omega\pm m2\pi$ are the same frequency The fastest rate at which a DT signal can oscillate is at an ordinary frequency of $f={1\over2}$ or angular frequency of $\omega = \pi$Complex Exponential $z^n$ where $z = e^s = e^{\sigma + j\omega} = e^\sigma e^{j\omega}$ $z^n = \left|z\right|^n e^{j\omega n}$$\left|z\right|^n$is real exponential $ e^{j\omega n}$ is complex sinusoid Complex Sinusoid$x[n] = e^{j\omega n} = cos \omega n + j sin \omega n$ Periodic iff $\omega = {m2\pi \over N}$ $e^{j\omega n} = e^{j(\omega +2k\pi)n}$ frequency is periodic for DT signal, and the highest angular frequency is $\pi$ Unit Step$u[n] = 1$ when $n\ge 0$$u[n] = 0$ when $n\lt 0$ Unit Impulse$\delta[0] = 1$ other place 0.Property same as CT. Relations with Unit step the first difference between two unit step signals.$\delta[n] = u[n]-u[n-1]$ unit step can be expressed as a sum of impulse by two ways $u[n]\sum_{m=-\infty} ^n \delta[m]$ $u[n]\sum_{k=0}^\infty \delta[n-k]$ the sampling property for $\delta[n]$when multiply $x[n]$ and $\delta[n]$, only one sampled value, not a continuous function anymore. Complex FrequencyCT and DT Complex Frequency $z = e^s = e^{\sigma+ j\omega}$ DT CT Growing $ z &gt;1$ $\sigma&gt;0$ Decaying $ z &lt;1$ $\sigma&lt;0$ Oscillatory frequency $\ang z$ $\omega$ SystemBasic System Characterization Memoryless: instantaneous Causality: does not depend on future Stability: BIBO(Bounded-Input Bounded Output) Invertibility: distinct inputs lead to distinct outputs Time-invariant Linearity Time InvarianceShifted input shifted output. Operation of time does not depends on time. $ y(t) = \int_{t-3}^{t+2} x(\tau)d\tau$ YES : 3 time before and 2 later$ y(t) = \int_{0}^{t} x(\tau)d\tau$ NO: has a beginning point 0 Linearitysuperposition property can be decomposed into two Additive property: $x_1(t) \to x_2(t) = y_1(t) \to y_2(t)$ Homogeneity (scaling): $ax_1(t) \to ay_1(t)$ LTI must satisfy zero-input/zero-output Elementary Linear Operation: Differentiation and integration: $x^{(k)}(t)$ ( LTI! ) Time shifting:$x(t-\tau)$ ( LTI! ) Multiply by a function: $g(t)x(t)$ (not TI unless $g(t)$ is constant) time reversal, time scaling ( not TI because there is a origin) Impulse ResponseDenoted by $h(t)$ or $h[n]$, is the output of $\delta(t)/\delta[t]$ in a LTI system. If the output of a system is given by convolving the input with a $h[n]$, then the system is LTI! Convolution is LTI Typical Examples: $h[n] = \delta[n]$ identity function. Do nothing. $h[n] = \delta[n-m]$ delay system. Delay the input by m. $h[n] = \delta[n]+0.3\delta[n-4]$ echo system. With echo volume 0.3. $h[n] = u[n]$ first sum system. $y[n] = \sum_{-\infty}^n x[k]$ $h[n] = {1\over 3} (u[n] - u[n-3]) = {1\over 3}(h[n] + h[h-1] + h[n-2])$ window smoother. $h[n] = \delta[n] -\delta[n-1]$ first difference system. LTI SystemsLinearity ensure thatIf $\delta[n-k]\to h_k[n]$ we can imply$x[n] = \sum_{k=-\infty}^{\infty} x[k]\delta[n-k]\to y[n] = \sum_{k=-\infty}^{\infty} x[k]h_k[n-k]$ Time Invariance ensure thatIn fact $h_k[n]=h[n-k]$$x[n] = \sum_{k=-\infty}^{\infty} x[k]\delta[n-k]\quad y[n] = \sum_{k=-\infty}^{\infty} x[k]h[n-k]$ For CT case$x(t) = \int_{-\infty}^{\infty}x(\tau)\delta(t-\tau)d\tau\quad y(t) = \int_{-\infty}^{\infty}x(\tau)h(t-\tau)d\tau$we use $$ to denote it.$y(t)=x(t)h(t)$ Convolution/LTI properties: commutative $x(t) \ast h(t) = h(t) \ast x(t)$ distributive $x(t) \ast\{h_1(t) + h_2(t)\} = x(t)\ast h_1(t) +x(t) \ast h_2(t)$ associative$\{x(t)\ast h_1(t)\}\ast h_2(t) = x(t)\{h_1(t)h_2(t)\}$ Charactering LTI system by Impulse Responses: Memoryless &lt;=&gt; $h[n] = 0 $ for $n\ne0$ Causality &lt;=&gt; $h[n] =0$ for $n\lt0$ Stability &lt;=&gt;impulse response is absolute integrable/summable $\int_{-\infty}^{\infty}\left|h(t)\right|dt \lt \infty$ or $\sum_{n=-\infty}^{\infty}\left|h(t)\right|dt \lt \infty$ If the impulse response of the system is a unit step function, the system is a integral/first sum system. Conceptually computingFor the DT convolution $y[n]=x[n]*h[n]=\sum_{k=-\infty}^\infty x[k]h[n-k]$ draw the original $x[k]$ and $h[k]$ with variable $k$ flip $h[k]$ to get $h[-k]$, the convolution kernel shift $h[-k]$ right by $n$ to obtain $h[n-k]$ Multiple $x[k]$ by $h[n-k]$ and sum over all results System Function and Frequency ResponseSystem Functiony[n] = \sum_{k=-\infty}^\infty h[k]x[n-k] = H(z)x[n] \\ H(z) = \sum_{k=-\infty}^{\infty} h[k]z^{-k}y(t) = \int_{\tau=-\infty}^\infty h(\tau)x(t-\tau) = H(s)x(t) \\H(s) = \int_{-\infty}^{\infty}h(\tau)e^{-s\tau}d\tau$z^n $/$ e^{sn}$ is an eigenfunction of DT/CT LTI systems. $H(z)$ or $H(s)$ is the eigenvalue that depends on the complex frequency $z$. They are also called system function of CT/DT LTI systems. $H(s)$ is a Laplace Transform of the CT impulse response $h(t)$$H(z)$ is a z-transform of the DT impulse response $h[n]$ Frequency ResponseFor LTI System, the eigenvalue at different oscillation frequency $\omega$ is called frequency response. $H(s)e^{st} = H(j\omega)e^{j\omega t}$ when $s=\sigma + j\omega$ and $\sigma=0$where $H(j\omega)$ is the cross-section of $H(s)$ along $j\omega$ axis. $H(z)z^{n} = H(e^{j\omega})e^{j\omega n}$ when $|z|= 1$ and $\ang z=\omega$where $H(e^{j\omega})$is the value of system function $H(z)$ along the unit circle $z=e^{j\omega}$ $H(j\omega)=H(s)$ given that $s=j\omega$$H(e^{j\omega})=H(z)$ given that $|z|=1;z=e^{j\omega}$]]></content>
      <categories>
        <category>Signal is Everything</category>
      </categories>
      <tags>
        <tag>Fourier</tag>
        <tag>signal</tag>
        <tag>system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Micro Economics]]></title>
    <url>%2F2019%2F01%2F30%2FMicro-Economics%2F</url>
    <content type="text"><![CDATA[[TOC] Ten PrinciplesHow people make decisions People Face Trade-offs The Cost of Something Is What You Give Up to Get It Rational People Think at the Margin People Respond to Incentives How people interact Trade Can Make Everyone Better Off Markets Are Usually a Good Way to Organize Economic Activity Governments Can Sometimes Improve Market Outcomes How the Economy as a whole works A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services Prices Rise When the Government Prints Too Much Money Society Faces a Short-Run Trade-off between Inflation and Unemployment Think like an Economist Scientist: Observation Theory and More Obeservation Assumption Model: Circular-Flow Diagram and Production Possibilities Frontier Police advisor: Positive vs Normative Statement Two hands Not accepted Disagree with each other Scientific Judgement Value Perception vs Reality Interdependence and Gains from Trade Absolute advantage the ability to produce a good using fewer inputs than another producer. Opportunity costwhatever must be given up to obtain some item. Comparative advantagethe ability to produce a good at a lower opportunity cost than another producer.One should do what he has lower comparative advantage and export them. Price of the trade is between the two opportunity costs. Market Forces of Supply and DemandDistinguish between demand and quantity demanded, supply and quantity supplied.For both curve, increase means a shift to right, decrease means a shift to left. Shifts in the Demand CurveDeterminants: Income normal good low income low demand inferior good low income high demand. Prices of Related Goods substitutes one price increase, quantity decrease. another demand increase. Both price and quantity increase. complements one price increase, another demand decrease. Both price and quantity decrease. Tastes Expectations Number of Buyers: more means more demand. Shifts in the Supply CurveDeterminants: Input PricesInput price rise, supply decrease. TechnologyIncrease supply. Expectations: Number of Sellers: more means more supply. ElasticityA measure of the responsiveness of quantity demanded or quantity supplied to a change in one of its determinants. DemandThe Price Elasticity of DemandA measure of how much the quantity demanded of a good responds to a change in the price. Determinants for Price Elasticity of Demand: Availability of Close Substitutes: goods with close substitutes are more elastic. Necessities vs Luxuries: Necessities tend to have inelastic demands. Definition of the Market: Narrowly defined markets tend to have more elastic demand than broadly defined markets because it is easier to find close substitutes. Time Horizon: Goods tend to have more elastic demand over longer time horizons. Computing $E_d=\Delta Q_d/\Delta P$ Price elasticity = Percentage change in quantity demanded / Percentage change in price Percentage change are often calculated by midpoint method. Variety In Demand Curve, flatter(horizontal) means elastic, steeper (vertical) means inelastic. Linear demand curve points with a low price and high quantity(right) are inelastic, while points with a high price and low quantity(left) are elastic. Other Demand Elasticities $E_i=\Delta Q_d/\Delta I$ The Income Elasticity of Demand = Percentage change in quantity demanded / Percentage change in income. $E_c=\Delta Q_{d,a}/\Delta P_b$ The Cross-Price Elasticity of Demand = Percentage change in quantity demanded of good A / Percentage change in the price of good B Total Revenue Total Revenue = Price x Quantity. $TR = P \times Q$ elasticity &lt; unit elasticity = 1: P up R up.$E&lt;1\quad P\uparrow R\uparrow$ elasticity &gt; unity elasticity = 1: P up R down.$E&gt;1\quad P\uparrow R\downarrow$ SupplyThe Price elasticity of supplyA measure of how much the quantity supplied of a good responds to a change in the price. Determinants for Price Elasticity of Supply: Flexibility of sellers to change the amount of good. Time period considered.(Most markets) Computing $E_s=\Delta Q_s/\Delta P$ Price elasticity of supply = Percentage change in quantity supplied / Percentage change in price Variety In Supply. Curve, flatter(horizontal) means elastic, steeper (vertical) means inelastic. Example and Application Good farming news may harm farmers: Supply shifts right, demand remain the same, elasticity greater than 1, total revenue decrease. OPEC fall to keep oil price high: Supply is elastic over long period. Curve becomes flatter and flatter. Drug interdiction may increase drug-related crime: Supply shifts left, demand remain the same, elasticity less than 1, total revenue increase. Instead, to avoid drug-related crime, we should spend resource on demand side, like drug education. Supply Demand and Government Policiesprice control price ceilinga binding price ceiling causes a shortage. Gas Pump. A long lines. Rent Control. the initial shortage caused by rent control is small. landlords use various mechanisms to ration housing. waiting list, preference, discrimination, under-the-table payments. price floora binding price floor causes a surplus. The Minimum Wage: unemployment. great impact on teenage labor. More of them tend to work instead of study. TaxWhen the government levies a tax on a good, the equilibrium quantity of the good falls. That is, a tax on a market shrinks the size of the market. Taxes levied on sellers and taxes levied on buyers are equivalent. Can Congress Distribute the Burden of a Payroll Tax? Doesn’t matter…Tax incidence depends on the forces of supply and demand A tax burden falls more heavily on the side of the market that is less elastic(inelastic). Who Pays the Luxury Tax? Producer! CS PS and Efficiency of MarketsConsumer Willingness to Paythe maximum amount that a buyer will pay for a good Consumer Surplus the amount a buyer is willing to pay for a good minus the amount the buyer actually pays for it. The area below the demand curve and above the price measures the consumer surplus in a market lower price raises CS(consumer surplus). it measures the benefit that buyers receive from a good as the buyers themselves perceive it. Producer Cost the value of everything a seller must give up to produce a good. A measure of producer’s willingness to sell. Producer Surplus the amount a seller is paid for a good minus the seller’s cost of providing it. The area below the price and above the supply curve measures the producer surplus in a market. higher price raises PS(producer surplus) Market Total SurplusCS+PS. the sum of consumer and producer surplus. Efficiencythe property of a resource allocation of maximizing the total surplus received by all members of society Equalitythe property of distributing economic prosperity uniformly among the members of society. Market in Organs Not fair Market Failurethe inability of some unregulated markets to allocate resources efficiently. The Costs of TaxationDeadweight Lossthe fall in total surplus that results from a market distortion, such as a tax. the elasticities of supply and demand determine the size of the deadweight loss from a tax.the greater the elasticities of supply and demand, the greater the deadweight loss of a tax. The Deadweight Loss Debate Some economists believe deadweight loss is small because labor supply is fairly inelastic. Some economists believe deadweight loss is large because labor supply is more elastic. adjust number of hours they work second earners elderly can choose when to retire consider engaging in underground economy When Taxes VaryDeadweight loss increase as $x^2$ Tax revenue change as $max-(x-optimal)^2$ Laffer Curve and supply-side economics Economists debates whether tax rates had in fact reached such extremelevels. Because the cut in tax rates was intended to encourage people to increase the quantity of labor they supplied, the views of Laffer and Reagan became known as supply-side economics. The Costs of Production total revenue(TR) $TR=P\times Q$the amount a firm receives for the sale of its output. Cost total cost(TC)$TC= FC+VC$the market value of the inputs a firm uses in production. explicit costsinput costs that require an outlay of money by the firm. Both economists and accountants care. implicit costsinput costs that do not require an outlay of money by the firm. Only economists care. Profittotal revenue minus total cost $TR-TC$ economic profittotal revenue minus total cost, including both explicit and implicit costs accounting profittotal revenue minus total explicit cost. usually larger than economic profit. Production and Cost production functionthe relationship between quantity of inputs used to make a good and the quantity of output produced. marginal productthe increase in output that arises from an additional unit of input diminishing marginal productthe property whereby the marginal product of an input declines as the quantity of the input increases Various Measures of Cost fixed costs(FC)costs that do not vary with the quantity of output produced variable costs(VC)costs that vary with the quantity of output produced Average Cost average total cost(ATC)$ATC=TC/Q$total cost divided by the quantity of output.usually U-shaped.decrease because of AFC, increase because of AVC. average fixed cost(AFC)$AFC=FC/Q$fixed cost divided by the quantity of output.shape like $1\over x$. average variable cost(AVC)$AVC=VC/Q$variable cost divided by the quantity of output.Usually rise as the quantity produced increases. Marginal Cost(MC)The increase in total cost(TC) that arises from an extra unit of production(Q). MC = \Delta TC/\Delta Qeventually rise as the quantity produced increases. reflect the property of diminishing marginal product Relationship with ATC Whenever marginal cost is less than average total cost, average total cost is falling. Whenever marginal cost is greater than average total cost, average total cost is rising. The marginal-cost curve crosses the average-total-cost curve at its minimum. Long Run Short Run economies of scalethe property whereby long-run average total cost (ATC) falls as the quantity of output increases diseconomies of scalethe property whereby long-run average total cost (ATC) rises as the quantity of output increases constant returns to scalethe property whereby long-run average total cost (ATC) stays the same as the quantity of output changes Firms in Competitive MarketsCompetition competitive market There are many buyers and many sellers in the market. The goods offered by the various sellers are largely the same. Firms can freely enter or exit the market.(perfectly competitive markets) Average Revenue(AR)$AR=TR/Q$total revenue divided by the quantity sold. For all types of firms, average revenue equals the price of the good marginal revenue(MR)$MR=\Delta TR/\Delta Q$the change in total revenue from an additional unit sold. For competitive firms, marginal revenue equals the price of the good. Profit Maximizationsupply decision If marginal revenue is greater than marginal cost, the firm should increase itsoutput. If marginal cost is greater than marginal revenue, the firm should decrease itsoutput. At the profit-maximizing level of output, marginal revenue and marginal costare exactly equal. In essence, because the firm’s marginal-cost curve determines the quantity of the good the firm is willing to supply at any price, the marginal-cost curve is also the competitive firm’s supply curve,precisely, the portion lies above AVC. short run shut downtemporary shut down if TR]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>Economics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shortest Paths and Flow]]></title>
    <url>%2F2018%2F12%2F15%2FShortest-Paths-and-Flow%2F</url>
    <content type="text"><![CDATA[Algorithm Comments Graph Rep Running time Space Used Bellman-Ford Single-Source Adj List 𝑂(𝑉𝐸) 𝑂(𝑉) In DAG Out DAG Single-Source DAG Adj List 𝑂(𝑉+𝐸) 𝑂(𝑉) Dijkstra Single-SourceNon-Neg Weights Adj List 𝑂((𝑉 + 𝐸)log𝑉) 𝑂(𝑉) All-pairs 1 All-Pairs Adj Matrix 𝑂(𝑉^4^) 𝑂(𝑉^2^) All-pairs 2 All-Pairs Adj Matrix 𝑂(𝑉^3^log𝑉) 𝑂(𝑉^2^) Floyd-Warshall All-Pairs Adj Matrix 𝑂(𝑉^3^) 𝑂(𝑉^2^) Shortest Paths All algorithms, except Dijkstra, allow negative weights,but there can NOT be negative cycle. Shortest path problem: Find the shortest path from s to t. Single-source shortest path: Find the shortest path from s to every node. Lemma (Cut and Paste Argument): Let 𝑃 = (𝑠, … , 𝑢, … , 𝑡) be the shortest 𝑠 − 𝑡 path. The n the subpaths 𝑃~1~ = (𝑠, … , 𝑢) and 𝑃2= (𝑢, . . 𝑡) must also be, respectively, shortest 𝑠 − 𝑢 and 𝑢 − 𝑡 paths. Relaxation: Let v.d be the shortest distance found so far from the starting node s to node v, and v.p be the last node in the current shortest path from s to node v. Single Source Shortest PathLet ss(s,v) store shortest path distance from s to v Bellman-Ford AlgorithmInitially, set n.d = inf for all nodes. Except starting node s.d = 0Relax all edges once … path length maximum 1Relax all edges second time … path length maximum 2 A path may have at most V-1 edges. So we it V-1 times. Finally, n.d is the actual shortest distance from s to n Basic Implementation1234567Shortest-Path(G,s):for each node n in V do n.d = infs.d = 0for i = 1 to V-1 for each edge (u,n) in E Relax(u,n) Running time: Θ(𝑉𝐸)Space: Θ(𝑉) Efficient ImplementationTake advantage of dynamic programming. Recurrence:Suppose (u,n) is the last edge of shortest path of length at most i from s to n. By cut and paste argument, the subpath from s to u must be a shortest path of length at most i - 1, followed by (u,n).So, for all i &gt; 0 and n != s n.d[i] = min{ u.d[i-1] + w(u,n) } u in V, (u,n) in E Final solution: n.d[m-1] 12345678910Bellman-Ford(G,s):for each node n in V n.d = inf, n.p = nils.d = 0for i = 1 to V-1 for each node u in V if u.d changed in previous iteration then for each un in Adj(u) Relax(u,un) if no un.d changed in this iteration =&gt; terminate Running time: Θ(𝑉𝐸)Space: Θ(𝑉) Shortest Paths in a DAGBy subpath optimality, we have ss(s,n) = min { ss(s,u) + w(u,n) } u in V, (u,v) in Es Unlike Bellman-Ford, each edge will only be relaxed once. We need to ensure that when n is processed, ALL u with (u,n) in E have already been processed. We can do that by processing n (also ss(s,n)) in the topological order of the nodes. 123456789DAG-Shortest-Path(G,s)topologically sort the vertices of Gfor each vertex n in V n.d = inf n.p = nils.d = 0for each vertex u in topological order for each vertex un in Adj(u) Relax(u,un) Running time: Θ(𝑉+𝐸) Dijkstra AlgorithmNOT allow negative weights. Maintain a set of explored nodes S for which we know u.d = ss(s,u)by variable status Initialize S = {s}, s.d = 0, v.d = inf Use a Min priority queueQ on V with d as key Key Lemma If all edges leaving S are relaxed, then v.d = ss(s,v),where v is the vertex in V-S with the minimum v.d So this v can be added to S, then repeat pseudocode123456789101112Dijkstra(G,s):for each node n in V do n.d = inf, n.p = nil, n.status = unknowns.d = 0create a min priority queue Q onV with d as keywhile Q not empty % E times u = Extract-Min(Q) % O(logV) time u.status = over for each un in Adj(u) do if un.status = unknown then Relax(u,un) Decrease-Key(Q,un,un.d) Running time: 𝑂(𝐸log𝑉)Very similar to Prim’s algorithm All-Pairs Shortest PathsInput: Directed graph G = (V,E) Weight w(e) = length of edge e Output: ss(u,v), for all pairs of nodes u, v A data structure from which the shortest path from 𝑢 to 𝑣 can be extracted efficiently, for any pair of nodes 𝑢, 𝑣 Note: Storing all shortest paths explicitly for all pairs requires 𝑂(𝑉^3^) space. Graph Representation: Assume adjacency matrix: 𝑤(𝑢, 𝑣) can be extracted in 𝑂(1) time. 𝑤(𝑢, 𝑢)= 0, 𝑤(𝑢, 𝑣)= ∞if there is no edge from 𝑢 to 𝑣. If the graph is stored in adjacency lists format, can convert to adjacency matrix in 𝑂(𝑉2) time. Using Previous AlgorithmsNOT negative cost edges: Dijkstra’s algorithmRunning time: 𝑂(𝐸 log 𝑉), totally 𝑂(𝑉𝐸 log 𝑉)Space: 𝑂 (𝑛^3^ log 𝑛 ) for dense graphs HAVE negative cost edges: Bellman-FordRunning time: 𝑂(𝑉𝐸), totally 𝑂(𝑉^2^𝐸)Space: 𝑂 (𝑛^4^) for dense graphs First DP FormulationDefine d(i,j,m) = length of the shortest path from i to j that contains at most m edges.Use D[m] to denote the matrix [d(i,j,m)] Recurrence( essentially the same as in Bellman-Ford): d(i,j,m) = min { d(i,k,m-1) + w(k,j) } k from 1 to ninitially d(i,j,1) = w(i,j) pseudocode1234567891011Slow-All-Pairs-Shortest-Paths(G):d(i,j,1) = w(i,j) for all 1&lt;=i,j&lt;=nfor m = 2 to n-1 let D[m] be a new n*n matrix for i = 1 to n for j = 1 to n d(i,j,m) = inf for l =1 to n if d(i,k,m-1) + w(k,j) &lt; d(i,j,m) then d(i,j,m) = d(i,k,m-1) + w(k,j)return D[n-1] Running time: 𝑂(𝑛^4^)Space: 𝑂(𝑛^3^) can be improved to 𝑂(𝑛^2^) Second DP FormulationObservationTo compute d(i,j,m), instead of looking at the last stop before j, we look at the middle point. This cuts down the problem size by half. Thus, to calculate D[1],D[2],D[4],D[8],… Note that overshooting D[n-1] is OK. Since D[n&#39;] , n&#39;&gt;n -1 has the shortest paths with up to n&#39; edges. It will not miss any shortest path with up to n-1 edges. Recurrence d(i,j,2s) = min { d(i,k,s) + d(k,j,s) } k from 1 to ninitially d(i,j,1) = w(i,j) AnalyzeRunning time: 𝑂(𝑛^3^) for each matrix , totally 𝑂(𝑛^3^log𝑛) Floyd-WarshallDefined(i,j,k) = length of the shortest path from i to j that all intermediate vertices on the path (if any) are in the set {1,2,...,k} ObservationWhen computing d(i,j,k) there are two cases: k is not a node on the shortest path from i to j=&gt; then the path uses only vertices in {1,2,...,k-1} k is an intermediate node on the shortest path from i to j=&gt; path can be spilt into shortest subpath from i to k and a subpath from k to jBoth subpaths use only vertices in {1,2,...,k-1} Recurrence pseudocode1234567891011Floyd-Warshall(G):d(i,j,0) = w(i,j) for all 1&lt;=i,j&lt;=nfor k=1 to n let D[k] be a new n*n matrix for i = 1 to n for j = 1 to n if d(i,k,k-1) + d(k,j,k-1) &lt; d(i,j,k-1) then d(i,j,k) = d(i,k,k-1) + d(k,j,k-1) else d(i,j,k) = d(i,j,k-1)return D[n] Running time: 𝑂(𝑛^3^)Space: 𝑂(𝑛^3^) but can be improved to 𝑂(𝑛^2^)Surprising discovery: If we just drop all third dimension. i.e. the algorithm just uses n*n array D, the algorithm still works! Maximum FlowInput: A directed connected graph 𝐺 =(𝑉, 𝐸) , where every edge 𝑒 ∈ 𝐸 has a capacity 𝑐(𝑒); a source vertex 𝑠 and a target vertex 𝑡. Output: A flow 𝑓: 𝐸 → 𝐑 from 𝑠 to 𝑡, such that For each 𝑒 ∈ 𝐸, 0 ≤ 𝑓(𝑒) ≤ 𝑐(𝑒) (capacity) For each 𝒗 ∈ 𝑽 − {𝒔, 𝒕}, sumOut( 𝒇(𝒆) ) = sumInto( 𝒇(𝒆) )(conservation)、 Define:The value of a flow is |𝑓| = sum(𝑓(𝑠, 𝑣))= sum(𝑓(𝑣, 𝑡)) where 𝑣 in V s-t Cut Residual Graph Ford Fulkerson Algorithm Start with f(e) = 0 for all edges e in E Construct Residual Graph G~f~ for current flow f(e) = 0 while there exists some s-t path P in G~f~ Let capacity of flow cf(p) = min { cf(e): e in P}This is the maximum amount of flow that can be pushed through residual capacity of P‘s edges Push c(f,p) units of flow along the edges e in P by adding cf(p) to f(e) for every e in P Construct Residual Graph G~f~ for new current flow f(e) When algorithm gets stuck, current flow is maximal! 12345678910111213141516Ford-Fulkerson(G,s,t):for each (u,n) in E do f(u,n) = 0 cf(u,n) = c(e) cf(n,u) = 0while there exists path P in residual graph Gf do cf(p) = min &#123;cf(e):e in P&#125; for each edge (u,n) in P do if (u,n) in E then f(u,n) = f(u,n) + cf(p) cf(u,n) = cf(u,n) - cf(p) cf(n,u) = cf(n,u) + cf(p) else f(n,u) = f(n,u) - cf(p) cf(n,u) = cf(n,u) + cf(p) cf(u,n) = cf(u,n) - cf(p) Construct Residual Graph G~f~ capacity of flow in 8, write back to G The G~f~ in next iteration is Until there is no s-t path in G~f~. Current flow is optimally maximal. Applicationshe Max Flow setup can model (surprisingly) many (seemingly) unrelated problems. The idea is to express the problem as a max flow and then feed individual instances into out max flow solver. The examples below all share the property that they are integer flow problems, e.g., al capacities are integral, so running-time analyses can use FF bound for integral flows. Edge-Disjoint Paths Define: Two paths are edge-disjoint if they have no edge in common. Circulations with Demands Given a number of source vertices 𝑠1, 𝑠2, …, each with a supply of 𝑠𝑢𝑝(𝑠𝑖)and a number of target vertices 𝑡1, 𝑡2, …, each with a demand of 𝑑𝑒𝑚 𝑡𝑖 ;sum of supply &gt;= sum of demand Need a flow meets all demands. Solution:Add a super source and a super target Maximum Bipartite Matching A Matching is a subset M ⊆ E such that:∀v ∈ V at most one edge in M is incident upon v. The Size |M| is the number of edges in M. A Maximum Matching is matching M such that:every other matching Mʹ satisfies |Mʹ | ≤ M. Given bipartite graph G, find a Maximum Matching. Formulation: Create directed graph 𝐺′ = (𝑋 ∪ 𝑌 ∪ {𝑠, 𝑡}, 𝐸′ ). Direct all edges from 𝑋 to 𝑌, and assign them capacity 1. Add source 𝑠, and unit capacity edges from 𝑠 to each node in 𝑋. Add target 𝑡, and unit capacity edges from each node in 𝑌 to 𝑡. Theorem: Max cardinality matching in 𝐺 = value of max flow in 𝐺′. Running time: 𝑂(𝑉𝐸) Baseball Elimination Input： 𝑛 teams: 1, 2, … , 𝑛 One particular team, say 𝑛 (without loss of generality) Team 𝑖 has won 𝑤~𝑖~ games already Team 𝑖 and 𝑗 still need to play 𝑟~𝑖𝑗~ games, 𝑟~𝑖𝑗~ = 0 or 1. Team 𝑖 has a total of 𝑟𝑖 = sum(𝑟~𝑖𝑗~: 𝑗) games to play Output: “Yes”, if there is an outcome for each remaining game such that team 𝑛 finishes with the most wins (tie is OK). “No”, if no such possibilities. Claim: There is a way for team n to finish in the first place iff the max flow has value of the sum of supply from source s]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Minimum Spanning Tree]]></title>
    <url>%2F2018%2F12%2F15%2FMinimum-Spanning-Tree%2F</url>
    <content type="text"><![CDATA[Uniqueness of MSTCut LemmaLet S be any subset of nodeslet e be the min cost edge with exactly one endpoint in S.Then every MST must contain e. ProofLet T* be some MSTConsider any edge e in T*Removing e from T* breaks T* into two parts Sand V-Se must be the min cost edge crossing the cut (S,S-V)Applying the cut lemma on S, every MST must contain eApply the above arguments to every edge in T*, we have_Every edge e in T* must be contained in every MST_ Prim’s Algorithm Idea Initialize explored set S = { any one node } Add min cost edge e = (u,n) with u in S and n in V-S to T Add n to S Repeat until S = V FeatureMaintain set of explored nodes S A Min priority queue Q to keep unknown nodes.key is their cheapest edge to node inS pseudocode12345678910111213Prim(G,r):for each n in V do n.key = inf, n.p = nil, n.status = unknownr.key = 0create a min priority queue Q on Vwhile Q not empty u = Extract-Min(Q) % need O(logV) time u.status = over for each n in Adj(u) do if n.status = unknown and w(u,n)&lt;n.key then n.p = u n.key = w(u,n) Decrease-Key(Q,n,w(u,n)) Running time: 𝑂(𝐸log𝑉) Kruskal’s Algorithm Idea Start with an empty tree T Consider edges in ascending order of weight. Case 1: If adding e to T create a cycle, discard eCase 2: Otherwise, insert e = (u,v) into T according to cut Lemma Union-Find Data StructureKey QuestionHow to check whether adding e to T will create a cycle? DFS take 𝑂(𝐸⋅𝑉) time in total.Can we do better in 𝑂(log𝑉) time? After an edge e is added, two sets union together Need such a “union-find”data structure: Find-Set(u): For a given node u, find which set this node belongs to. Union(u,v): For two given nodes u and v, merge the two sets containing u and v together. Set as A Tree The trees in the union-find data structure are NOT the same as the partial MST trees! The root of the tree is the representative node of all nodes in that tree(i.e., use the root’s ID as the unique ID of the set). Every node (except the root), has a pointer pointing to its parent. The root has a parent pointer to itself. No child pointers (unlike BST), so a node can have many children. 12Make-Set(x):x.parent = x, x.hight = 0 123456Find-Set(x):x.height = 0while x!= x.parent do x.height = x.height + 1 x= x.parentreturn x 12345678Union(x,y):a = Find-Set(x)b = Find-Set(y)if a.height &lt;= b.height then if a.height = b.height then b.height = b.height + 1 a.parent = belse b.parent = a Path Compressionwhile Find-Set(x) we can update its parent pointer to compress the path. pseudocode12345678MST-Kruskal(G):for each node n in V Make-Set(n)sort the edges of G into increasing order by weight % O(ElogE)for each edge (u,n) in E taken in the above order if Find-Set(u) != Find-Set(v) then % O(E) output edge (u,n) Union(u,n) Running time: 𝑂(𝐸log𝐸+𝐸log𝑉)=𝑂(𝐸log𝑉)Note: If edges are already sorted(𝑂(𝐸log𝐸)) and we use path compression, then the running time is close to 𝑂(𝐸)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph Basic and Algorithm]]></title>
    <url>%2F2018%2F12%2F15%2FGraph-Basic-and-Algorithm%2F</url>
    <content type="text"><![CDATA[Basicnode: vertex edge: connection between nodes Undirected Edges have no direction (or both directions) deg (𝑣) = # edges at 𝑣 sum of deg(𝑣) = 2𝐸 Directed Edges have directions If an edge has both directions, we will use two edges with opposite directions deg~out~ (𝑣) = # edge leaving 𝑣;deg~in~ (𝑣 ) = # edge entering 𝑣. sum of deg𝑜𝑢𝑡(𝑣) = sum of deg𝑖𝑛(𝑣) = 𝐸 Path A path in a (directed or undirected) graph 𝐺 = (𝑉, 𝐸) is a sequence 𝑃 of nodes 𝑣1, 𝑣2, … , 𝑣𝑘−1, 𝑣𝑘 such that (𝑣𝑖, 𝑣𝑖+1) is an edge. The length of the path is 𝑘 − 1 (i.e., # edges in the path). A path is simple if all nodes are distinct. Connectivity An undirected graph is connected if for every pair of nodes 𝑢 and 𝑣, there is a path between 𝑢 and 𝑣. Theorem: For a connected graph, 𝐸 ≥ 𝑉 − 1. Cycle A cycle is a path v~1~, v~2~, … , v~k-1~, v~k~ in which v~1~ = v~k~ , k &gt; 2 A cycle is simple if the first 𝑘 − 1 nodes are all distinct. Data structureAdjacency Nodes Adjacency List A node-indexed array of lists. Given node 𝑢, retrieving all neighbors in Θ(deg(𝑢)) time Given 𝑢, 𝑣, checking if (𝑢, 𝑣) is an edge takes Θ(deg(𝑢)) time. Space: Θ(𝑉 + 𝐸). Adjacency Matrix A 𝑉 × 𝑉 matrix. Given node 𝑢, retrieving all neighbors in Θ (𝑉) time Given 𝑢, 𝑣, checking if (𝑢, 𝑣) is an edge takes 𝑂(1) time. Space: Θ(𝑉2). Treesconnected &amp;&amp; no cycle =&gt; treeno cycle =&gt; forest(several trees) After we have run BFS or DFS on an undirected graph, the edges can be classified into 3 types: Tree edges:traversed by the BFS/DFS Back edges:connecting a node with one of its ancestors(other than its parent) Cross edges:connecting two nodes with no ancestor/descendent relationship. AlgorithmBFS span a tree with NO back edges pseudo-code1234567891011initialize an empty queue QEnqueue(Q,r)while Q not empty do n = Dequeue(Q) for each v in Adj(n) if v.status = unknown v.status = processing v.d = n.d + 1 v.p = n Enqueue(Q,v) n.status = over Running time: Θ(𝑉+𝐸), which is Θ𝐸 if the graph is connected ApplicationFind connected components. DFS span a tree with NO cross edges pseudo-code123456789101112DFS(G)for each vertex n in V do if n.status = unknown then DFS-Visit(n)DFS-Visit(n):n.status = processingfor each v in Adj(n) do if v.status = unknown the v.p = n DFS-Vist(v)n.status = over Running time: Θ(𝑉+𝐸) ApplicationCycle detection Topological Sort A topological ordering of a graph is a linear ordering of the vertices of a DAG(Directed Acyclic Graph) such that if (u,v) is in the graph, u appears before v in the linear ordering. idea Output a vertex u with in-degree zero in current graph Remove u and all edges (u,v) from current graph If the graph is not empty, go to step 1 pseudo-code1234567891011121314Initialize Q to be an empty queuefor each u in V do If inDegree(u) = 0 then % find all starting vertices Enqueue(Q,u)while Q not empty u = Dequeue(Q); Output u for each v in Adj(u) % remove u&apos;s outgoing edges inDegree(v) = inDegree(v) - 1 if inDegree(v) = 0 then Enqueque(Q,v)return Running time: 𝑂(𝑉+𝐸)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming]]></title>
    <url>%2F2018%2F12%2F15%2FDynamic-Programming%2F</url>
    <content type="text"><![CDATA[Main idea of DP: Analyze the structure of an optimal solution Recursively define the value of an optimal solution Compute the value of an optimal solution (usually bottom-up) @[TOC] One-DimensionStairs Climbing 1D You can climb 1 or 2 stairs with one step.How many different ways can you climb n stairs? Recurrence F(n) = F(n-1) + F(n-2) Base case F(1) = 1, F(2) = 2 pseudo-code1234567F(n):allocate an array A of size nA[1] = 1A[2] = 2for i=3 to n A[i] = A[i-1] + A[i-2]return A[n] AnalyzeRunning time: Θ(𝑛)Space: Θ(𝑛) but can be improved to Θ(1) by freeing array. Rod Cutting Problem Given a rod of length n and prices p~i~ for i = 1,…,n, where p~i~ is the price of a rod of length i . Find a way to cut the rod to maximize total revenue. length i 1 2 3 4 5 6 7 8 9 10 price p~i~ 1 5 8 9 10 17 17 20 24 30 Recurrencetotal optimal revenue is price of cut rod and optimal revenue of remaining rood. r~n~ = max{p~n~, p~1~ + r~n-1~, p~2~ + r~n-2~, …, p~n-1~+r~1~} Base caseDefine r~n~ be the maximum revenue obtainable from cutting a rod of length n. r~1~ = p~1~ pseudo-code123456789101112131415cutRod(n):let r[0...n] and s[0...n] be new arraysr[0] = 0 for j=1 to n % every optimal length q = -inf for i =1 to j % every cut length if q &lt; p[i] + r[j-i] then q = p[i]+r[j-i] % keep track the optimal cut length s[j] = i r[j] = qj = n while j&gt;0 do print s[j] j = j-s[j] i 0 1 2 3 4 5 6 7 8 9 10 p[i] 0 1 5 8 9 10 17 17 20 24 30 r[i] 0 1 5 8 9 13 17 18 22 25 30 s[i] 0 1 2 3 2 2 6 1 2 3 10 AnalysizeRunning time: Θ(𝑛^2^)Space: Θ(𝑛) Weighted Interval Scheduling 1D Jobs j starts at s~j~, finish at f~j~ and has weight(value) v~j~.Two jobs compatible if they don’t overlap.Goal: find maximum-weight subset if mutually compatible jobs. RecurrenceFirstly, label all jobs by finishing time. Maximum subset is eitherDO take this job and maximum subset of compatible jobs setNOT take this job and maximum subset of remaining jobs V[j] = max{v~j~ + V[c(j)], V[j-1]} function c(j) return the largest index i&lt;j such that job i is compatible job j. Base caseThe goal is to find a subset of a set. We start from a empty set. pseudo-code12345678910111213141516schedule():sort all jobs by finish timeV[0] = 0for i = 1 to n % DO take job j if V[i] + V[c(i)] &gt; V[j-1] then V[i] = value[i] + V[c(i)] keep[i] = 1 % NOT take job j else V[i] = V[i-1] keep[i] = 0 i = nwhile i &gt; 0 do if keep[i] =1 then print i, i = c(i) else i = i-1 AnalyzeRunning time: Θ(𝑛log𝑛)Space: Θ(𝑛) Two-DimensionSometimes sub-problem also need to use 1D-DP to solve.Sometimes there are two variables that requires a 2D array. The 0/1 Knapsack Problem A set of n items, where item i has weight w~i~ and value v~i~ ,and a knapsack with capacity W.Find x~1~,… ,x~n~ (either 0 or 1) such thatsum(x~i~w~i~) &lt;= W and V=sum(x~i~v~i~) is maximum RecurrenceStart to put items into the knapsack.—1D Maximum V of capacity W is the max among whether take each item.v~i~ + maximum XV of capacity j-w~i~We found it necessary to consider capacity of knapsack.**—2D** V[j] = max{v(i) + V[j-w(i)]} i from 1 to n WRONG: This may pick the same item more than once!Thus, both in v~i~ and V[j-w(i)]! New def: let V[i,j] be the largest obtained value with capacity j, choosing ONLY from the first i items.Below formula truly reflect whether. Left is NOT, right is YES. V[i,j] = max{V[i-1,j], v~i~ + V[i-1,j-w(i)]} i from i to n Base case We start from take nothing: i= 0, V[0,j] = 0and empty knapsack j =0, V[i,0] = 0 pseudo-code1234567891011121314let V[0...n,0...W] and keep[0...n,0...W] be new arrays of all 0 for i = 1 to n do % put things for j = 1 to W do % each capacity if w[i]&lt;=j and v[i]+V[i-1,j-w[i]] &gt; V[i-1,j] then V[i,j] = v[i]+V[i-1,j] keep[i,j] = 1 else V[i,j] = V[i-1,j] keelp[i,j] = 0K = Wfor i = n downto 1 do if keep[i,K] = 1 then print i K = K-w[i] input: i 1 2 3 4 v~i~ 10 40 30 50 w~i~ 5 4 6 3 running: V[i,j] 0 1 2 3 4 5 6 7 8 9 10 i=0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 10 10 10 10 10 10 2 0 0 0 0 40 40 40 40 40 40 40 3 0 0 0 0 40 40 40 40 40 50 70 4 0 0 0 50 50 50 50 90 90 90 90 AnalyzeRunning time: Θ(𝑛𝑊)Space: Θ(𝑛𝑊)without keep array can be improved to Θ(𝑛+𝑊). Longest Common Subsequence LCS Given two sequences X = (x~1~,x~2~,…,x~m~) and Y = (y~1~,y~2~,…,y~n~), we say that Z is a common subsequence of X and Y if Z has a strictly increasing sequence of indices i and j of both X and Y such that we have x~ip~= y~jp~ = z~p~ for all p = 1, 2, … , k. The goal is to find the longest common subsequence of X and Y. RecurrenceLet c[i,j] be the length of the longest common subsequence of X[1…i] and Y[1…j].So we need 2D that i for X and j for Y. Firstly, go through i, and then go through jIf X[i] = Y[j], we match.c[i,j] = 1 + the longest common subsequence of X[1…i-1] and Y[1…j-1].else , it’s max ofthe longest common subsequence of X[1…i-1] and Y[1…j] andthe longest common subsequence of X[1…i] and Y[1…j-1] If X[i] = Y[j] , c[i,j] = max{ c[i,i-1], c[i-1,j] } else c[i,j]= c[i-1,j-1] +1 Base case c[0,j] =0c[i,0] = 0 pseudo-code1234567891011121314151617181920212223LSC(X,Y):let c[0...m,0...n] and b[0...m,0...n] be new arrys of all 0for i=1 to m for j=1 to n if X(i) = Y(i) then c[i,j] = c[i-1,j-1]+1 b[i,j] = &quot;↖&quot; else if c[i-1,j] &gt;= c[i,j-1] then c[i,j] = c[i-1,j] b[i,j] = &quot;↑&quot; else c[i,j] = c[i,j-1] b[i,j] = &quot;←&quot;Print-LCS(b,m,n)Print-LCS(b,i,j): if i=0 or j=0 then return if b[i,j]=&quot;↖&quot; then Print-LCS (b,i−1,j−1) print X(i) else if 𝑏[𝑖,𝑗]=&quot;↑&quot; Print-LCS( b,i−1,j) else Print-LCS( b,i,j−1) AnalyzeRunning time: Θ(𝑚𝑛)Space: Θ(𝑚𝑛)without b array can be improved to Θ(𝑚+𝑛). Longest Common Substring Given two strings X = x~1~x~2~…x~m~ and Y = y~1~y~2~…y~m~, we wish to find their longest common substring Z, that is, the largest k for which there are indices 𝑖 and 𝑗 withx~i~x~i+1~…x~i+k-1~ = y~j~y~j+1~…y~j-k-1~ RecurrenceLet d[i,j] keep track of k, the longest string length of X[1…i] Y[1…j].Firstly go through X and then Y so we need 2D If X(i) = Y(j), d[i,j] = 1 + the longest string length of X[1…i-1] Y[1…j-1].else it’s 0! If X(i) = Y(j), d[i,j] = 1 + d[i-1.j-1]else d[i,j] = 0 Base case d[0,j] = 0, d[i,0]= 0 pseudo-code1234567891011let d[0...m,0...n] be a new array of all 0length = 0, endIndex = 0for i = 1 to m for j = 1 to n if X(i) = Y(i) then d[i,j] = d[i-1,j-1] + 1 if d[i,j] &gt; length then length = d[i,j] endIndex = ifor i = endIndex - length + to endIndex print X(i) AnalyzeRunning time: Θ(𝑚𝑛)Space: Θ(𝑚𝑛) but can be improved to Θ(𝑚+𝑛). Over Intervals Goal is to find optimal (min or max ) solution on problem with Problem of size n Ordered input of items 1,2,…n Define substructures as Ordered input of items i..j Problem of size j-i+1 Recurrence gives optimal solution of subproblem as function of optimal solution of smaller subproblems Algorithm fills in DP table from smallest to largest problem size Often, final subproblem filled is solution for original problemSometimes, solution of original problem is min/max over table values Longest Palindromic SubstringA palindrome is a string that reads the same backward or forward. Given a string X = x~1~x~2~…x~n~, find the longest palindromic substring. Ex:X =ACCABAPalindromic substrings: CC, ACCA ABALongest palindromic substrings: ACCA RecurrenceLet p[i,j] be true if and only if X[i…j] is a palindrome.Obviously, we need 2D, though i &lt;= j. It’s like a half plane. If X[i] = X[j], p[i,j] is true iff p[i+1,j-1] is true. If X(i) = X(j), p[i,j] = p[i+1,j-1] Order: from (i,j) to (i+1,j-1) it’s a diagonal path. Base If i = j, p[i,j] = true pseudo-code12345678910111213141516let p[0...n,0...n] be a new array of all falsemax = 1 for i = 1 to n-1 do p[i,i] = true if X(i) = X(i+1) then p[i,i+1] = true max = 2% updating along diagonal% started from the third diagonalfor k = 3 to n do for i = 1 to n-k+1 do j = i+k-1 if p[i+1,j-1] = true and X(i) = X(j) then p[i,j] = true max = kreturn max AnalyzeRunning time: 𝑂(𝑛^2^)Space: 𝑂(𝑛^2^) but can be improved to 𝑂(𝑛) Optimal BST Given n keys a~1~ &lt; a~2~ &lt; … &lt; a~n~, with weights f(a~1~), … , f(a~n~), find a binary search tree T on these n keys such thatB(T) = sum{ f(a~i~)*(d(a~i~)+1) } i from 1 to nis minimized, where d(a~i~) is the depth of a~i~. RecurrenceLet T~i,j~ be some tree on the subset of nodes a~i~ &lt; a~i+1~ &lt; … &lt; a~j~Define w[i,j] = f(a~i~) + … + f(a~j~)The cost is defined as B(T~i,j~) = sum{ f(a~i~)*(d(a~i~)+1) } from i to jDefine e[i,j] = optimal value of B(T~i,j~) The optimal cost of T~i,j~ isThe optimal cost of left subtree + The optimal cost of right subtree + root’s weight e[i,j] = e[i,k-1 + e[k+1,j] +w[i,j] To find k, try every value between i and j to figure out the min. e[i,j] = min{ e[i,k-1 + e[k+1,j] +w[i,j] } for i&lt;=k&lt;=j Order: (i,j) (i,k-1) (k+1,j) Base e[i,j]= 0 for i&gt;je[i,i] = f(a~i~) for all i pseudo-code1234567891011121314151617181920212223242526Optimal-BST(a,n):let e[1...n,1...n],w[1...n,1...n],root[1...n,1...n] be new arrays of all 0for i = 1 to n w[i,i] = f(a(i)) for j = i + 1 to n % complete the w[] table w[i,j] = w[i,j-1] + f(a(i))for l = 1 to n for i = 1 to n-l+1 j = i+l-1 e[i,j] = inf % find k minimizes e[] for k = i to j t = e[i,k-1]+e[k+1,j]+w[i,j] if t &lt; e[i,j] then e[i,j] = t root[i,j] = kreturn Construct-BST(root,1,n)Construct-BST(root,i,j):if i &gt; j then return nilcreate a node zz.key = a[root[i,j]]z.left = Construct-BST(root,i,root[i,j]-1)z.right = COnstruct-BST(root,root[i,j]+1,j)return z AnalyzeRunning time: Optimal 𝑂(𝑛^3^) Construct 𝑂(𝑛^2^)Space 𝑂(𝑛^2^)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Finite State Machine]]></title>
    <url>%2F2018%2F12%2F08%2FFinite-State-Machine%2F</url>
    <content type="text"><![CDATA[State Machine State: A set of particular condition that the machine is in at a specific time FSM is a generic synchronous sequential circuit FSM has finite state memory, inputs &amp; outputs State memory is implemented using flip-flops State transition logic implemented using combinational circuit Output logic implemented using combinational circuit Output depends on state only - Moore Output depends on state and input - Mealy State equationsDescribe the behavior of a clocked sequential circuit algebraically.e.g. A(t+1) = D~A~(t) = A(t)x(t) + B(t)x(t)Simple form:A(t+1) = Ax + Bx or A^+^ = Ax + Bx State Diagram and Transition Tables The diagram is a Mealy Machine Moore Machine vs. Mealy MachineMoore MachineThe output value is inside the state bubbles.Outputs are function solely of the current states.Outputs change synchronously with states. Mealy MachineThe output value is on the transition edge.Outputs depend on state AND inputsChange of inputs causes an immediate change of outputs. Example Basic Design Steps of FSM Understand the Specificationswith a block diagram Obtain an abstract specification of the FSMin state-transition diagram or table Perform state reductionequivalent states can be merged Perform state assignmentassign binary values to the state in a way that next-state logic can be simplified Choose FF to implement the FSM state register D-FF : Q+ = D T-FF : Q^+^ = TQ’ + T’QT = 0 hold ;T = 1 toggle; JK-FF: Q^+^ = JQ’ + K’Q| JK | Data | function || :—: | :—: | :———: || 0/0 | 1 | hold || 0/1 | 0 | reset || 1/0 | Q | set || 1/1 | Q’ | toggle | Implement the FSMdesign of next-state and output logic Happy DEBUGGING!! FSM Implementation Procedure Start with a state diagram (bubble diagram) Describes the Finite State Machine according to the specification Reduce the number of states if necessary/possible(state reduction) equivalent-states Two internal states are said to be equivalentif for each input combination they give exactly the same output AND send the circuit to the same(equivalent) state. Decide on the State Encoding (how many flip-flops to use and what should be the FF outputs be for each state) Produce the binary-code state table. Decode what kind of FFs to use. D-type FFs are normally used (JK or T are more complicated and may give you smaller circuit if you play with TTLs) Determine the FF input equations (use FF excitation rules for JK and T type FFs) and general output equations from the transition tables Implement the next state logic, output logic using combinational circuit techniques Draw the complete logic diagram]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>FSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VHDL]]></title>
    <url>%2F2018%2F12%2F08%2FVHDL%2F</url>
    <content type="text"><![CDATA[EntityIdentify a distinct logic block Consists of the input and output ports (I/O pins) Each port can have several variables 1234entity nand2 isport (a,b : in std_logic; y : out std_logic);end nand2; Ports can be in (read), out (write), inout (bi-directional) Std_logic is the type of signal being carried at the ports Std_logic can have values such as ‘0’, ‘1’, ‘X’, ‘Z’, ‘-’ and others.(‘ z ‘ means high impedance (tristate)) Another signal type is “bit” which only assumes binary values of ‘0’ and ‘1’. ArchitectureFollow immediately after the entity declaration,to define the internal operation (function) of the logic module Defines the relationship between the input and output signals May have internal signals in the definition AN entity can have several different architecture specifications to describe different views or different levels 12345678entity inverter_gate is port (A: in std_logic; Z: out std_logic);end inverter_gate;architecture DATAFLOW of inverter_gate isbegin z &lt;= not A after 10 ns;end DATAFLOW; 1234567Entity OR3 isport (I1,I2,I3: in std_logic; O: out std_logic);end OR3;architecture BEHAVIOR of OR3 isbegin O &lt;= I1 or I2 or I3;end BEHAVIOR; Internal signalsused to simplify the model ordefine connections between sub-components inside the architecture 1234architecture example of special_mus is signal control, tmp : std_logic;beginend example; Concurrent Statements Signal Assignments Component Instantiations for structural description) Processes IN ORDER Signal Assignments1C &lt;= '0'; Delayed1c &lt;= A and B after 10ns; --Only for simulation. Selected (outside PROCESS)connect target to many cases of one source. 1234with sel select DAta_out &lt;= a when '0', b when '1', 'X' when others; Conditional (outside PROCESS)connect target to many cases of several sources. 123Y &lt;= a when en = ‘1’ else ‘Z’ when en = ‘0’ else ‘X’; Component Instantiations for structural description123COMPONENT xor_gate PORT ( A, B: IN STD_LOGIC; C: OUT STD_LOGIC);END COMPONENT; Processes IN ORDERIf-else Branchmany signals many cases.1234567if sth = '0' then A = '1'; B = '0'; -- IN-ORDER processeselsif sth = '1' then -- optional B = '1'; C = '0'; -- optionalelse -- IMPORTANT! C = '1';end if； case-when Branchone signal many cases.12345case sth is when '0' =&gt; A = '1'; B = '0'; when '1' =&gt; B = '1'; C = '0'; when others =&gt; C = '1'; -- IMPORTANT!end case; Variable and SignalThe variable is updated without any delay as soon as the statement is executed.Variables must be declared inside a process (and are local to the process).Assignment := executed sequentially! 123456789101112131415architecture VAR of EXAMPLE is signal TRIGGER, RESULT: integer :=0;begin process variable variable1: integer := 1; variable variable2: integer := 2; variable variable3: integer := 3; begin wait on TRIGGER; variable1 := variable2; --2 variable2 := variable1 + variable3; --2+3=5 variable3 := variable2; --5 RESULT &lt;= variable1 + variable2 + variable3; --2+5+5 end process;end VAR; The signal is updated after a time delay (delta delay if no delay is specified).Signals must be declared outside the process.Assignment &lt;= executed concurrently! 123456789101112131415architecture SIGN of EXAMPLE is signal TRIGGER, RESULT: integer :=0; signal signal1: integer := 1; signal signal2: integer := 2; signal signal3: integer := 3;begin process begin wait on TRIGGER; signal1 &lt;= signal2; --2 signal2 &lt;= signal1 + signal3; --1+3 signal3 &lt;= signal2; --2 RESULT &lt;= signal1 + signal2 + signal3; --1+2+3 end process;end VAR; Dataflow architectureSpecify input/output relations in Boolean expression or conditional statements 1234567891011ENTITY 2_to_4_decoder IS PORT (A, B, E: IN STD_LOGIC; D: OUT STD_LOGIC_VECTOR (3 downto 0);END 2_to_4_decoder;ARCHITECTURE dataflow OF 2_to_4_decoder ISBEGIN D(0) &lt;=‘0’ WHEN (A=‘0’ AND B=‘0’ AND E=‘0’) ELSE ‘1’; D(1) &lt;=‘0’ WHEN (A=‘0’ AND B=‘1’ AND E=‘0’) ELSE ‘1’; D(2) &lt;=‘0’ WHEN (A=‘1’ AND B=‘0’ AND E=‘0’) ELSE ‘1’; D(3) &lt;=‘0’ WHEN (A=‘1’ AND B=‘1’ AND E=‘0’) ELSE ‘1’;END dataflow; Structural architectureDescribes a set of interconnected components. 1234567891011121314151617ARCHITECTURE structural OF four_bit_adder_sub IS COMPONENT four_bit_adder PORT ( A: IN STD_LOGIC_VECTOR (3 downto 0); B: IN STD_LOGIC_VECTOR (3 downto 0); C0: IN STD_LOGIC; C4: OUT STD_LOGIC; S: OUT STD_LOGIC_VECTOR (3 downto 0)); END COMPONENT; COMPONENT xor_arrays PORT ( X: IN STD_LOGIC_VECTOR (3 downto 0); Z: IN STD_LOGIC; Y: OUT STD_LOGIC_VECTOR (3 downto 0)); END COMPONENT; signal Y: STD_LOGIC_VECTOR (3 downto 0);BEGIN Block1: xor_arrays PORT MAP (B, M, Y); Block2: four_bit_adder PORT MAP (A, Y, M, C, S);END structural; Behavioral architectureAny change in the values of Sensitivity List will cause immediate execution of the process 1234567891011121314151617ENTITY mux4 ISPORT ( S: IN STD_LOGIC_VECTOR (1 downto 0); I: IN STD_LOGIC_VECTOR (3 downto 0); Y: OUT STD_LOGIC);END mux4;ARCHITECTURE behavioral OF mux4 ISBEGIN PROCESS (S, I) -- Sensitivity List BEGIN CASE S IS WHEN “00” =&gt; Y &lt;= I(0); WHEN “01” =&gt; Y &lt;= I(1); WHEN “10” =&gt; Y &lt;= I(2); WHEN “11” =&gt; Y &lt;= I(3); END CASE; END PROCESS;END behavioral;]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>VHDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[COMP2611 Final Review]]></title>
    <url>%2F2018%2F12%2F07%2FCOMP2611-Final-Review%2F</url>
    <content type="text"><![CDATA[@[TOC] Arithmetic for ComputersOverflow Detection Operation Sign bit of X Sign Bit of Y Sign Bit of Result X+Y 0 0 1 X+Y 1 1 0 X-Y 0 1 1 X-Y 1 0 0 Multiplication Division PipelineStages:IF : Fetch the instructions from meomryID : Instruction decode &amp; register readEX : Perform ALU operationMEM : Memory access (if necessary)WB : Write result back to register Registers:located between the stages:​ IF/ID, ID/EX, EX/MEM, MEM/WB Control:IF : no control signalsID : no control signalsEX : RegDst, ALUOP, ALUSrcMEM : Branch, MemRead, MemWriteWB : MenToReg, RegWrite Hazards:cause: Data dependence and Control dependence.types: Structural hazards, Data hazards, Control hazards Structural hazardsConflict for use of memory(MEM/IF) and registers(WB/ID). Solution: separate instruction and data memories Fact: Register very fast. Registers can Write during 1st half of clock cycle and Read during 2nd half. Data hazards add $s0, $t0, $t1sub $t2, $s0, $t3 Solution: Forwarding (partial, can’t solve below code) lw $s0, 20$(t1)sub $t2, $s0, $t3 Scheduling (assume forwarding is used) Control hazardsWhen we need to beq , bne operations Solution: Fetch instruction after branch. If wrong, flush them away. Add comparator to compare earlier Datapath with HazardsData forwardingTwo types forwarding: EX/MEM -&gt; ID/EX and MEM/WB -&gt; ID/EXUsing two multiplexers to decide what is the input of operands A and B of the ALU.note: A is for Rs. B is for Rt. EX forwarding:123456if (EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠ 0) and (EX/MEM.RegisterRd = ID/EX.RegisterRs )) ForwardA = 10 if (EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠ 0) and (EX/MEM.RegisterRd = ID/EX.RegisterRt )) ForwardB = 10 MEM forwarding (only when NOT EX forwarding ):12345678910if (MEM/WB.RegWrite and (MEM/WB.RegisterRd ≠ 0) and not (EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠ 0) and (EX/MEM.RegisterRd = ID/EX.RegisterRs ))and (MEM/WB.RegisterRd = ID/EX.RegisterRs )) ForwardA = 01 if (MEM/WB.RegWrite and (MEM/WB.RegisterRd ≠ 0) and not (EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠ 0) and (EX/MEM.RegisterRd = ID/EX.RegisterRt ))and (MEM/WB.RegisterRd = ID/EX.RegisterRt )) ForwardB = 01 Load-use detectionWhen Load-use hazard occur, we can do nothing but stall a clock cycle.To detect it: 123Load-use hazard = ID/EX.MemRead and ((ID/EX.RegisterRt = IF/ID.RegisterRs ) or (ID/EX.RegisterRt = IF/ID.RegisterRt )) How to Stall Force control values in ID/EX register to 0 :​ EX, MEM and WB do EX, MEM and WB do nop (no -operation) Prevent update of PC and IF/ID registerUsing instruction is decoded againFollowing instruction is fetched again1-cycle stall allows MEM to read data for lw (with forwarding) Data Hazard when branchIn pipeline datapath, branch target address calculation is executed in another ALU NOT in EXE stage. It is executed during ID staged. ID Stage: Target address calculator, Register file, Register comparator. add $1, $2, $3add $4, $5, $6add $7, $8, $9 # another instructionbeq $1, $4, target Can resolve using forwarding. From MEM/WB and EXE/MEM to ID add $1, $2, $3add $4, $5, $6beq stalled…beq $1, $4, target Need one cycle stalled. lw $1,addrbeq stalled…beq stalled…beq $1, $0, target Need two cycles stalled. MemoryRAM(Random Access Memory) technology includes two types: Static RAM — mostly used for cache0.5ns-2.5ns, $2000 -$5000 per GBconsists only Transistors Dynamic RAM(DRAM) — mostly used for main memory 50ns - 70ns, $20-$75 per GB consists of Transistor and Capacitor Magnetic disk: 5ms - 20ms, $0.2-$2 per GB Disk Sectors and AccessesEach sector records:Sector IDData (512 bytes, 4096 bytes proposed)Error correcting code (ECC)Synchronization fields and gaps Access to a sector involves:Queuing delay if other accesses are pendingSeek: move the headsRotational latencyData transferController overhead Principle of Locality Programs access a small proportion of their address space at any time Temporal locality Items accessed recently are likely to be accessed again soon Spatial localityItems near those accessed recently are likely to be accessed soon This property is the KEY to memory hierarchy! Memory HierachyInitially,Instructions and data are loaded into DRAM from disk. Upon first access,A copy of the referenced instruction or data item is kept in cache In subsequent accesses,First, look for the requested item in the cache​ If the item is in the cache, return the item to the CPU​ If NOT in the cache, look it up in the memory We can say: If not found in this level, look it up at next level until foundKeep (cache) a copy of the found item at this level after use CacheEach memory locations is mapped to ONE location in the cache. average latency = hit + miss= r*t1 + ((1-r)t1 + rt2) = t1 + (1-r)t2NOT r*t1 + (1-r)t2 Three basic organizations: Direct-mapped( one memory block to one possible cache block) Set-mapped( one memory block to one set of possible cache blocks) Fully-mapped( one memory block to all possible cache blocks) Direct Mapped CacheIf the number of cache blocks (N) is a power of 2; N=2^mcache_location = the low-order m bits of block addresse.g. map the value1200 to a 8 blocks, 32 byte/block cache. Block address = 1200/32 = 37Block number = 37 % 8 = 5 Disadvantage:block access sequence: 100011, 001011, 100011later arrival will kill out previous one. called cache conflict. Tags and Valid BitsTags: decide which particular block is stored in a cache location.Store block address as well as the data.Only need the high-order bits.tags size= original address size- block size- blocks number(binary) - 1(valid bit) Valid Bits: decide whether the data exists. Initially 0. Consider a direct-mapped cache with 8 cache frames and a block size of 32 bytesMemory (byte) address generated by CPU|Block address|Hit or miss|Assigned cache block—-|—-|—|—-0010 1100 0010|0010 110|Miss|1100011 0100 0000|0011 010|Miss|0100010 1100 0100|0010 110|Hit|1100010 1100 0010|0010 110|Hit|1100010 0000 1000|0010 000|Miss|0000000 0110 0000|0000 011|Miss|0110010 0001 0000|0010 000|Hit|0000010 0100 0001|0010 010|Miss|010 Associate CacheFully associative(FA):Each block can be placed anywhere in the cache.pros: No cache conflict. But still have misses due to size.cons: Costly (hardware and time) N-way Set associative(SA):Each block can be placed in a certain number(N) of cache locations.A good compromise between DM &amp; FA Block Replacement Random Least recently used(LRU):Upon miss:Replace the LRU with missed address -&gt; Move the miss address to MRU positionUpon hit:Move the hit address to MRU position -&gt; Pack the rest Deal with Dirty DataWrite-backCPU only write to cache.CPU can write individual words at the rate of the cache.Multiple writes to a block are merged into one write to main memory.more and more caches use this strategy. Write-throughCPU write both on cache and memory.(memory always up-to-date)]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>MIPS</tag>
        <tag>assemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashing]]></title>
    <url>%2F2018%2F12%2F07%2FHashing%2F</url>
    <content type="text"><![CDATA[Hash TableA hash table is an array of some fixed size,containing all the data items.Each item has a key search is performed based on the keys.Each key is mapped into some position in the array in the range 0 to m − 1, where m is thearray size.The mapping is called hash function. Hashing table is a data structure that supports: searching insertion deletion The implementation of hash tables is called hashing.Hashing is a technique which allows the executions of above operations in constant time on average.Unlike other data structures such as linked list or binary trees, data items are generally not ordered in hash tables.As a consequence, hash tables don’t support the following operations: find_min and find_max finding successor and predecessor reporting data with a given range listing out the data in order Hashing FunctionDesignA simple and reasonable strategy: h(k) = k mod m. Its a good practice to set the table size m to a prime number. h(key) =(key[0] + 37*key[1] + 37^2*key[2] + … ) mod m Handling collisionMake hash table an array of linked lists Seperate ChainingInsertionTo insert a key k: Compute h(k)If T(h(k)) contains nullptr , initial this table entry to point to linked list node of kk .If T(h(k)) contains non-nullptr, add k to the beginning of the list. DeletionTo delete a key k: Compute h(k) to determine which list to traverse.Search for the key k, in the list that T(h(k)) points to.Delete the item with key k if it is found. Open addressingInstead of putting keys of the same hash table into a chain, open addressing will relocate the key k to be inserted if it collides with an existing key. Linear probing f(i) = ihi(k) = (hash(k) + i) mod m Disadvantage: fall into cluster quadratic probing f(i) = i^2^hi(k) = (hash(k) + i^2^) mod m Disadvantage: second cluster… same key same probe steps. double hashing f (i) = i × hash2(k)h~i~(k) = (hash(k) + i × hash~2~(k)) mod m hash~2~(k) must be relatively prime to the table size m.Otherwise, we will only be able to examine a fraction of the table entries.]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>hashing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AVL Trees]]></title>
    <url>%2F2018%2F12%2F06%2FAVL-Trees%2F</url>
    <content type="text"><![CDATA[(All codes below are written by Dr. Desmond) AVL(Adelson-Velsky and Landis) TreesEvery sub-tree of an AVL tree is itself an AVL tree.(An empty tree is an AVL tree too.)123456struct AVLNode&#123; T data; int height; // Important! AVLNode* left; AVLNode* right; &#125; AVL Tree Searchingsame as BST searching AVL Tree InsertionInsertion may violate the AVL tree property in 4 cases: Insertion into the right sub-tree of the right childLeft(anti-clockwise) rotation [single rotation] Insertion into the left sub-tree of the left childRight(clockwise) rotation [single rotation] Insertion into the right sub-tree of the left childLeft-right rotation [double rotation] Insertion into the left sub-tree of the right childRight-left rotation [double rotation] !!! Distinguish L/R of subtree or L/R of child !!! First insert the node to right place, then balance it by rotation.123456789AVLNode&lt;T&gt;* AVL&lt;T&gt;::insert(AVLNode&lt;T&gt;* p, const T&amp; d)&#123; if( !p ) return new AVLNode&lt;T&gt;(d); // Empty AVL tree if( d &lt; p-&gt;data ) p-&gt;left = insert(p-&gt;left, d); // Recursion on the left sub-tree else p-&gt;right = insert(p-&gt;right, d); // Recursion on the right sub-tree return balance(p);&#125; RotationSingle rotation operate on the node, replace original node by its childDouble rotation first operate on the child, then on the node, replace original node by its grandchild. 1234567891011121314151617181920212223242526272829303132AVLNode&lt;T&gt;* AVL&lt;T&gt;::rotateRight(AVLNode&lt;T&gt;* p)&#123; AVLNode&lt;T&gt;* pl = p-&gt;left; p-&gt;left = pl-&gt;right; pl-&gt;right = p; updateHeight(p); updateHeight(pl); return pl;&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::rotateRightLeft(AVLNode&lt;T&gt;* p)&#123; rotateRight(p-&gt;right); return rotateLeft(p);&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::balance(AVLNode&lt;T&gt;* p)&#123; h-&gt;height = max(h-&gt;right-&gt;height,h-&gt;left-&gt;height); // heightDiff() return (right subtree height) - (left subtree height) if( heightDiff(p) == 2 )&#123; if( heightDiff(p-&gt;right) &lt; 0 ) // if need double rotation p-&gt;right = rotateRight(p-&gt;right); return rotateLeft(p); // single rotation &#125; if( heightDiff(p) == -2 )&#123; if( heightDiff(p-&gt;left) &gt; 0 ) // if need double rotation p-&gt;left = rotateLeft(p-&gt;left); return rotateRight(p); // single rotation &#125; return p;&#125; AVL Tree DeletionThe idea is same as BST.When delete node with two children, the following code delete the node andrise the min node up.Then balance the whole tree.123456789101112131415161718192021222324252627282930313233AVLNode&lt;T&gt;* AVL&lt;T&gt;::findMin(AVLNode&lt;T&gt;* p) const&#123; return p-&gt;left ? findMin(p-&gt;left) : p;&#125;// NOT delete the Min-node!!AVLNode&lt;T&gt;* AVL&lt;T&gt;::removeMin(AVLNode&lt;T&gt;* p)&#123; if(p-&gt;left == 0) // If it's the min node, not return itself return p-&gt;right; // Return the second min means removeMin! p-&gt;left = removeMin(p-&gt;left); // Recursion on the left sub-tree return balance(p);&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::remove(AVLNode&lt;T&gt;* p, const T&amp; d) &#123; if( !p ) return 0; // Item is not found; do nothing if( d &lt; p-&gt;data ) p-&gt;left = remove(p-&gt;left,d); // Recursion on the left sub-tree else if( d &gt; p-&gt;data ) p-&gt;right = remove(p-&gt;right,d); // Recursion on the right sub-tree else &#123; // Item is found AVLNode&lt;T&gt;* pl = p-&gt;left; AVLNode&lt;T&gt;* pr = p-&gt;right; delete p; // Remove the node with value d if( !pr ) return pl; // Return left sub-tree if no right sub-tree AVLNode&lt;T&gt;* min = findMin(pr); // Find min. node of the right sub-tree min-&gt;right = removeMin(pr); // Did NOT delete the min node on right sub-tree min-&gt;left = pl; return balance(min); // Balance this node &#125; return balance(p); // Balance this node&#125; Explanationp-&gt;left = remove(p-&gt;left,d);Connect between parent and child. So no need to store parent pointer.delete p;Truly delete node with required data. But store its left and right nodes.Later return min of right sub-tree to its parent node. No memory leak.min-&gt;right = removeMin(pr);Return from the second min node and balanced from bottom up to the deleted node’s right child.Did not truly delete the min node. It should already be stored by findMin() function. Also refer here]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trees, Binary Trees, Binary Search Trees]]></title>
    <url>%2F2018%2F12%2F05%2FTrees-Binary-Trees-Binary-Search-Trees%2F</url>
    <content type="text"><![CDATA[Binary TreeInorder traversalpreorder traversalpostorder traversal Binary Search TreeBST InsertionAlways insert element at bottom as leaf.Remember to check if the element already exists in the tree. Recursion12345678910bool insert(const T&amp; x, Node*&amp; n ) // pass by referenceNode* insert(const T&amp; x, Node* n ) // pass by value need return pointer&#123; // base case 1: reach the node beyond leaf if(!n) &#123;n-&gt;x = new T(x);return true / new T(x);&#125; // base case 2: the data already exists if(n-&gt;x == x) return false / nullptr ; if(x &lt; n-&gt;x) return insert(x,n-&gt;left); else return insert(x,n-&gt;right);&#125; The insert function parameter Node*&amp; is a reference.We always pass reference to it, thought the value of the reference may be nullptr.e.g. insert(x,t-&gt;left) .In this case, t always refers to a Node*. Iteration12345678910111213141516bool insertIterative(const T&amp; x, Node* p ) // pass by value&#123; // A new pointer to refer to the leaf Node* pp = nullptr; for(;p!=nullptr;) &#123; // already exists if(p-&gt;x == x) &#123;return false;&#125; pp = p; // save parent for reference if(p &lt; p-&gt;x) p = p-&gt; left; else p = p-&gt;right; &#125; // root is nullptr if(!pp) root = new T(x); else pp = new T(x);&#125; We only need to know the value of the root of the tree.But we need to record the reference of the parent of the insertion node. BST Searchpublic interface: 1234567891011bool contain(const T&amp; x) const&#123; Node* p = root; Node* pp = nullptr; // Non-recursion method COMMON return search(x,p,pp); // Recursion method return searchRecur(x,p) != nullptr; &#125; The implementation of search(x,p) and searchIterative(x,p,pp) is as below. Recursioncan NOT keep track of the parent node pointer.123456789Node* searchRecur(const T&amp; x, Node* n ) // pass by value&#123; // base case 1: reach the node beyond leaf if(!n) &#123;return nullptr;&#125; // base case 2: find the data if(n-&gt;x == x) return n; if(x &lt; n-&gt;x) return search(x,n-&gt;left); else return search(x,n-&gt;right);&#125; Iteration1234567891011bool search(const T&amp; x, Node*&amp; n, Node*&amp; pn )// pass by reference&#123; for(;n!=nullptr;) &#123; if(n-&gt;x == x) &#123;return true;&#125; pn = n; if(x &lt; n-&gt;x) n = n-&gt; left; else n = n-&gt;right; &#125; return false;&#125; Can keep track of parent node pointer.Note the Node*&amp; n and Node*&amp; pn is pas by reference.They must be created before calling the function BST deletionneed findMin() function to help. 123456Node* findMin(Node* p)&#123; if(!p)return nullptr; while(p-&gt;left)p=p-&gt;left; return p;&#125; Delete a node with no child : pointer: current parentSimply delete it and make its parent’s original pointer to it nullptr Delete a node with one child : pointer: current parent childPass its pointer to the only child to its parent’s pointer to it. Then delete it. Delete a node with two child : pointer: current min/max parent of min/maxOverride its data to the max of left or min of rightActually delete the min/max node of the BST, which has no child Recursion1234567891011121314151617181920212223242526272829303132bool remove(const T&amp; x,Node*&amp; n) // pass by reference&#123; // Base case: NOT found if(!n) return false; // Recursive steps if(x&lt;n-&gt;x) return remove(x,n-&gt;left); if(x&gt;n-&gt;x) return remove(x,n-&gt;right); // node with no child if(!n-&gt;left &amp;&amp; !n-&gt;right) &#123; delete n; n = nullptr; &#125; // node with two children else if(n-&gt;left &amp;&amp; n-&lt;right) &#123; Node* rightMin = findMin(n-&gt;right); n-&gt;x = rightMin-&gt;x; // always true remove(n-&gt;x,rightMin); &#125; // node with only one child else &#123; Node* child = root-&gt;left?root-&gt;left:root-&gt;right; // save &amp;n to curr, later delete Node* curr = n; n = n-&gt;child; delete curr; &#125; return true;&#125; IterationNeed the parent node pointer of Min node.Hard to implement.]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Static Variables]]></title>
    <url>%2F2018%2F12%2F05%2FStatic-Variables%2F</url>
    <content type="text"><![CDATA[Static variablesGlobal Scope created only once in a program. reside on the static data region of the loaded program. have a lifetime across the entire run of a program. still may have limited scope: file, function, class. Function Scope initialized only once regardless how many times the function is called. retain their values across the function calls. can be accessed only inside the function. Class Scope variables in different objects are actually one variable. initialize variable must be done in global scopecannot initialize it inside the class. NOT in any function! Not in main(), either!NO keyword statice.g. 1234&gt; class A &#123;static int a;&#125;;&gt; int A::a = 10;&gt; int main()&#123;&#125;&gt; variable exists even if no class object is created. Static FunctionsGlobal Functions almost same as static variables. Member Functions do not have the implicit this pointer like regular non-static member functions. may be used even when there are no objects of the class! can only make use of static data members of the class. cannot be const nor virtual functions. can be defined outside the class declaration NO keyword static e.g. 1234&gt; class A &#123;static void f();&#125;;&gt; void A::f()&#123;&#125;&gt; int main()&#123;&#125;&gt; &gt;&gt;]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
        <tag>static variable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inheritance and Polymorphism]]></title>
    <url>%2F2018%2F12%2F05%2FInheritance-and-Polymorphism%2F</url>
    <content type="text"><![CDATA[Inheritance_Inheritance_: describe “is-a relationship”_Polymorphism_: derived class can perform like base class(object,pointer,reference) _Construction Order_: from inner to outer_Destruction Order_: from outer to inner member access control_public_: member functions of the class(inner)any member funtions of other classes(other class)any global functions(outside) _protected_: member functions and _friends_ of the classmember functions and _friends_ of its _derived classes_NOT for outside functions _private_: member functions and friends of the class Without inheritance, private and protected have exactly the same meaning. public protected private inheritance inheritance\member public protected private public inheritance public protected private protected inheritance protected protected private private inheritance private private private Public inheritance implements the “is-a” relationship Private inheritance is similar to “has-a” relationship Friendfriend of Base is not friend of Derived PolymorphismDynamic Binding&amp;Virtual FunctionOnce a method is declared virtual in the base class, it is automatically virtual in all directly and indirectly derived classes. Even if derived classed do not announce it’s virtual functions. Virtual destructor can make deleting a base pointer to derived object operate correctly. Do not rely on the virtual function mechanism during the execution ofa constructor. Similarly, if a virtual function is called inside the base class destructor,it represents base class’ virtual function: when a derived class is beingdeleted, the derived-specific portion has already been deleted beforethe base class destructor is called. Abstract Base Class(ABC):NO object of ABC can be created Its derived classes must implement the pure virtual functions. ABC is just an interface.]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ Memory Leak]]></title>
    <url>%2F2018%2F11%2F26%2FC-Memory-Leak%2F</url>
    <content type="text"><![CDATA[Memory detectionIn Windows using Visual Studiohere Open source toolshere]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI Chap.3 Searching]]></title>
    <url>%2F2018%2F06%2F28%2FAI-Chap-3-Searching%2F</url>
    <content type="text"><![CDATA[Describe problem formallystate: initial state:actions: each state corresponds to a set of available actionstransition model: return the result state of an action from the previous stategoal test:path cost: the essence of a solution is a sequence of actions that lead from the initial state to the goal state. Problem formulationincremental formulation: e.g. 8-queens puzzle. Initial state: no queen; Action: add a queen.complete-state formulation: e.g. 8-queens puzzle. Initial state: 8 queens; Action: move a queen. Searchsearch algorithms all use a search tree data structure with nodes. noden.state: correspond state in state space;n.parent: the node in the search tree that generated this node;n.action: the action that was applied to the parent to generate this node;n.path-cost: succeeded from parent; other conceptsexpanding: apply each legal action to the current state, thereby generating a new set of states.parent node ; child node ; leaf node: a node with no child nodefrontier(open list): the set of all leaf nodes avaiable for expansionexplored set(close list): the set of expanded nodesthe frontier separates the state space into explored region and unexplored regionrepeated state ; loopy path: a special case of the more general concept of redundant path Uninformed(blind) searchtree searchnot remember history causes loopy paths which is a special case of redundant path graph searchadd explored set(close list) to remember breadth-first searchFIFO queue; expand shallowest node;time complexity: O(b^d) (test while generating) O(b^(d+1)) (test while selected)space complexity: all nodes uniform-cost searchpriority queue; expand cheapest node;test while selected because while generating it may not be the optimal one.time complexity: e be the minimum step-cost,C* be the optimal cost. O(b^floor(1+(C*/c)))space complexity: near nodes depth-first searchLIFO stack; first to deepest node that has no successors;common to implement with a recursive function, recursive depth-first search(RDFS)time complexity: O(b^m^)space complexity: m is the maximum depth. O(bm)or O(m)( backtracking search in which expanded node remembers which successor to generate next) depth-limited searchinfinite state space cause depth-first search fall. It can be alleviated by a depth limit l. iterative deepening depth-first searchoften used in combination with depth-first search to find the best depth limit.gradually increase the depth limit till the goal is found.upper levels generate multiple times: N(IDS) = db+(d-1)b^2^+…+(1)b^d^ bidirectional search b^(d/2)^ + b^(d/2)^ &lt; b^d^ but action step must be reversible Summary Informed(Heuristic) searchgreedy best-first searchevaluates nodes by using just the heuristic function f(n) = h(n) time complexity: depends on heuristic function, worst O(b^m^), m is the maximum depth. space complexity: depends on heuristic function, worst O(b^m^), m is the maximum depth. A* search f(n) = g(n) + h(n) admissible heuristic: never overestimates the cost to reach the goalconsistent heuristic: the estimated cost of n is no greater than n’tree-search optimal if h(n) is admissible;graph-search optimal if h(n) is consistent absolute error: E=h*-h; relative error: e=(h*-h)/h time complexity: O(b^E^) or O(b^ed^)(constant step costs)space complexity: all nodes that f(n) &lt; C* Memory-bounded heuristic searchiterative-deepening A*(IDA*) cutoff = f(n) - g(n) - h(n) each iteration cutoff value is the smallest (f-g-h) of any node exceeded the cutoff on the previous iterationtoo little memory: between iteration, only retains cutoff = f-g-h recursive best-first search(RBFS)use f_limit variable to keep track of f-valueuse alternative variable to record the second-lowest f-value among successors (backed-up value)too little memory: each time change mind to alternative path, forget what it have done and regenerate nodes MA* and SMA*MA*(memory-bounded A*) and SMA*(simplified MA*) SMA* expands the best leaf until memory is full, drops the worst leaf node(highest h),like RBFS, SMA* back up the value of the forgotten node to its parent learning to search bettermetalevel state space: the internal(computational) state of a program that is searching in anobject-level state space: the real world problem Heuristic functionsFrom relaxed problemsignore some restriction rules and estimate the costrelaxed problem must be able to solved without searchwe can use more than one heuristic function like below: h(n) = max{h1(n), h2(n), …, hm(n)} From subproblems: Pattern databases subproblems: just solve part of the problem like going just half waypattern database: store these exact solution costs for every possible subproblem instancedisjoint pattern database: only consider partial cost of the subproblem that matters learning heuristics from experienceEach solution is an example for study.From these examples, a learning algorithm can be used to construct a function h(n)to predict solution costs for other states that arise during search.Inductive learning works best if supplied with features of a state relevant to predicting state’s valuethe feature is x1(n), x2(n), …, xm(n) after solved the problem, we found that h1(n), h2(n), …, hm(n) then we approach is to use a linear combination h(n) = c1*x1(n) + c2*x2(n) + … + cm*xm(n) Summary ![AI-Ch3-summary-2]/assets/AI-Chap-3-Searching/AI-Ch3-summary-2.png)]]></content>
      <categories>
        <category>Artificial Intelligence</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>AI</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dancing Video]]></title>
    <url>%2F2018%2F06%2F18%2FDancing-Video%2F</url>
    <content type="text"><![CDATA[Locking Judge Showhere Poppin Semi-final (win)here Poppin Final (lose)here]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>dance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Notes]]></title>
    <url>%2F2018%2F06%2F08%2FLinux-notes%2F</url>
    <content type="text"><![CDATA[Keyboardhere About finding fileshere]]></content>
      <categories>
        <category>Linux Is Not UniX</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cpp Notes]]></title>
    <url>%2F2018%2F05%2F27%2FCpp-Notes%2F</url>
    <content type="text"><![CDATA[About private member1 在C+的类的成员函数中，允许直接访问该类的对象的私有成员变量2 在类的成员函数中可以访问同类型实例的私有变量量3 拷贝构造函数里，可以直接访问另外一个同类对象（引用）的私有成员4 类的成员函数可以直接访问作为其参数的同类型对象的私有成员 Below codes work!!123456789101112private: int data;public: void lookOtherConst(const A&amp; a)&#123; cout&lt;&lt;"other's private data is "&lt;&lt; a.data &lt;&lt;endl;&#125; void lookOtherReference(A&amp; a)&#123; cout&lt;&lt;"other's private data is " &lt;&lt;a.data&lt;&lt;endl;&#125; void lookOther(A a)&#123; cout&lt;&lt;"other's private data is " &lt;&lt;a.data&lt;&lt;endl;&#125; About f*cking consthere 前缀 与 后缀 运算符前缀：在表达式计算中使用值之前递增或递减，表达式的值与操作数的值不同后缀：在表达式使用值之后递增或递减， 表达式的值与操作数相同e.g.1234567int a=5,b =5;cout&lt;&lt;"a&lt;b++ = "&lt;&lt;bool(a&lt;b++)&lt;&lt;endl;cout&lt;&lt;"a&lt;++b = "&lt;&lt;bool(a&lt;++b)&lt;&lt;endl;output:a&lt;b++ = 0 a&lt;++b = 1]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim Notes]]></title>
    <url>%2F2018%2F05%2F25%2FVim-notes%2F</url>
    <content type="text"><![CDATA[Neovim and vim-pluginA good blog link for both introduction of neovim and vim-plugin. search and substitutionhere fold and unfoldhere keyboard remaphere]]></content>
      <categories>
        <category>VIM is so hard to use</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Origin]]></title>
    <url>%2F2018%2F05%2F23%2FOrigin%2F</url>
    <content type="text"><![CDATA[Trust me, you would not like to read the shix inside, nor do I. Decrypt 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]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
</search>
