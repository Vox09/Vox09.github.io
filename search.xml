<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Differential Equation]]></title>
    <url>%2F2019%2F04%2F10%2FDifferential-Equation%2F</url>
    <content type="text"><![CDATA[First OrderLinearfirst order linear equation standard form: {dy\over dt}+p(t)y=g(t)In some cases it is possible to solve a first order linear equation immediately byintegrating the equation.Unfortunately, most of them should use a method called integrating factor. \mu(t){dy\over dt}+\mu(t)p(t)y=\mu(t)g(t)where ${d\mu\over dt}=p(t)\mu(t)$ so that $\mu(t)=ce^{G(t)}$ where $G(t) = \int g(t)$ SeparableM(x)dx+N(y)dy=0We can directly integral both sides to get the solution. Second Order Linear{d^2y\over dt^2}=f(t,y,{dy\over dt})Linear if $f(t,y,{dy\over dt})=g(t)-p(t){dy\over dt} -q(t)y$ Homogenous EquationWhen $g(t)=0$ the second order linear equation is said to be homogenous y''+p(t)y'+q(t)y=0Constant CoefficientsA second order linear homogenous equation with constant ay’’+by’+cy=0has characteristic equation ar^2+br+c=0The roots $r_1$ and $r_2$ correspond to $y_1=e^{r_1}$ and $y_2=e^{r_2}$Any linear combination of $y_1$ and $y_2$ is a solution. The distribution of the roots have three conditions Two distinct real roots. Just exponential function Two distinct complex roots. By, Euler’s formula, exponential of complex number is Sinusoids. Its real part $u$ and its imaginary part $v$ are also solutions of it. We need to express$y=c_1u+c_2v$ Two identical real roots$r_0$. Then the solution is $y_1=e^{r_0t}\quad y_t=te^{r_0t}$. We can get this result by assume $y_2=v(t)y_1$ and finally we can find $v(t)=c_1+c_2t$ WronskianWhen given a initial point $y_0$, we have the equations: c_1y_1(t_0)+c_2y_2(t_0)=y_0\\ c_1y_1’(t_0)+c_2y_2’(t_0)=y_0’The Wronskian determinant is defined as W=\begin{vmatrix}y_1&y_2\\y_1’&y_2’\end{vmatrix} = y_1y_2’+y_1’y_2It is a function of independent variable t. Thus, different t correspond to different W. If at a point $t=t_0\quad W(t_0)\ne0$ the solution is unique. The constants are c_1={\begin{vmatrix}y_0&y_2(t_0)\\y_0’&y_2’(t_0)\end{vmatrix} \over W(t_0)} \quad c_2={-\begin{vmatrix}y_0&y_1(t_0)\\y_0’&y_1’(t_0)\end{vmatrix}\over W(t_0)}For the equation $y_0=c_1y_1+c_2y_2$ If at a point $t=t_0\quad W(t_0)=0$ the initial conditions cannot be satisfied not matter how $c_1$ and $c_2$ are chosen. $y=c_1y_1(t)+c_2y_2(t)$ is called the general solution.$y_1$ and $y_2$ are said to form a fundamental set of solutions if Wronskian is nonzero. Abel TheoremIf $y_1$ and $y_2$ are solutions of L[y]=y’’+p(t)y’+q(t)y=0Where p and q are continuous on an open interval $I$, then the Wronskian is given by W(y_1,y_2)(t)=c e^{-\int p(t)dt}Therefore, $W(y_1,y_2)(t)$ is either zero or else is never zero for all t in the interval. Nonhomogeneous EquationsA nonhomogenous equation is L[y]=y’’+p(t)y’+q(t)y=g(t)\ne 0We can solve it by thee steps: Find the general solution $c_1y_1(t) +c_2y_2(t)$ of the corresponding homogenous equation. Find some single solution $Y(t)$ of the nonhomogenous equation. Often this solution is referred to as a particular solution. Form the sum of the functions found in steps 1 and 2. Method of Undetermined CoefficientsMake an initial assumption about the form of the particular solution $Y(t)$. But with the coefficients left unspecified.e.g. for $y’’-3y’-4y=3e^{2t}$ we guess that $Y(t)=Ae^{2t}$ and finally obtain $A=-{1\over 2}$ ==VERY IMPORTANT==If one of the terms of our first guess $Y(t)$ is included in the general solution to the corresponding homogeneous equation, try $tY,t^2Y,…$ and so on. Series Solutions of Second Order Linear EquationWe now consider the second order linear equations when the coefficients are functions of the independent variable. P(x){d^2y \over dx^2 }+ Q(x){dy\over dx} +R(x)y =0A point $x_0$ where $Q(x_0)\over P(x_0)$ is analytic is called an ordinary point, otherwise is called singular point. Solutions near ordinary point set $y=\sum_{n=0}^\infty a_nx^n$ Then we have the kth derivative $y^{(k)}=\sum_{n=k}^\infty n(n-1)…(n-k+1)a_nx^{n-k}$ substitute the series for the original function to get the recurrence relation of the coefficients $a_n$ Usually we need to shift the index of summation by replacing $n$ by $n-sth$ The two initial items are the arbitrary coefficientsy=a_0y_1+a_1y_2=a_0\sum...+a_1\sum...where $y_1,y_2$are two power series solutions that are analytic at $x_0$. The radius of convergence for each of the series solutions $y_1$ and $y_2$ is at least as the minimum of the radii of convergence of the series for $Q\over P$ and $R \over P$ Solutions near singular pointWe first consider a relatively simple differential equation that has a singular point, Euler equation, singular point at $x=0$. Euler equationL[y]=x^2y''+\alpha xy' +\beta y = 0set $y=x^r$ we can obtain $r_1,r_2={-(\alpha-1)\pm\sqrt{(\alpha-1)^2-4\beta}\over 2}$Three possibilities: Real, distinct roots $y=c_1x^{r_1}+c_2x^{r_2}$ Equal roots $y=(c_1+c_2lnx)x^{r_1}$ Complex roots $y=c_1x^\lambda cos(\mu lnx)+c_2x^\lambda sin(\mu lnx)$ Regular singular pointP(x)y'' +Q(x)y' +R(x)y=0where $x_0$ is a singular point. This means that $P(x_0)=0$ and that at least one of $Q$ and $R$ is not zero at $a_0$ weak singularities$\lim_{x\to x_0}(x-x_0){Q(x)\over P(x)}$ is finite and $\lim_{x\to x_0}(x-x_0)^2{R(x)\over P(x)}$ is finite This means the singularity in $Q/P$ can be no worse than $(x-x_0)^{-1}$ and the singularity in $R/P$ can be no worse than $(x-x_0)^{-2}$. Such a point is called a regular singular point. Otherwise, called irregular singular point. Laplace TransformAmong the tools that are very useful for solving linear differential equations are integral transforms.An integral transform is a relation of the form F(s)=\int_\alpha^\beta K(s,t)f(t)dywhere $K(s,t)$ is given function, called the kernel of the transformation. When $K(s,t)=e^{-st}$ the transform is called Laplace transforms. \mathcal L\{f^{(n)}(t)\}=s^n\mathcal L\{f(t)\}-s^{n-1}f(0)-...-sf^{(n-2)}(0)-f^{(n-1)}(0)ProcedureTo solve a second order linear equation with constant coefficients ay''+by'+cy=f(t)By Laplace transform, we have $a[s^2Y(s)-sy(0)-y’(0)]+b[sY(s)-y(0)]+cY(s)=F(x)$where the $F(s)$ is the transform of $f(t)$. Then we find that $Y(s)={(as+b)y(0)+ay’(0)+F(s)\over as^2+bs+c}$ Then the $y(t)$ whose LT is $Y(t)$ is the solution Unit step function\mathcal L\{u(t-c)\}=\int_c^\infty e^{-st}dt={e^{-cs}\over s}\quad s>0If $F(s)=\mathcal L\{f(t)\}$ exists for $s&gt;a\ge0$, and if $c$ is a constant, then \mathcal L\{e^{ct}f(t)\}=F(s-c), \quad s>a+cConversely, if $f(t)=\mathcal L^{-1}\{F(s-c\}$, then $e^{ct}f(t)=\mathcal L^{-1}\{F(s-c)\}$ Systems of First Order Linear EquationsSystems of a homogeneous linear equations with constant coefficients are of the form$\boldsymbol{x}’=A\boldsymbol x$ where $A$ is a constant $n \times n$ matrix. Solution By setting $\boldsymbol x =\xi e^{rt}$, we can get $r\xi e^{rt}=A\xi e^{rt}$. Therefore, $(A-r\boldsymbol I)\xi=0$.where $\boldsymbol I$ is the $n\times n$ identity matrix. Solve $det(A-r\boldsymbol I) = 0 $We got $r$, the eigenvalues of $A$. Each one correspond to a constant eigenvectors $\xi^{(i)}$ For phase portrait, $r0$ means outward direction. Possibilities of eigenvalues All eigenvalues are real and different from each other.$\boldsymbol x = c_1\xi^{(1)}e^{r_1t}+…+c_n\xi^{(n)}e^{r_nt}$ Some eigenvalues occur in complex conjugate pairs.$\boldsymbol x^{(1)}(t)=\xi^{(1)}e^{r_1t}\quad \boldsymbol x^{(2)}(t)=\bar\xi^{(1)}e^{\bar r_1t}$ if $r_1 = \lambda +\mu \quad\xi = \boldsymbol a +i\boldsymbol b$, we can write $\boldsymbol x^{(1)}(t)=\boldsymbol u(t) + i\boldsymbol v(t)$$\boldsymbol u(t) = e^{\lambda t}(\boldsymbol a cos \mu t-\boldsymbol b sin \mu t)\\\boldsymbol v(t) = e^{\lambda t}(\boldsymbol a sin\mu t+\boldsymbol b cos\mu t)$ Then the general solution is$\boldsymbol x= c_1\boldsymbol u(t) +c_2\boldsymbol v(t) +c_3\xi^{(3)}e^{r_3t}+…+c_n\xi^{(n)}e^{r_nt}$ Some eigenvalues, either real or complex, are repeated.TBC…]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>ODE</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Probability]]></title>
    <url>%2F2019%2F03%2F21%2FProbability%2F</url>
    <content type="text"><![CDATA[Basic Concepts random experimentan experimental procedure and one or more measurements or observations. sample spacethe set of all possible outcomes. discrete sample space if the number of outcomes is countable continuous sample space if not countable eventcertain conditions for an outcome. A subset A of the sample space S An event A occurs if the outcome is a member of A. the certain event, S, consists of all outcomes. the null event, $\phi$, contains no outcomes. An elementary event contains only one outcome. Replacement and OrderingWe can choose $k$ objects from a set $A$ that has $n$ members indifferent ways Replacement: With replacementAfter selecting an object and noting its identity, the object is put back before the next selection. Without replacementThe object is not put back before the next selection. Ordering: With orderingThe order in which we draw the objects is recorded Without orderingOnly the identity and number of times each object is drawn is important Replacement ordering number of outcomes Y N - Y Y $n^k$ N N ${n\choose k}={n!\over (n-k)! k!}$ N Y ${n\choose k}k!={n!\over(n-k)!}$ Note: when with replacement and without ordering, outcomes are NOT equally probable. There are two ways to choose a same object. But they counts the same outcome. Conditional Probability Independence$P[A\cap B]=P[A]P[B]$ also $P[A|B] = P[A]$P[A_1A_2...A_n]=\prod_{i=1}^n P[A_i] Exclusive$P[A\cup B]=P[A]+P[B]-P[A\cap B] = P[A]+P[B]$ P[A_1A_2...A_n] = 0 Conditional probability$P[A|B]={p[A\cap B]\over p[B]}$ where we assume that $P[B]&gt;0$ Total Probability Theorem P[A]=\sum_{i=1}^n P[A|B_i]P[B_i] Bayes’ RuleP[B_j|A]={P[AB_j]\over P[A]}={P[A|B_j]P[B_j]\over\sum_{k=1}^n P[A|B_k]P[B_k]} Sequential ExperimentsExperiments that involve repetitions or multiple participants can often be viewed as a sequence of sub-experiments. The sub-experiments can be identical or non-identical, dependent or independent. The individual sample spaces can be identical or non-identical. If the experiments are identical, the individual sample spaces are identical but not vice versa. Tossing a coin n times: repetition independent identical sub-experiments identical individual sample spaces Checking the number of students sleeping in class: multiple participants independent? (maybe not. Others sleep then I sleep) identical individual sample spaces non-identical sub-experiments (Dalaos do not sleep but I do) Bernoulli trialsan experiment once with two possible outcomes Binomial probability lawn independent Bernoulli trials$p_n(k)={n\choose k}p^k(1-p)^{n-k}$ Multinomial probability law*M partition of sample space, so $p_1…p_M$ mutual exclusive$P[(k_1,k_2,…,k_M)]={n!\over k_1!k_2!…k_M!}p_1^{K_1}p_2^{K_2}…p_M^{K_M}$ Geometric probability law M be the time until the first success$p_M(m)=p(1-p)^{m-1}$ N be the time before the first success$p_N(n)=p(1-p)^n$ Sequences of dependent experimentsReferred to as a Markov Chain Single Random VariablesA random variable $X$ is a function that assigns a number to everyoutcome $\xi$ of an experiment. Underlying sample space $S$ is called the domain of the RV. $\xi\in S$Set of all possible values of $X$ called the range of the RV. $X(\xi)\in S_X$ Equivalent Event$A=\{\xi:X(\xi)\in B\}$ CharacteristicPMFprobability mass function p_X(x)=P[X=x]=P[\{\xi:X(\xi)=x\}]For continuous RV, $P[X=x]=0$. $p_X(x)&gt;0$ $\sum_{x\in S_x}p_X(x)=1$ $P[X\ in\ B]=\sum_{x\in B}p_X(x)$ where $B\in S_X$ CDFcumulative distribution functionThe summation of PMF, the integral of PDF, is the cumulative distribution(CDF). F_X(x)=P[X\le x]\\=\sum_{n=-\infty}^x p_X(n)=\int_{-\infty}^xf_X(s)ds Discrete RV: like a series of stairs jumping up. Continuous RV: a continuous increasing function. Mixed RV: continuous function with discrete jumps where $P[X=x_j]=c$ PDF probability density function. The value is not a probability! P[a\le X\le b]=\int_a^bf_X(x)dx\\f_X(x)={d\over dx}F_X(x)For discrete RV: PMF multiply an impulse $\delta(x-x_k)$. MeanExpectation/Mean of a RV (if the sum/integral converges absolutely) m_X = E[X]=\sum_{x\in S_x}xp_X(x) \\E[X]=\int_{-\infty}^\infty xf_X(x)dxExpectation/Mean of a function of a RV $Y=g(x)$ E[Y]=\sum_k g(x_k)p_X(x_k)\\E[Y] = \int_{-\infty}^\infty g(x)f_X(x)dxLinearity of ExpectationGenerally $E[\sum_i a_ig_i(X)]=\sum_i a_iE[g_i(X)] \ne \sum a_ig_i(E[X])$if $g_i$ are all linear, the the tree above equal. VarianceVar[X] = E[D^2]=E[(X-E[X])^2]\\Std[X]=\sqrt {Var[X]}Useful formula$Var[X] = E[X^2]-(E[X])^2 \\Var[cX] = c^2Var[X]$ MomentsThe Mean and variance are examples of the moments of a RV. The $n^{th}$ moment of a RV E[X^n] = \sum_kx_k^np_X(x_k)\\E[X^n] = \int_{-\infty}^\infty x^nf_X(x)The $n^{th}$ central moment of a RV E[(X-E[X])^n] = \sum_k(x_k-E[X])^np_X(x_k)\\E[(X-E[X])^n] = \int_{-\infty}^\infty(x-E[X])^nf_X(x_k)Important moments:1st moment, $E[X]$ is the Mean2nd central moment, $E[(x-E[x])^2]$ is the variance Minimum Mean Squared Error EstimationMSE(c) = E[(X-c)^2]=\int_{-\infty}^\infty(x-c)^2f_X(x)dxThus, the best guess is $c=E[X]$!Also, the variance is ten the minimum mean squared error associated with the best guess. Discrete RVA discrete random variable assumes values from a countable set. $S_X=\{x_1,x_2,…\}$ A discrete random variable is finite if its range is finite.$S_X=\{x_1,x_2,…,x_n\}$ BernoulliX only take two values, either 1 or 0. P_X[0] = 1-p\quad P_X[1] = pMean $E[X]=p$Variance $Var[X] = p(1-p)$ Single coin toss Occurrence of an event of interest BinomialThe number of times that an event occurs. P_X[k]={n\choose k} p^k(1-p)^{n-k}\quad\text{for k = 0,1,2...}Mean $E[X] = np$Variance $Var[X]=np(1-p)$ Multiple coin flips Occurrence of a property in individuals of a population(e.g. bit errors in a transmission) When $n\to\infty$ , turn to Poisson RV GeometricSuppose a random experiment is repeated, In each repeat, it occurs independently and with probability $p$ TrialsThe number of trials until the first success occurs. P_X[k]=(1-p)^{k-1}p\quad\text{for k = 1,2,...}Mean $E[X]={1 \over p}$Variance $Var[X] = {1-p\over p^2}$ FailuresThe number of failures before the first success, $X’ = X-1$ is also a geometric RV. P_{X’}[k]=(1-p)^kp\quad\text{for k = 0,1,2...}Mean $E[X‘]={1-p \over p}$Variance $Var[M’]={1-p\over p^2}$ Number of transmissions required until an error free transmission. Memoryless propertyNo matter how hard you ever tried, the probability to succeed remains the same. Uniform(discrete)Values in a set of integers are with equal probability. S_X=\{0,1,2...,M-1\}\\ P_X[k]={1\over M}\quad \text{for k }\in S_xMean $E[X] = {M-1\over 2}={a+b\over 2}$Variance $Var[X]={M^2-1\over 12}={(b-a+1)^2-1\over12}$ PoissonThe number of occurrences of an event in a certain interval of time or space. P_X[k]={\alpha^k \over k!}e^{-\alpha}\quad \text{for k=0,1,2...}The parameter $\alpha$ is the average number of events in the interval. Mean $E[N]=\alpha$Variance $Var[N]=\alpha$ Number of hits on a website in one hour number of particles emitted by a radioactive mass in a fixed time period Relationship with Binomial RV:For a Binomial random variable with $p = {\alpha\over n}$:$p_0=(1-p)^n = (1-{\alpha\over n})^n \to e^{-\alpha} \quad\text{as}\ n\to\infty$${p_{k+1}\over p_k} = {(n-k)p\over(k+1)(1-p)}={(1-k/n)\alpha\over(k+1)(1-\alpha/n)}$when$n\to\infty\quad{p_{k+1}\over p_k}={\alpha\over k+1}$ Continuous RVUniform(continuous)Intervals of the same length on the domain have the same probability. S_X=[a,b]\\ f_X(x)={1\over {b-a}}\quad \text{for x }\in S_x\\ F_X(x)={x-a\over {b-a}}\quad \text{for x }\in S_xMean $E[X] = {a+b\over 2}$Variance $Var[X]={(b-a)^2\over 12}$ ExponentialThe length of time that wait next train. $\lambda$ is the occurrence time per unit time. f_X(x)=\lambda e^{-\lambda x}\quad\text{when }x\gt 0\\ F_X(x)=1-e^{-\lambda x}\quad\text{when }x\gt 0Mean $E[X]={1\over\lambda}$Variance $Var[X] = {1\over \lambda^2}$ Memoryless propertyNo matter how hard you ever wait, the probability to meet the right one remains the same. Gaussian(Normal)Variables that tend to occur around a certain value,m , the mean. f_X= {1\over \sqrt{2\pi}\sigma}e^{-(x-m)^2\over 2\sigma^2}\we define the CDF of the”normalized” Gaussian with mean m=0 and standard deviation $\sigma=1$ as \Phi(x)={1\over \sqrt{2\pi}}\int_{-\infty}^x e^{-t^2 \over 2}dtThen the CDF of a Gaussian RV, $X$, with mean $m$ and standard deviation $\sigma$ is F_X(x)=\Phi({x-m\over \sigma})={1\over \sqrt{2\pi}}\int_{-\infty}^{x-m\over\sigma} e^{-t^2 \over 2}dt Q-function$Q(x) = 1-\Phi(x)=P[X&gt;x]$By symmetry, $Q(0)={1\over 2}$ and $Q(x)=\Phi(-x)$ Conditional Conditional PMF/CDF/PDF p_X(x_K|C)={P[\{X=x_k\}\cap C]\over P[C]}\\F_X(x|C)={P[\{X\le x\}\cap C]\over P[C]}\\f_X(x|C)={d\over dx}F_X(x|C) Total probability p_X(x)=\sum_i p_X(x|B_i)P[B_i] Conditional expected valuem_{X|B}=E[X|B]=\sum_{x\in S_X}xp_X(x|B) Conditional varianceVAR[X|B]=E[(X-m_{X|B})^2|B]=E[X^2|B]-m_{X|B}^2 Compute Everything by Conditioningf_X(x)=\sum_i f_X(x|b_i)P[B_i]\\E[X]=\sum_i E[X|B_i]P[B_i] TransformationGiven a random variable $X$ with known distribution and a real valued function $g(x)$, such that $Y = g(X)$ is also a random variable. Find the distribution of $Y$. Type of Y If X is discrete, Y must be discrete. If X is continuous, Y can be all three types, depends on $g(x)$ Key ideaTo find equivalent events in X for suitably defined events in Y. Approaches If Y is Discrete, find the PMF at the possible values of Y$p_Y(k) = P[Y=k]=P[\{X:g(X)=k\}]$ If Y is continuous, there are two appsroaches: Find the CDF of Y$F_Y(y)=P[Y\le y]=P[\{X:g(X)\le y\}]$ Find the PDF of Y$f_Y(y_0)=\sum_{x:g(x)=y}{f_X(x_)\over |g’(x)|}$VERY USEFUL $\uparrow\uparrow\uparrow\uparrow\uparrow$ Multiple Random Variables vector random variableA vector random variable $\vec X$ is a function that assigns a vector of real numbers to every outcome of an experiment. One random variable can be considered as a mapping from the sample space to the real line.Two random variables can be considered as a mapping from the sample space to the plane. scattergramsvisualize the joint behavior of two RVs. PairsDT case Joint PMF $p_{X,Y}(j,k)=P[\{X=j\}\cap \{Y=k\}]$ Marginal PMF$p_X(j)=P[X=j,Y=anything]$The function of $X$ is the sum of all $Y$ along y-axis. PMF of FunctionSuppose that $Z=g(X,Y)$, is an integer valued function$p_Z(k)=P[\{Z=k\}]=\sum_{(i,j):g(i,j)=k}p_{X,Y}(j,l)$ CT case Joint CDF$F_{X,Y}(x,y)=P[\{X&lt;x\}\cap \{Y&lt;y\}]$ Joint PDF$f_{X,Y}=\frac {\partial ^2 F_{X,Y}(x,y)}{\partial x\partial y}$ Marginal PDF$f_X(x)={dF_X(x)\over dx}={dF_{X,Y}(x,\infty)\over dx}=\int_{-\infty}^\infty f_{X,Y}(x,\alpha)d\alpha$The function of $X$ is the integral of all $Y$ along y-axis. PDF of FunctionFor $Z=g(X,Y)$. To find the $f_Z(z)$ Pick a real number z Find the region $D_z=\{(x,y):g(x,y)\le z\}$ Evaluate $F_Z(z)=\int\int_{D_z}f(x,y)dxdy$ Differentiate $f_z(z)={d\over dz}F_Z(z)$ If $X$ and $Y$ are independent, the PDF of $Z=X+Y$ is$f_Z=\int_{-\infty}^\infty f_X(x)f_Y(z-x)dx$The convolution integral CharacteristicMeanfor a function $Z=g(X,Y)$ E[Z]=\sum_j \sum_k g(j,k)p_{X,Y}(j,k)\\E[Z] = \int_{-\infty}^\infty \int_{-\infty}^\infty g(x,y)f_{X,Y}(x,y)dxdy The mean of sum is the sum of mean.$E[X_1+X_2+…+X_n]=E[X_1]+E[X_2]+…+E[X_n]$ If $X$ and $Y$ are independent$E[g_1(X)g_2(Y)]=E[g_1(X)]E[g_2(Y)]$ MomentsThe $j^{th},k^{th}$ moment of a pair E[X^jY^k]=\int_{-\infty}^\infty\int_{-\infty}^\infty x^jy^kf(x,y)dxdyThe $j^{th},k^{th}$ central moment of a pair E[(X-E[X])^j(Y-E[Y])^k]For $Z=X+Y$$VAR[Z]=VAR[X]+2COV(X,Y)+VAR[Y]$If $X$ and $Y$ are uncorrelated, then $VAR[Z]=VAR[X]+VAR[Y]$ CovarianceThe covariance indicates how $X$ and $Y$ vary together. E[(X-E[X])(Y-E[Y])]=COV(X,Y) Positive$X$ smaller, $Y$ smaller Negative$X$ larger, $Y$ larger Magnitude is not a good measure of their relationship. Useful formula$COV[X,Y]=E[XY]-E[X]E[Y]$If either $X$ or $Y$ is zero mean, the covariance and correlation are identical $COV[X,Y]=E[XY]$ Correlation$X$ and $Y$ are uncorrelated if $COV[X,Y]=0$ thus $E[XY]=E[X]E[Y]$ correlation coefficient$\rho_{X,Y}={COV[X,Y]\over \sigma_X\sigma_Y}=\rho_{Y,X}$where $\sigma_x^2 = VAR(X)$ $|\rho_{X,Y}|\le 1$ Positive$X$ larger, $Y$ larger Negative$X$ larger, $Y$ smaller Conditional Probability Conditional PMF/CDF/PDF p_{Y|X}(k|j)={p_{X,Y}(j,k)\over p_X(j)}\\F_{Y|X}(y|x)={\int_{-\infty}^yf_{X,Y}(x,\beta)d\beta \over f_X(x)}\\f_{Y|X}(y|x)={f_{X,Y}(x,y)\over f_X(x)}All functions above are a cross-plane parallel to y-axis of original 2D function. Total probability theorem f_Y(y)=\int_{-\infty}^\infty f_{Y|X}(y|x)f_X(x)dx Bayes theorem f_{X|Y}(x|y)={f_{Y|X}(y|x)f_x(x)\over\int_{-\infty}^\infty f_{Y|X}(y|x)f_X(x)dx} Independence P[\{X\in A_X\}\cap \{Y\in A_Y\}]=P[X\in A_X]P[Y\in A_Y]If $X$ and $Y$ are independent then$p_{Y|X}(k|j) = p_Y(k)$ Conditional MeanE[Y|x]=\int_{-\infty}^\infty yf_{Y|X}(y|x)dyThe conditional expected value of $Y$ is a function of $X$.More general, $E[g(Y)|x]=\int_{-\infty}^\infty g(y)f_{Y|X}(y|x)dy$ Since $E[Y|X]$ is a function of $X$, it is also a RV.Taking the expectation over $X$, the conditioning disappears! E[E[Y|X]] = E[Y]more generally, $E[g(Y)]=E[E[g(Y)|X]]$This is an expectation version of the total probability theorem More than two RVsAn $N$ dimensional random vector is a mapping from a probability space $S$ to $R^N$. Often we think $n$ RVs as a single n-dimensional random vector. CDF/PDF/PMF$F_{X_1,X_2,…X_n}(x_1,x_2,…x_n) = P[X_1\le x_1,X_2\le x_2,…X_n\le x_n]$$f_{X_1,X_2,…X_n}(x_1,x_2,…x_n)={\partial^n\over\partial x_1…\partial x_n}F_{X_1,X_2,…X_n}(x_1,x_2,…x_n)$$p_{X_1,X_2,…X_n}(x_1,x_2,…x_n)=P[X_1= x_1,X_2= x_2,…X_n= x_n]$ Multinomial DistributionA generalization of the binomial distribution.An experiment consists of $n$ independent trials of a sub-experiment with $m$ possible outcomes. $\sum p_i=1$ and $\sum X_i=n$. p_{X_1,X_2,...X_n}(k_1,k_2,...k_n)={n!\over k_1!k_2!...k_m!}p_1^{k_1}p_2^{k_2}...p_m^{k_m}if $k_1+k_2+…+k_m=n$ and $k_i\ge 0$ for all $i$, and zero other wise. Marginal StatisticsEliminate variable from a PDF/PMF by integrating/summing$f_{X_1,X_2,…X_{n-1}}(x_1,x_2,…x_{n-1})=\\\int_{-\infty}^\infty f_{X_1,X_2,…X_n}(x_1,x_2,…x_n)dx_n$$p_{X_1,X_2,…X_{n-1}}(x_1,x_2,…x_{n-1})=\\\sum_{-\infty}^\infty p_{X_1,X_2,…X_n}(k_1,k_2,…k_n)$ ConditionalConditional PDF$f_{X_1X_2X_3}(x_1,x_2|x_3)={f_{X_1X_2X_3}(x_1,x_2,x_3)\over f_{X_3}(x_3)}\\f_{X_1X_2X_3}(x_1|x_2,x_3)={f_{X_1X_2X_3}(x_1,x_2,x_3)\over f_{X_2X_3}(x_2,x_3)}$ Conditional PMF$p_{X_1X_2X_3}(k_1,k_2|k_3)={p_{X_1X_2X_3}(k_1,k_2,k_3)\over p_{X_3}(x_3)}\\p_{X_1X_2X_3}(k_1|k_2,k_3)={p_{X_1X_2X_3}(k_1,k_2,k_3)\over p_{X_2X_3}(k_2,k_3)}$ Correlation Matrix Covariance Matrix Relationship between Correlation and Covariance\boldsymbol C=\boldsymbol R-E[\vec X]E[\vec Y]if the mean vector is zero, then $\boldsymbol C=\boldsymbol R$ Symmetric Diagonal if and only if the elements of the random vector are uncorrelated Sample and Stochastic ProcessSampleSuppose $X_1,X_2,…,X_n$ are independent and identically distributed(i.i.d.), i.e. they all have the same distribution.Let $\mu=E[X_i]$ and $\sigma^2=VAR(X_i)$ and $S=\sum_{i=1}^nX_i$ $E[S]=n\mu\\VAR(S)=n\sigma^2$ Sample Mean$M_n={1\over n}\sum_{j=1}^nX_j$ Then we can get imply$E[M_n] = m\quad VAR[M_n]={\sigma^2\over n}$The $M_n$ is getting closer and closer to the exact value. Markov Inequality$X$ non-negative RV$P[X\ge a]\le {E[X]\over a}$Based on the mean ONLY! Chebyshev Inequality$P[|X-m|\ge a]\le{\sigma^2\over a^2}$ Laws of Large NumbersLet $X_1,X_2,…$ be a sequence of i.i.d. RV with finite mean $\mu$.Weak version $\lim_{n\to\infty}P[|M_n-\mu|&lt;\epsilon]=1$Strong version $P[\lim_{n\to\infty}M_n=\mu]=1$ Central LimitLet $X_i$ for $i\in\{1,2,…,n\}$ are i.i.d. with mean and variance $\mu, \sigma^2$.Define $S_n$ to be the sum of $X_i$If we define $Z_n={S_n-n\mu\over\sigma\sqrt n}$$\lim_{n\to\infty}P[Z_n&lt;z]={1\over \sqrt{2\pi}}\int_{-\infty}^ze^{-x^2\over 2}dx$ In other words, the distribution of $Z_n$ approaches the distribution of a Gaussian with zero mean and unit variance.$P[\{X&gt;x\}]=Q(x)\quad P[|X|&lt;\epsilon=1-2Q[(\epsilon)]$ $\lim_{n\to\infty}P[Z_n&lt;z]={1\over \sqrt{2\pi}}\int_{-\infty}^ze^{-x^2\over 2}dx$ Stochastic ProcessDefinitionA random process or stochastic process maps a probability space $S$ to a set of functions, $X(t,\xi)$ It assign to every outcome $\xi \in S$ a time function $X(t,\xi)$ for $ t\in I$,where $I$ is a discrete or continuous index set. A random process is discrete-time if the time index set $I$ is a countable set. Otherwise it is continuous. $t$ $\xi$ $X(t,\xi)$ vary vary ensemble/family of functions vary fixed realization/ sample functions fixed vary random variable fixed fixed number A random process is uniquely specified by the collection of all n-th order distribution or density functions.$F_{X(t_1)X(t_2)…X(t_n)}(x_1,x_2,…,x_n)=P[\bigcap_{i=1}^nX(t_i)\le x_i]$$f_{X(t_1)X(t_2)…X(t_n)}(x_1,x_2,…,x_n)={\partial^n\over\partial x_1\partial x_2…\partial x_n}F_X$ CharacteristicsMean function$m_X(t)=E[X(t)]$ function of time mean of i.i.d. is constant mean of i.s.i. grow linearly Variance function$VAR[X(t)]=E[(X(t)-m_X(t))^2]$ function of time mean of i.i.d. is constant mean of i.s.i. grow linearly Autocorrelation$R_X(t_1,t_2)=E[X(t_1)X(t_2)]=\iint xyf_{X(t_1),X(t_2)}(x,y)dxdy$ Covariance$C_X(t_1,t_2)=Cov(X(t_1),X(t_2))=R_X(t_1,t_2)-m_X(t_1)m_X(t_2)$ covariance of i.i.d. is a delta function$C_X(n_1,n_2)=\sigma^2\delta(n_1,n_2)$ covariance of i.s.i.$C_S(m,n)=\sigma^2min(m,n)$ Correlation coefficient$\rho_x(t_1,t_2)={C_X(t_1,t_2)\over \sqrt{C_X(t_2,t_2)}\sqrt{C_X(t_2,t_2)}}$ correlation of i.i.d. is$R_X(n_1,n_2)=\sigma^2\delta(n_1,n_2)+m^2$ StationaryThe joint distribution of any set of samples does not depend on the placement of the time origin. i.id. random process is stationary. The mean and variance of stationary processes are constant. The autocorrelation and covariance functions of a stationary process depend only upon the time difference $t1 - t2$. Wide Sense Stationary(WSS)$m_X(t)=m$ for all $t$$R_X(t_1,t_2)=R_X(t_1-t_2)$ for all $t_1,t_2$$C_X(t_1,t_2)=C_X(t_1-t_2)$ for all $t_1,t_2$ $R_X(0)$ is the average power of the process, $E[X(t)^2]$ $R_X(\tau)$ is an even function of $\tau$ $R_X(\tau)\le R_X(0)$ If $X(t)$ is Gaussian, it is also stationary ProcessesIndependent and Identically Distributed ProcessA discrete-time process $X_n$ is said to be independent and identically distributed or i.i.d. if all vectors formed by a finite number of samples of the process are i.i.d. F_{X_1,X_2,...,X_k}(x_1,x_2,...,x_k)=\prod_{i=1}^kF(x_i)Thus, an i.i.d. process is completely specified by a single marginal distribution (density or mass) function. Sum Process A sum process $Sn$ is a discrete-time random process obtained by taking the sum of all past values of an i.i.d. random process $X_n$. $S_n=\sum_{i=1}^nX_i$ One-dimensional random walk: Let $D_n = 2I_n - 1$.Then $S_n=2k-n$ for $0\le k\le n$$P[S_n=2k-n]={n\choose k}p^k(1-p)^{n-k}$ Sum Processes are i.s.i Independent Stationary Increment Process An increment of a random process is the difference between the values of the random process at two different points in time. A process X(t) has independent increments if for any $k \ge 3$ time points, $t_1&lt; t_2 &lt; … &lt; t_k$, the increment random variables $X(t_2) - X(t_1), X(t_3) - X(t_2),… $and $X(t_k) -X(t_{k-1})$ are independent. Only non-overlapping intervals are independent A process X(t) has stationary increments if the increments over any two intervals with the same length have the same distribution. The increments may overlap A process is said to be an independent stationary increment (i.s.i.) process if its non-overlapping increments are both independent and stationary. Any discrete-time i.s.i. process can be expressed as a sum process. $p_{S_mS_n}(j,k)=p_{S_m}(j)\times p_{S_{n-m}}(k-j)$ Poisson ProcessDivide each unit interval of the real line into $m$ equal sub intervals. At each sub interval, we toss a coin with probability of heads $p=\lambda/m$.((The average number of heads in each unit interval is $\lambda$.) If heads appears, step forward by 1. If tails appears, stay. $N_m(t)=\sum_{i=1}^{\lfloor mt\rfloor}X_i$where $X_i$ is a Bernoulli random process with parameter $p$ Denote $n=\lfloor mt\rfloor$ The underlying discrete-time process is i.s.i.$N_m(n)=\sum_{i=0}^nX_i$ For fixed $t$ and $m$, the distribution of $N_m(t)$ is binomial with parameters $n=\lfloor mt\rfloor$ and $p=\lambda/m$ The distribution of $N_m(t)$ approaches a Poisson distribution with mean $np=\lfloor mt\rfloor{\lambda\over m}\to\lambda t$ Counting Process Binomial Counting Process$X_n$ is a sequence of i.i.d. Bernoulli RV with parameter $p$. $S_n$ is the number of 1’s in the first $n$ trials. $p_{S_n}(k)={n\choose k}p^k(1-p)^{n-k}$ for $k=0,1,…,n$ Poisson Counting Process$N(t)$ is the continuous time non-negative integer valued i.s.i. process whose first order density is Poisson. $P_{N(t)}(k)= {(\lambda t)^k\over k!}e^{-\lambda t}$ for $k=1,2,…$ InterpretationThe number of events that have occurred up to time $t$, where events occur at random instants in time at an average rate of $\lambda $ per unit time. The time interval between adjacent events is exponentially distributed with parameter $\lambda$ The inter-arrival times are independent. The time to the mth event an m-Erlang random ariable Suppose exactly one event occurs in[0,t]. the time the event occurs, $X$, is uniformly distributed on [0,t] If exactly $n$ events occur in an interval, the $n$ arrival times are independent and uniformly distributed.]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>probability</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Fourier Transform]]></title>
    <url>%2F2019%2F03%2F21%2FFourier-Transform%2F</url>
    <content type="text"><![CDATA[Fourier SeriesCTFSany T-periodic functions can be expressed by many Sinusoids $\omega_0 = {2\pi\over T}$ is the fundamental frequency$f_0 = {1 \over T}$ is the fundamental frequency in ordinary frequency Synthesis EquationA weighted sum of infinite Sinusoidsx(t) =\sum_{k=-\infty}^{\infty} a_ke^{jk\omega_0t} Analysis Equation CTcalculate the coefficients of each Sinusoidsa_k = {1\over T}\int_0^Tx(t)e^{-jk\omega_0t}dtThe coefficient $a_k$ is aperiodic. Intuitionthe analysis equation is a normalized inner product that computes a projection coefficient that specifies how much $x(t)$ projecting on $e^{jk\omega_0t}$ . The Projection is The normalization process is Tricks $2Acos\omega t\to a_{\omega\over\omega_0}=A\quad a_{-\omega\over\omega_0}=A$$2Asin\omega t\to a_{\omega\over\omega_0}=-jA\quad a_{-\omega\over\omega_0}=jA$ DTFSany N-periodic signal can be expressed by N distinct harmonics. $\omega_0 = {2\pi\over N}$ is the fundamental frequency, must contain $\pi$ otherwise not periodic$f_0 = {1 \over N}$ is the fundamental frequency in ordinary frequency Synthesis Equationa weighted sum of N complex sinusoids. x[n] = \sum_{k=0}^{N-1} a_ke^{jk\omega_0n} Analysis Equation DTcalculates the FS coefficients which are the weights for the harmonics. a_k = {1 \over N}\sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}The coefficient $a_k$ is $N$ periodic ==IMPORTANT NOTE==:for N is even, $a_{N\over 2} $ is definitely real. It has no conjugates. i.e. $N=4$ The DTFS of $2cos({2\pi\over4}n)+2cos({2\pi\over2}n)$ is $a_{-1}=1,a_0=0,a_1=1,a_2=2$ Properties Fourier TransformRegarding aperiodic signals as periodic signals in the limit of period T going to infinity. CTFTRecall that $a_k = {1\over T}\int_{t\in T}x(t)e^{-jk\omega_0t}dt$ is the projection coefficient that $x(t)$ on $e^{-jk\omega_0t}$.However, when $T\to\infty\quad a_k\to0$, so we define $X_T(j\omega)=\int_{-T/2}^{T/2}x(t)e^{-j\omega t}dt$.Therefore, $X_T(j{k2\pi\over T}) = Ta_k$ . We rewrite the Synthesis Equation with $\omega_0={2\pi\over T}$ : x(t) = \sum_{k=-\infty}^{\infty}{\omega_0\over 2\pi}X_T(jk\omega_0) e^{jk\omega_0t}as $T\to\infty\quad \omega_0\to0$, we can rewrite it in integral form. Synthesis Equationconsider the signal as a superposition of complex sinusoids with density x(t) = {1\over 2\pi}\int_{-\infty}^\infty X(j\omega)e^{j\omega t}d\omega = \mathcal{F}^{-1}\{X(j\omega)\} Analysis Equationcalculate the density of complex sinusoids with different frequency X(j\omega)=\int_{-\infty}^{\infty}x(t)e^{-j\omega t}dt = \mathcal{F}\{x(t)\} Duality propertyThere is symmetry in the FT(Fourier Transform) and IFT(Inverse Fourier Transform) integrals.They are identical except for a factor $2\pi$ and a change in sign. Examples Window$x(t) = \text{window width }2T_1 \iff X(j\omega) = {2sin\omega T_1\over \omega}$ Complex Sinusoid$x(t) = e^{j\omega_0t} \iff X(j\omega) = 2\pi\delta(\omega-\omega_0)$ Periodic SignalFT integral of a FS sum equals the sum of a FT integral. X(j\omega) = \int_{-\infty}^{\infty}(\sum_{k=-\infty}^\infty a_ke^{jk\omega_0t})e^{-j\omega t}dt \\= \sum_{k=-\infty}^\infty a_k\int_{-\infty}^{\infty}e^{jk\omega_0t}e^{-j\omega t}dt \\=2\pi \sum_{k=-\infty}^\infty a_k\delta(\omega-k\omega_0)\\=2\pi a_{\omega\over\omega_0} It is discrete(including $\delta()$ ) The value is $2\pi$ the CTFS Periodic Extension FS coefficients of the periodic extension of a signal are the sample values of the signal’s FT (scaled by $1/T$) $a_k={1\over T}X(jk\omega_0)$ DTFTDTFT must be $2\pi$-periodic Synthesis Equationconsider the signal as a superposition of complex sinusoids with densityx[n] = {1\over 2\pi}\int_{2\pi} X(e^{j\omega})e^{j\omega n}d\omega = \mathcal{F}^{-1}\{X(e^{j\omega})\} Analysis Equationcalculate the density of complex sinusoids with different frequencyX(e^{j\omega})=\sum_{n=-\infty}^{\infty}x[n]e^{-j\omega n}dt = \mathcal{F}\{x[n]\} Periodic Signal$X(e^{j\omega})=2\pi\delta(\omega-\omega_0)$ for $0\le\omega&lt;2\pi$ DFTDFT is the same as DTFS, except for a scaling factor DTFS a_k = {1 \over N}\sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}x[n] = \sum_{k=0}^{N-1} a_ke^{jk\omega_0n}DFT X[k] = \sum_{n=0}^{N-1}x[n]e^{-jk\omega_0n}x[n] = {1 \over N}\sum_{k=0}^{N-1} X[k]e^{jk\omega_0n}PropertyFor CTFT For DTFT]]></content>
      <categories>
        <category>Matters All THings</category>
      </categories>
      <tags>
        <tag>math</tag>
        <tag>Fourier</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Signal and System]]></title>
    <url>%2F2019%2F03%2F19%2FSignal-and-System%2F</url>
    <content type="text"><![CDATA[Signals What is signal?A signal is some measurable quantity that varies over time and/or space. What are systems?We define system to be anything that takes input signals and produces output signals. Basic Characterization and ManipulationManipulation multiplying signal by constant adding/multiplying signals together Multiplying two straight line segments produces a quadratic curve transformation of the independent variable time shifting $x(t)\to x(t-t_0)$Minus = Left = Delay time reflection/reversal $x(t)\to x(-t)$ time scaling $x(t)\to x(at)$ Combination of shifting and time scalingShift first and scale is easier. First scale requires we also scale the shift. Characterization Even signal $x(-t)=-x(t)$Odd signal $x(-t)=x(t)$Any signal can be viewed as the sum of an even part and an odd part $x_{even}={x(t)+x(-t)\over 2}\quad x_{odd}={x(t)-x(-t)\over 2}$ infinite duration &amp; finite duration A finite duration DT signal is a sequence of N complex numbers. We can treat it as a complex N-vector. right-sided &amp; left-sidedright-sided $u(t-c)$ left-sided $u(-t-c)$ Fundamental SignalsCTComplex Exponentiale^{st} = e^{(\sigma+j\omega)t} = e^{\sigma t}e^{j\omega t}Where $e^{\sigma t}$ is purely real. $\sigma&gt;0$ amplitude grows, $\sigma&lt;0$ amplitude decays.$e^{j\omega t}$ is purely imaginary and called Complex Sinusoids Complex Sinusoids$e^{j\omega t} = cos\omega t +j sin\omega t$ Unit Function$ u(t) = 1$ when $t\ge 0$$ u(t) = 0$ when $t\lt 0$ Impulse SignalDenoted by $\delta(t)$ u(t) = \int_{-\infty}^{t} \delta(\tau)d\tau \\ \delta(t) = {du(t)\over dt} Zero everywhere else, infinite at $t=0$ Unit total area $\int_{-\infty}^{\infty} \delta(t)dt = 1$ Integral is the unit step $u(t) = \int_{-\infty}^{\infty} \delta(\tau)d\tau$ Scaled impulse k$\delta(t)$ has total area k. $\int_{-\infty}^{\infty} k\delta(t)dt = k\int_{-\infty}^{\infty}\delta(t)dt = k $ Sampling property$x(t)\delta(t-t_0) = x(t_0)\delta(t-t_0)$ a scaled delta function Sifting property $\int_{-\infty}^{\infty}x(t)\delta(t-t_0)dt = x(t_0)$ a value at t~0~ Infinite energy. Therefore $\delta(t)$ is an idealization that cannot physically exist. Time Scaling corresponds to Magnitude Scaling $\delta(\alpha t) = {\delta(t)\over \left|{\alpha}\right|}$ DTReal Exponential$ \alpha^n = e^{an} $ where $\alpha = e^a$ $\alpha&gt;1$ growing exponential$0&lt;\alpha&lt;1$ decaying exponential$-1&lt;\alpha&lt;0$ decaying oscillating$\alpha&lt;-1$ growing oscillating Real Sinusoids$cos(\omega n) = cos(2\pi fn)$ Periodic only if its ordinary frequency $f$ is rational number $f$ and $f\pm m(integer)$ are the same frequency. $\omega$ and $\omega\pm m2\pi$ are the same frequency The fastest rate at which a DT signal can oscillate is at an ordinary frequency of $f={1\over2}$ or angular frequency of $\omega = \pi$Complex Exponential $z^n$ where $z = e^s = e^{\sigma + j\omega} = e^\sigma e^{j\omega}$ $z^n = \left|z\right|^n e^{j\omega n}$$\left|z\right|^n$is real exponential $ e^{j\omega n}$ is complex sinusoid Complex Sinusoid$x[n] = e^{j\omega n} = cos \omega n + j sin \omega n$ Periodic iff $\omega = {m2\pi \over N}$ $e^{j\omega n} = e^{j(\omega +2k\pi)n}$ frequency is periodic for DT signal, and the highest angular frequency is $\pi$ Unit Step$u[n] = 1$ when $n\ge 0$$u[n] = 0$ when $n\lt 0$ Unit Impulse$\delta[0] = 1$ other place 0.Property same as CT. Relations with Unit step the first difference between two unit step signals.$\delta[n] = u[n]-u[n-1]$ unit step can be expressed as a sum of impulse by two ways $u[n]\sum_{m=-\infty} ^n \delta[m]$ $u[n]\sum_{k=0}^\infty \delta[n-k]$ the sampling property for $\delta[n]$when multiply $x[n]$ and $\delta[n]$, only one sampled value, not a continuous function anymore. Complex FrequencyCT and DT Complex Frequency $z = e^s = e^{\sigma+ j\omega}$ DT CT Growing $ z &gt;1$ $\sigma&gt;0$ Decaying $ z &lt;1$ $\sigma&lt;0$ Oscillatory frequency $\ang z$ $\omega$ SystemBasic System Characterization Memoryless: instantaneous Causality: does not depend on future Stability: BIBO(Bounded-Input Bounded Output) Invertibility: distinct inputs lead to distinct outputs Time-invariant Linearity Time InvarianceShifted input shifted output. Operation of time does not depends on time. $ y(t) = \int_{t-3}^{t+2} x(\tau)d\tau$ YES : 3 time before and 2 later$ y(t) = \int_{0}^{t} x(\tau)d\tau$ NO: has a beginning point 0 Linearitysuperposition property can be decomposed into two Additive property: $x_1(t) \to x_2(t) = y_1(t) \to y_2(t)$ Homogeneity (scaling): $ax_1(t) \to ay_1(t)$ LTI must satisfy zero-input/zero-output Elementary Linear Operation: Differentiation and integration: $x^{(k)}(t)$ ( LTI! ) Time shifting:$x(t-\tau)$ ( LTI! ) Multiply by a function: $g(t)x(t)$ (not TI unless $g(t)$ is constant) time reversal, time scaling ( not TI because there is a origin) Impulse ResponseDenoted by $h(t)$ or $h[n]$, is the output of $\delta(t)/\delta[t]$ in a LTI system. If the output of a system is given by convolving the input with a $h[n]$, then the system is LTI! Convolution is LTI Typical Examples: $h[n] = \delta[n]$ identity function. Do nothing. $h[n] = \delta[n-m]$ delay system. Delay the input by m. $h[n] = \delta[n]+0.3\delta[n-4]$ echo system. With echo volume 0.3. $h[n] = u[n]$ first sum system. $y[n] = \sum_{-\infty}^n x[k]$ $h[n] = {1\over 3} (u[n] - u[n-3]) = {1\over 3}(h[n] + h[h-1] + h[n-2])$ window smoother. $h[n] = \delta[n] -\delta[n-1]$ first difference system. LTI SystemsLinearity ensure thatIf $\delta[n-k]\to h_k[n]$ we can imply$x[n] = \sum_{k=-\infty}^{\infty} x[k]\delta[n-k]\to y[n] = \sum_{k=-\infty}^{\infty} x[k]h_k[n-k]$ Time Invariance ensure thatIn fact $h_k[n]=h[n-k]$$x[n] = \sum_{k=-\infty}^{\infty} x[k]\delta[n-k]\quad y[n] = \sum_{k=-\infty}^{\infty} x[k]h[n-k]$ For CT case$x(t) = \int_{-\infty}^{\infty}x(\tau)\delta(t-\tau)d\tau\quad y(t) = \int_{-\infty}^{\infty}x(\tau)h(t-\tau)d\tau$we use $$ to denote it.$y(t)=x(t)h(t)$ Convolution/LTI properties: commutative $x(t) \ast h(t) = h(t) \ast x(t)$ distributive $x(t) \ast\{h_1(t) + h_2(t)\} = x(t)\ast h_1(t) +x(t) \ast h_2(t)$ associative$\{x(t)\ast h_1(t)\}\ast h_2(t) = x(t)\{h_1(t)h_2(t)\}$ Charactering LTI system by Impulse Responses: Memoryless &lt;=&gt; $h[n] = 0 $ for $n\ne0$ Causality &lt;=&gt; $h[n] =0$ for $n\lt0$ Stability &lt;=&gt;impulse response is absolute integrable/summable $\int_{-\infty}^{\infty}\left|h(t)\right|dt \lt \infty$ or $\sum_{n=-\infty}^{\infty}\left|h(t)\right|dt \lt \infty$ If the impulse response of the system is a unit step function, the system is a integral/first sum system. Conceptually computingFor the DT convolution $y[n]=x[n]*h[n]=\sum_{k=-\infty}^\infty x[k]h[n-k]$ draw the original $x[k]$ and $h[k]$ with variable $k$ flip $h[k]$ to get $h[-k]$, the convolution kernel shift $h[-k]$ right by $n$ to obtain $h[n-k]$ Multiple $x[k]$ by $h[n-k]$ and sum over all results System Function and Frequency ResponseSystem Functiony[n] = \sum_{k=-\infty}^\infty h[k]x[n-k] = H(z)x[n] \\ H(z) = \sum_{k=-\infty}^{\infty} h[k]z^{-k}y(t) = \int_{\tau=-\infty}^\infty h(\tau)x(t-\tau) = H(s)x(t) \\H(s) = \int_{-\infty}^{\infty}h(\tau)e^{-s\tau}d\tau$z^n $/$ e^{sn}$ is an eigenfunction of DT/CT LTI systems. $H(z)$ or $H(s)$ is the eigenvalue that depends on the complex frequency $z$. They are also called system function of CT/DT LTI systems. $H(s)$ is a Laplace Transform of the CT impulse response $h(t)$$H(z)$ is a z-transform of the DT impulse response $h[n]$ Frequency ResponseFor LTI System, the eigenvalue at different oscillation frequency $\omega$ is called frequency response. $H(s)e^{st} = H(j\omega)e^{j\omega t}$ when $s=\sigma + j\omega$ and $\sigma=0$where $H(j\omega)$ is the cross-section of $H(s)$ along $j\omega$ axis. $H(z)z^{n} = H(e^{j\omega})e^{j\omega n}$ when $|z|= 1$ and $\ang z=\omega$where $H(e^{j\omega})$is the value of system function $H(z)$ along the unit circle $z=e^{j\omega}$ $H(j\omega)=H(s)$ given that $s=j\omega$$H(e^{j\omega})=H(z)$ given that $|z|=1;z=e^{j\omega}$]]></content>
      <categories>
        <category>Signal is Everything</category>
      </categories>
      <tags>
        <tag>Fourier</tag>
        <tag>signal</tag>
        <tag>system</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Micro Economics]]></title>
    <url>%2F2019%2F01%2F30%2FMicro-Economics%2F</url>
    <content type="text"><![CDATA[[TOC] Ten PrinciplesHow people make decisions People Face Trade-offs The Cost of Something Is What You Give Up to Get It Rational People Think at the Margin People Respond to Incentives How people interact Trade Can Make Everyone Better Off Markets Are Usually a Good Way to Organize Economic Activity Governments Can Sometimes Improve Market Outcomes How the Economy as a whole works A Country’s Standard of Living Depends on Its Ability to Produce Goods and Services Prices Rise When the Government Prints Too Much Money Society Faces a Short-Run Trade-off between Inflation and Unemployment Think like an Economist Scientist: Observation Theory and More Obeservation Assumption Model: Circular-Flow Diagram and Production Possibilities Frontier Police advisor: Positive vs Normative Statement Two hands Not accepted Disagree with each other Scientific Judgement Value Perception vs Reality Interdependence and Gains from Trade Absolute advantage the ability to produce a good using fewer inputs than another producer. Opportunity costwhatever must be given up to obtain some item. Comparative advantagethe ability to produce a good at a lower opportunity cost than another producer.One should do what he has lower comparative advantage and export them. Price of the trade is between the two opportunity costs. Market Forces of Supply and DemandDistinguish between demand and quantity demanded, supply and quantity supplied.For both curve, increase means a shift to right, decrease means a shift to left. Shifts in the Demand CurveDeterminants: Income normal good low income low demand inferior good low income high demand. Prices of Related Goods substitutes one price increase, quantity decrease. another demand increase. Both price and quantity increase. complements one price increase, another demand decrease. Both price and quantity decrease. Tastes Expectations Number of Buyers: more means more demand. Shifts in the Supply CurveDeterminants: Input PricesInput price rise, supply decrease. TechnologyIncrease supply. Expectations: Number of Sellers: more means more supply. ElasticityA measure of the responsiveness of quantity demanded or quantity supplied to a change in one of its determinants. DemandThe Price Elasticity of DemandA measure of how much the quantity demanded of a good responds to a change in the price. Determinants for Price Elasticity of Demand: Availability of Close Substitutes: goods with close substitutes are more elastic. Necessities vs Luxuries: Necessities tend to have inelastic demands. Definition of the Market: Narrowly defined markets tend to have more elastic demand than broadly defined markets because it is easier to find close substitutes. Time Horizon: Goods tend to have more elastic demand over longer time horizons. Computing $E_d=\Delta Q_d/\Delta P$ Price elasticity = Percentage change in quantity demanded / Percentage change in price Percentage change are often calculated by midpoint method. Variety In Demand Curve, flatter(horizontal) means elastic, steeper (vertical) means inelastic. Linear demand curve points with a low price and high quantity(right) are inelastic, while points with a high price and low quantity(left) are elastic. Other Demand Elasticities $E_i=\Delta Q_d/\Delta I$ The Income Elasticity of Demand = Percentage change in quantity demanded / Percentage change in income. $E_c=\Delta Q_{d,a}/\Delta P_b$ The Cross-Price Elasticity of Demand = Percentage change in quantity demanded of good A / Percentage change in the price of good B Total Revenue Total Revenue = Price x Quantity. $TR = P \times Q$ elasticity &lt; unit elasticity = 1: P up R up.$E&lt;1\quad P\uparrow R\uparrow$ elasticity &gt; unity elasticity = 1: P up R down.$E&gt;1\quad P\uparrow R\downarrow$ SupplyThe Price elasticity of supplyA measure of how much the quantity supplied of a good responds to a change in the price. Determinants for Price Elasticity of Supply: Flexibility of sellers to change the amount of good. Time period considered.(Most markets) Computing $E_s=\Delta Q_s/\Delta P$ Price elasticity of supply = Percentage change in quantity supplied / Percentage change in price Variety In Supply. Curve, flatter(horizontal) means elastic, steeper (vertical) means inelastic. Example and Application Good farming news may harm farmers: Supply shifts right, demand remain the same, elasticity greater than 1, total revenue decrease. OPEC fall to keep oil price high: Supply is elastic over long period. Curve becomes flatter and flatter. Drug interdiction may increase drug-related crime: Supply shifts left, demand remain the same, elasticity less than 1, total revenue increase. Instead, to avoid drug-related crime, we should spend resource on demand side, like drug education. Supply Demand and Government Policiesprice control price ceilinga binding price ceiling causes a shortage. Gas Pump. A long lines. Rent Control. the initial shortage caused by rent control is small. landlords use various mechanisms to ration housing. waiting list, preference, discrimination, under-the-table payments. price floora binding price floor causes a surplus. The Minimum Wage: unemployment. great impact on teenage labor. More of them tend to work instead of study. TaxWhen the government levies a tax on a good, the equilibrium quantity of the good falls. That is, a tax on a market shrinks the size of the market. Taxes levied on sellers and taxes levied on buyers are equivalent. Can Congress Distribute the Burden of a Payroll Tax? Doesn’t matter…Tax incidence depends on the forces of supply and demand A tax burden falls more heavily on the side of the market that is less elastic(inelastic). Who Pays the Luxury Tax? Producer! CS PS and Efficiency of MarketsConsumer Willingness to Paythe maximum amount that a buyer will pay for a good Consumer Surplus the amount a buyer is willing to pay for a good minus the amount the buyer actually pays for it. The area below the demand curve and above the price measures the consumer surplus in a market lower price raises CS(consumer surplus). it measures the benefit that buyers receive from a good as the buyers themselves perceive it. Producer Cost the value of everything a seller must give up to produce a good. A measure of producer’s willingness to sell. Producer Surplus the amount a seller is paid for a good minus the seller’s cost of providing it. The area below the price and above the supply curve measures the producer surplus in a market. higher price raises PS(producer surplus) Market Total SurplusCS+PS. the sum of consumer and producer surplus. Efficiencythe property of a resource allocation of maximizing the total surplus received by all members of society Equalitythe property of distributing economic prosperity uniformly among the members of society. Market in Organs Not fair Market Failurethe inability of some unregulated markets to allocate resources efficiently. The Costs of TaxationDeadweight Lossthe fall in total surplus that results from a market distortion, such as a tax. the elasticities of supply and demand determine the size of the deadweight loss from a tax.the greater the elasticities of supply and demand, the greater the deadweight loss of a tax. The Deadweight Loss Debate Some economists believe deadweight loss is small because labor supply is fairly inelastic. Some economists believe deadweight loss is large because labor supply is more elastic. adjust number of hours they work second earners elderly can choose when to retire consider engaging in underground economy When Taxes VaryDeadweight loss increase as $x^2$ Tax revenue change as $max-(x-optimal)^2$ Laffer Curve and supply-side economics Economists debates whether tax rates had in fact reached such extremelevels. Because the cut in tax rates was intended to encourage people to increase the quantity of labor they supplied, the views of Laffer and Reagan became known as supply-side economics. The Costs of Production total revenue(TR) $TR=P\times Q$the amount a firm receives for the sale of its output. Cost total cost(TC)$TC= FC+VC$the market value of the inputs a firm uses in production. explicit costsinput costs that require an outlay of money by the firm. Both economists and accountants care. implicit costsinput costs that do not require an outlay of money by the firm. Only economists care. Profittotal revenue minus total cost $TR-TC$ economic profittotal revenue minus total cost, including both explicit and implicit costs accounting profittotal revenue minus total explicit cost. usually larger than economic profit. Production and Cost production functionthe relationship between quantity of inputs used to make a good and the quantity of output produced. marginal productthe increase in output that arises from an additional unit of input diminishing marginal productthe property whereby the marginal product of an input declines as the quantity of the input increases Various Measures of Cost fixed costs(FC)costs that do not vary with the quantity of output produced variable costs(VC)costs that vary with the quantity of output produced Average Cost average total cost(ATC)$ATC=TC/Q$total cost divided by the quantity of output.usually U-shaped.decrease because of AFC, increase because of AVC. average fixed cost(AFC)$AFC=FC/Q$fixed cost divided by the quantity of output.shape like $1\over x$. average variable cost(AVC)$AVC=VC/Q$variable cost divided by the quantity of output.Usually rise as the quantity produced increases. Marginal Cost(MC)The increase in total cost(TC) that arises from an extra unit of production(Q). MC = \Delta TC/\Delta Qeventually rise as the quantity produced increases. reflect the property of diminishing marginal product Relationship with ATC Whenever marginal cost is less than average total cost, average total cost is falling. Whenever marginal cost is greater than average total cost, average total cost is rising. The marginal-cost curve crosses the average-total-cost curve at its minimum. Long Run Short Run economies of scalethe property whereby long-run average total cost (ATC) falls as the quantity of output increases diseconomies of scalethe property whereby long-run average total cost (ATC) rises as the quantity of output increases constant returns to scalethe property whereby long-run average total cost (ATC) stays the same as the quantity of output changes Firms in Competitive MarketsCompetition competitive market There are many buyers and many sellers in the market. The goods offered by the various sellers are largely the same. Firms can freely enter or exit the market.(perfectly competitive markets) Average Revenue(AR)$AR=TR/Q$total revenue divided by the quantity sold. For all types of firms, average revenue equals the price of the good marginal revenue(MR)$MR=\Delta TR/\Delta Q$the change in total revenue from an additional unit sold. For competitive firms, marginal revenue equals the price of the good. Profit Maximizationsupply decision If marginal revenue is greater than marginal cost, the firm should increase itsoutput. If marginal cost is greater than marginal revenue, the firm should decrease itsoutput. At the profit-maximizing level of output, marginal revenue and marginal costare exactly equal. In essence, because the firm’s marginal-cost curve determines the quantity of the good the firm is willing to supply at any price, the marginal-cost curve is also the competitive firm’s supply curve,precisely, the portion lies above AVC. short run shut downtemporary shut down if TR]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>Economics</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Shortest Paths and Flow]]></title>
    <url>%2F2018%2F12%2F15%2FShortest-Paths-and-Flow%2F</url>
    <content type="text"><![CDATA[Algorithm Comments Graph Rep Running time Space Used Bellman-Ford Single-Source Adj List 𝑂(𝑉𝐸) 𝑂(𝑉) In DAG Out DAG Single-Source DAG Adj List 𝑂(𝑉+𝐸) 𝑂(𝑉) Dijkstra Single-SourceNon-Neg Weights Adj List 𝑂((𝑉 + 𝐸)log𝑉) 𝑂(𝑉) All-pairs 1 All-Pairs Adj Matrix 𝑂(𝑉^4^) 𝑂(𝑉^2^) All-pairs 2 All-Pairs Adj Matrix 𝑂(𝑉^3^log𝑉) 𝑂(𝑉^2^) Floyd-Warshall All-Pairs Adj Matrix 𝑂(𝑉^3^) 𝑂(𝑉^2^) Shortest Paths All algorithms, except Dijkstra, allow negative weights,but there can NOT be negative cycle. Shortest path problem: Find the shortest path from s to t. Single-source shortest path: Find the shortest path from s to every node. Lemma (Cut and Paste Argument): Let 𝑃 = (𝑠, … , 𝑢, … , 𝑡) be the shortest 𝑠 − 𝑡 path. The n the subpaths 𝑃~1~ = (𝑠, … , 𝑢) and 𝑃2= (𝑢, . . 𝑡) must also be, respectively, shortest 𝑠 − 𝑢 and 𝑢 − 𝑡 paths. Relaxation: Let v.d be the shortest distance found so far from the starting node s to node v, and v.p be the last node in the current shortest path from s to node v. Single Source Shortest PathLet ss(s,v) store shortest path distance from s to v Bellman-Ford AlgorithmInitially, set n.d = inf for all nodes. Except starting node s.d = 0Relax all edges once … path length maximum 1Relax all edges second time … path length maximum 2 A path may have at most V-1 edges. So we it V-1 times. Finally, n.d is the actual shortest distance from s to n Basic Implementation1234567Shortest-Path(G,s):for each node n in V do n.d = infs.d = 0for i = 1 to V-1 for each edge (u,n) in E Relax(u,n) Running time: Θ(𝑉𝐸)Space: Θ(𝑉) Efficient ImplementationTake advantage of dynamic programming. Recurrence:Suppose (u,n) is the last edge of shortest path of length at most i from s to n. By cut and paste argument, the subpath from s to u must be a shortest path of length at most i - 1, followed by (u,n).So, for all i &gt; 0 and n != s n.d[i] = min{ u.d[i-1] + w(u,n) } u in V, (u,n) in E Final solution: n.d[m-1] 12345678910Bellman-Ford(G,s):for each node n in V n.d = inf, n.p = nils.d = 0for i = 1 to V-1 for each node u in V if u.d changed in previous iteration then for each un in Adj(u) Relax(u,un) if no un.d changed in this iteration =&gt; terminate Running time: Θ(𝑉𝐸)Space: Θ(𝑉) Shortest Paths in a DAGBy subpath optimality, we have ss(s,n) = min { ss(s,u) + w(u,n) } u in V, (u,v) in Es Unlike Bellman-Ford, each edge will only be relaxed once. We need to ensure that when n is processed, ALL u with (u,n) in E have already been processed. We can do that by processing n (also ss(s,n)) in the topological order of the nodes. 123456789DAG-Shortest-Path(G,s)topologically sort the vertices of Gfor each vertex n in V n.d = inf n.p = nils.d = 0for each vertex u in topological order for each vertex un in Adj(u) Relax(u,un) Running time: Θ(𝑉+𝐸) Dijkstra AlgorithmNOT allow negative weights. Maintain a set of explored nodes S for which we know u.d = ss(s,u)by variable status Initialize S = {s}, s.d = 0, v.d = inf Use a Min priority queueQ on V with d as key Key Lemma If all edges leaving S are relaxed, then v.d = ss(s,v),where v is the vertex in V-S with the minimum v.d So this v can be added to S, then repeat pseudocode123456789101112Dijkstra(G,s):for each node n in V do n.d = inf, n.p = nil, n.status = unknowns.d = 0create a min priority queue Q onV with d as keywhile Q not empty % E times u = Extract-Min(Q) % O(logV) time u.status = over for each un in Adj(u) do if un.status = unknown then Relax(u,un) Decrease-Key(Q,un,un.d) Running time: 𝑂(𝐸log𝑉)Very similar to Prim’s algorithm All-Pairs Shortest PathsInput: Directed graph G = (V,E) Weight w(e) = length of edge e Output: ss(u,v), for all pairs of nodes u, v A data structure from which the shortest path from 𝑢 to 𝑣 can be extracted efficiently, for any pair of nodes 𝑢, 𝑣 Note: Storing all shortest paths explicitly for all pairs requires 𝑂(𝑉^3^) space. Graph Representation: Assume adjacency matrix: 𝑤(𝑢, 𝑣) can be extracted in 𝑂(1) time. 𝑤(𝑢, 𝑢)= 0, 𝑤(𝑢, 𝑣)= ∞if there is no edge from 𝑢 to 𝑣. If the graph is stored in adjacency lists format, can convert to adjacency matrix in 𝑂(𝑉2) time. Using Previous AlgorithmsNOT negative cost edges: Dijkstra’s algorithmRunning time: 𝑂(𝐸 log 𝑉), totally 𝑂(𝑉𝐸 log 𝑉)Space: 𝑂 (𝑛^3^ log 𝑛 ) for dense graphs HAVE negative cost edges: Bellman-FordRunning time: 𝑂(𝑉𝐸), totally 𝑂(𝑉^2^𝐸)Space: 𝑂 (𝑛^4^) for dense graphs First DP FormulationDefine d(i,j,m) = length of the shortest path from i to j that contains at most m edges.Use D[m] to denote the matrix [d(i,j,m)] Recurrence( essentially the same as in Bellman-Ford): d(i,j,m) = min { d(i,k,m-1) + w(k,j) } k from 1 to ninitially d(i,j,1) = w(i,j) pseudocode1234567891011Slow-All-Pairs-Shortest-Paths(G):d(i,j,1) = w(i,j) for all 1&lt;=i,j&lt;=nfor m = 2 to n-1 let D[m] be a new n*n matrix for i = 1 to n for j = 1 to n d(i,j,m) = inf for l =1 to n if d(i,k,m-1) + w(k,j) &lt; d(i,j,m) then d(i,j,m) = d(i,k,m-1) + w(k,j)return D[n-1] Running time: 𝑂(𝑛^4^)Space: 𝑂(𝑛^3^) can be improved to 𝑂(𝑛^2^) Second DP FormulationObservationTo compute d(i,j,m), instead of looking at the last stop before j, we look at the middle point. This cuts down the problem size by half. Thus, to calculate D[1],D[2],D[4],D[8],… Note that overshooting D[n-1] is OK. Since D[n&#39;] , n&#39;&gt;n -1 has the shortest paths with up to n&#39; edges. It will not miss any shortest path with up to n-1 edges. Recurrence d(i,j,2s) = min { d(i,k,s) + d(k,j,s) } k from 1 to ninitially d(i,j,1) = w(i,j) AnalyzeRunning time: 𝑂(𝑛^3^) for each matrix , totally 𝑂(𝑛^3^log𝑛) Floyd-WarshallDefined(i,j,k) = length of the shortest path from i to j that all intermediate vertices on the path (if any) are in the set {1,2,...,k} ObservationWhen computing d(i,j,k) there are two cases: k is not a node on the shortest path from i to j=&gt; then the path uses only vertices in {1,2,...,k-1} k is an intermediate node on the shortest path from i to j=&gt; path can be spilt into shortest subpath from i to k and a subpath from k to jBoth subpaths use only vertices in {1,2,...,k-1} Recurrence pseudocode1234567891011Floyd-Warshall(G):d(i,j,0) = w(i,j) for all 1&lt;=i,j&lt;=nfor k=1 to n let D[k] be a new n*n matrix for i = 1 to n for j = 1 to n if d(i,k,k-1) + d(k,j,k-1) &lt; d(i,j,k-1) then d(i,j,k) = d(i,k,k-1) + d(k,j,k-1) else d(i,j,k) = d(i,j,k-1)return D[n] Running time: 𝑂(𝑛^3^)Space: 𝑂(𝑛^3^) but can be improved to 𝑂(𝑛^2^)Surprising discovery: If we just drop all third dimension. i.e. the algorithm just uses n*n array D, the algorithm still works! Maximum FlowInput: A directed connected graph 𝐺 =(𝑉, 𝐸) , where every edge 𝑒 ∈ 𝐸 has a capacity 𝑐(𝑒); a source vertex 𝑠 and a target vertex 𝑡. Output: A flow 𝑓: 𝐸 → 𝐑 from 𝑠 to 𝑡, such that For each 𝑒 ∈ 𝐸, 0 ≤ 𝑓(𝑒) ≤ 𝑐(𝑒) (capacity) For each 𝒗 ∈ 𝑽 − {𝒔, 𝒕}, sumOut( 𝒇(𝒆) ) = sumInto( 𝒇(𝒆) )(conservation)、 Define:The value of a flow is |𝑓| = sum(𝑓(𝑠, 𝑣))= sum(𝑓(𝑣, 𝑡)) where 𝑣 in V s-t Cut Residual Graph Ford Fulkerson Algorithm Start with f(e) = 0 for all edges e in E Construct Residual Graph G~f~ for current flow f(e) = 0 while there exists some s-t path P in G~f~ Let capacity of flow cf(p) = min { cf(e): e in P}This is the maximum amount of flow that can be pushed through residual capacity of P‘s edges Push c(f,p) units of flow along the edges e in P by adding cf(p) to f(e) for every e in P Construct Residual Graph G~f~ for new current flow f(e) When algorithm gets stuck, current flow is maximal! 12345678910111213141516Ford-Fulkerson(G,s,t):for each (u,n) in E do f(u,n) = 0 cf(u,n) = c(e) cf(n,u) = 0while there exists path P in residual graph Gf do cf(p) = min &#123;cf(e):e in P&#125; for each edge (u,n) in P do if (u,n) in E then f(u,n) = f(u,n) + cf(p) cf(u,n) = cf(u,n) - cf(p) cf(n,u) = cf(n,u) + cf(p) else f(n,u) = f(n,u) - cf(p) cf(n,u) = cf(n,u) + cf(p) cf(u,n) = cf(u,n) - cf(p) Construct Residual Graph G~f~ capacity of flow in 8, write back to G The G~f~ in next iteration is Until there is no s-t path in G~f~. Current flow is optimally maximal. Applicationshe Max Flow setup can model (surprisingly) many (seemingly) unrelated problems. The idea is to express the problem as a max flow and then feed individual instances into out max flow solver. The examples below all share the property that they are integer flow problems, e.g., al capacities are integral, so running-time analyses can use FF bound for integral flows. Edge-Disjoint Paths Define: Two paths are edge-disjoint if they have no edge in common. Circulations with Demands Given a number of source vertices 𝑠1, 𝑠2, …, each with a supply of 𝑠𝑢𝑝(𝑠𝑖)and a number of target vertices 𝑡1, 𝑡2, …, each with a demand of 𝑑𝑒𝑚 𝑡𝑖 ;sum of supply &gt;= sum of demand Need a flow meets all demands. Solution:Add a super source and a super target Maximum Bipartite Matching A Matching is a subset M ⊆ E such that:∀v ∈ V at most one edge in M is incident upon v. The Size |M| is the number of edges in M. A Maximum Matching is matching M such that:every other matching Mʹ satisfies |Mʹ | ≤ M. Given bipartite graph G, find a Maximum Matching. Formulation: Create directed graph 𝐺′ = (𝑋 ∪ 𝑌 ∪ {𝑠, 𝑡}, 𝐸′ ). Direct all edges from 𝑋 to 𝑌, and assign them capacity 1. Add source 𝑠, and unit capacity edges from 𝑠 to each node in 𝑋. Add target 𝑡, and unit capacity edges from each node in 𝑌 to 𝑡. Theorem: Max cardinality matching in 𝐺 = value of max flow in 𝐺′. Running time: 𝑂(𝑉𝐸) Baseball Elimination Input： 𝑛 teams: 1, 2, … , 𝑛 One particular team, say 𝑛 (without loss of generality) Team 𝑖 has won 𝑤~𝑖~ games already Team 𝑖 and 𝑗 still need to play 𝑟~𝑖𝑗~ games, 𝑟~𝑖𝑗~ = 0 or 1. Team 𝑖 has a total of 𝑟𝑖 = sum(𝑟~𝑖𝑗~: 𝑗) games to play Output: “Yes”, if there is an outcome for each remaining game such that team 𝑛 finishes with the most wins (tie is OK). “No”, if no such possibilities. Claim: There is a way for team n to finish in the first place iff the max flow has value of the sum of supply from source s]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Minimum Spanning Tree]]></title>
    <url>%2F2018%2F12%2F15%2FMinimum-Spanning-Tree%2F</url>
    <content type="text"><![CDATA[Uniqueness of MSTCut LemmaLet S be any subset of nodeslet e be the min cost edge with exactly one endpoint in S.Then every MST must contain e. ProofLet T* be some MSTConsider any edge e in T*Removing e from T* breaks T* into two parts Sand V-Se must be the min cost edge crossing the cut (S,S-V)Applying the cut lemma on S, every MST must contain eApply the above arguments to every edge in T*, we have_Every edge e in T* must be contained in every MST_ Prim’s Algorithm Idea Initialize explored set S = { any one node } Add min cost edge e = (u,n) with u in S and n in V-S to T Add n to S Repeat until S = V FeatureMaintain set of explored nodes S A Min priority queue Q to keep unknown nodes.key is their cheapest edge to node inS pseudocode12345678910111213Prim(G,r):for each n in V do n.key = inf, n.p = nil, n.status = unknownr.key = 0create a min priority queue Q on Vwhile Q not empty u = Extract-Min(Q) % need O(logV) time u.status = over for each n in Adj(u) do if n.status = unknown and w(u,n)&lt;n.key then n.p = u n.key = w(u,n) Decrease-Key(Q,n,w(u,n)) Running time: 𝑂(𝐸log𝑉) Kruskal’s Algorithm Idea Start with an empty tree T Consider edges in ascending order of weight. Case 1: If adding e to T create a cycle, discard eCase 2: Otherwise, insert e = (u,v) into T according to cut Lemma Union-Find Data StructureKey QuestionHow to check whether adding e to T will create a cycle? DFS take 𝑂(𝐸⋅𝑉) time in total.Can we do better in 𝑂(log𝑉) time? After an edge e is added, two sets union together Need such a “union-find”data structure: Find-Set(u): For a given node u, find which set this node belongs to. Union(u,v): For two given nodes u and v, merge the two sets containing u and v together. Set as A Tree The trees in the union-find data structure are NOT the same as the partial MST trees! The root of the tree is the representative node of all nodes in that tree(i.e., use the root’s ID as the unique ID of the set). Every node (except the root), has a pointer pointing to its parent. The root has a parent pointer to itself. No child pointers (unlike BST), so a node can have many children. 12Make-Set(x):x.parent = x, x.hight = 0 123456Find-Set(x):x.height = 0while x!= x.parent do x.height = x.height + 1 x= x.parentreturn x 12345678Union(x,y):a = Find-Set(x)b = Find-Set(y)if a.height &lt;= b.height then if a.height = b.height then b.height = b.height + 1 a.parent = belse b.parent = a Path Compressionwhile Find-Set(x) we can update its parent pointer to compress the path. pseudocode12345678MST-Kruskal(G):for each node n in V Make-Set(n)sort the edges of G into increasing order by weight % O(ElogE)for each edge (u,n) in E taken in the above order if Find-Set(u) != Find-Set(v) then % O(E) output edge (u,n) Union(u,n) Running time: 𝑂(𝐸log𝐸+𝐸log𝑉)=𝑂(𝐸log𝑉)Note: If edges are already sorted(𝑂(𝐸log𝐸)) and we use path compression, then the running time is close to 𝑂(𝐸)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Graph Basic and Algorithm]]></title>
    <url>%2F2018%2F12%2F15%2FGraph-Basic-and-Algorithm%2F</url>
    <content type="text"><![CDATA[Basicnode: vertex edge: connection between nodes Undirected Edges have no direction (or both directions) deg (𝑣) = # edges at 𝑣 sum of deg(𝑣) = 2𝐸 Directed Edges have directions If an edge has both directions, we will use two edges with opposite directions deg~out~ (𝑣) = # edge leaving 𝑣;deg~in~ (𝑣 ) = # edge entering 𝑣. sum of deg𝑜𝑢𝑡(𝑣) = sum of deg𝑖𝑛(𝑣) = 𝐸 Path A path in a (directed or undirected) graph 𝐺 = (𝑉, 𝐸) is a sequence 𝑃 of nodes 𝑣1, 𝑣2, … , 𝑣𝑘−1, 𝑣𝑘 such that (𝑣𝑖, 𝑣𝑖+1) is an edge. The length of the path is 𝑘 − 1 (i.e., # edges in the path). A path is simple if all nodes are distinct. Connectivity An undirected graph is connected if for every pair of nodes 𝑢 and 𝑣, there is a path between 𝑢 and 𝑣. Theorem: For a connected graph, 𝐸 ≥ 𝑉 − 1. Cycle A cycle is a path v~1~, v~2~, … , v~k-1~, v~k~ in which v~1~ = v~k~ , k &gt; 2 A cycle is simple if the first 𝑘 − 1 nodes are all distinct. Data structureAdjacency Nodes Adjacency List A node-indexed array of lists. Given node 𝑢, retrieving all neighbors in Θ(deg(𝑢)) time Given 𝑢, 𝑣, checking if (𝑢, 𝑣) is an edge takes Θ(deg(𝑢)) time. Space: Θ(𝑉 + 𝐸). Adjacency Matrix A 𝑉 × 𝑉 matrix. Given node 𝑢, retrieving all neighbors in Θ (𝑉) time Given 𝑢, 𝑣, checking if (𝑢, 𝑣) is an edge takes 𝑂(1) time. Space: Θ(𝑉2). Treesconnected &amp;&amp; no cycle =&gt; treeno cycle =&gt; forest(several trees) After we have run BFS or DFS on an undirected graph, the edges can be classified into 3 types: Tree edges:traversed by the BFS/DFS Back edges:connecting a node with one of its ancestors(other than its parent) Cross edges:connecting two nodes with no ancestor/descendent relationship. AlgorithmBFS span a tree with NO back edges pseudo-code1234567891011initialize an empty queue QEnqueue(Q,r)while Q not empty do n = Dequeue(Q) for each v in Adj(n) if v.status = unknown v.status = processing v.d = n.d + 1 v.p = n Enqueue(Q,v) n.status = over Running time: Θ(𝑉+𝐸), which is Θ𝐸 if the graph is connected ApplicationFind connected components. DFS span a tree with NO cross edges pseudo-code123456789101112DFS(G)for each vertex n in V do if n.status = unknown then DFS-Visit(n)DFS-Visit(n):n.status = processingfor each v in Adj(n) do if v.status = unknown the v.p = n DFS-Vist(v)n.status = over Running time: Θ(𝑉+𝐸) ApplicationCycle detection Topological Sort A topological ordering of a graph is a linear ordering of the vertices of a DAG(Directed Acyclic Graph) such that if (u,v) is in the graph, u appears before v in the linear ordering. idea Output a vertex u with in-degree zero in current graph Remove u and all edges (u,v) from current graph If the graph is not empty, go to step 1 pseudo-code1234567891011121314Initialize Q to be an empty queuefor each u in V do If inDegree(u) = 0 then % find all starting vertices Enqueue(Q,u)while Q not empty u = Dequeue(Q); Output u for each v in Adj(u) % remove u&apos;s outgoing edges inDegree(v) = inDegree(v) - 1 if inDegree(v) = 0 then Enqueque(Q,v)return Running time: 𝑂(𝑉+𝐸)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>algorithm</tag>
        <tag>graph</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dynamic Programming]]></title>
    <url>%2F2018%2F12%2F15%2FDynamic-Programming%2F</url>
    <content type="text"><![CDATA[Main idea of DP: Analyze the structure of an optimal solution Recursively define the value of an optimal solution Compute the value of an optimal solution (usually bottom-up) @[TOC] One-DimensionStairs Climbing 1D You can climb 1 or 2 stairs with one step.How many different ways can you climb n stairs? Recurrence F(n) = F(n-1) + F(n-2) Base case F(1) = 1, F(2) = 2 pseudo-code1234567F(n):allocate an array A of size nA[1] = 1A[2] = 2for i=3 to n A[i] = A[i-1] + A[i-2]return A[n] AnalyzeRunning time: Θ(𝑛)Space: Θ(𝑛) but can be improved to Θ(1) by freeing array. Rod Cutting Problem Given a rod of length n and prices p~i~ for i = 1,…,n, where p~i~ is the price of a rod of length i . Find a way to cut the rod to maximize total revenue. length i 1 2 3 4 5 6 7 8 9 10 price p~i~ 1 5 8 9 10 17 17 20 24 30 Recurrencetotal optimal revenue is price of cut rod and optimal revenue of remaining rood. r~n~ = max{p~n~, p~1~ + r~n-1~, p~2~ + r~n-2~, …, p~n-1~+r~1~} Base caseDefine r~n~ be the maximum revenue obtainable from cutting a rod of length n. r~1~ = p~1~ pseudo-code123456789101112131415cutRod(n):let r[0...n] and s[0...n] be new arraysr[0] = 0 for j=1 to n % every optimal length q = -inf for i =1 to j % every cut length if q &lt; p[i] + r[j-i] then q = p[i]+r[j-i] % keep track the optimal cut length s[j] = i r[j] = qj = n while j&gt;0 do print s[j] j = j-s[j] i 0 1 2 3 4 5 6 7 8 9 10 p[i] 0 1 5 8 9 10 17 17 20 24 30 r[i] 0 1 5 8 9 13 17 18 22 25 30 s[i] 0 1 2 3 2 2 6 1 2 3 10 AnalysizeRunning time: Θ(𝑛^2^)Space: Θ(𝑛) Weighted Interval Scheduling 1D Jobs j starts at s~j~, finish at f~j~ and has weight(value) v~j~.Two jobs compatible if they don’t overlap.Goal: find maximum-weight subset if mutually compatible jobs. RecurrenceFirstly, label all jobs by finishing time. Maximum subset is eitherDO take this job and maximum subset of compatible jobs setNOT take this job and maximum subset of remaining jobs V[j] = max{v~j~ + V[c(j)], V[j-1]} function c(j) return the largest index i&lt;j such that job i is compatible job j. Base caseThe goal is to find a subset of a set. We start from a empty set. pseudo-code12345678910111213141516schedule():sort all jobs by finish timeV[0] = 0for i = 1 to n % DO take job j if V[i] + V[c(i)] &gt; V[j-1] then V[i] = value[i] + V[c(i)] keep[i] = 1 % NOT take job j else V[i] = V[i-1] keep[i] = 0 i = nwhile i &gt; 0 do if keep[i] =1 then print i, i = c(i) else i = i-1 AnalyzeRunning time: Θ(𝑛log𝑛)Space: Θ(𝑛) Two-DimensionSometimes sub-problem also need to use 1D-DP to solve.Sometimes there are two variables that requires a 2D array. The 0/1 Knapsack Problem A set of n items, where item i has weight w~i~ and value v~i~ ,and a knapsack with capacity W.Find x~1~,… ,x~n~ (either 0 or 1) such thatsum(x~i~w~i~) &lt;= W and V=sum(x~i~v~i~) is maximum RecurrenceStart to put items into the knapsack.—1D Maximum V of capacity W is the max among whether take each item.v~i~ + maximum XV of capacity j-w~i~We found it necessary to consider capacity of knapsack.**—2D** V[j] = max{v(i) + V[j-w(i)]} i from 1 to n WRONG: This may pick the same item more than once!Thus, both in v~i~ and V[j-w(i)]! New def: let V[i,j] be the largest obtained value with capacity j, choosing ONLY from the first i items.Below formula truly reflect whether. Left is NOT, right is YES. V[i,j] = max{V[i-1,j], v~i~ + V[i-1,j-w(i)]} i from i to n Base case We start from take nothing: i= 0, V[0,j] = 0and empty knapsack j =0, V[i,0] = 0 pseudo-code1234567891011121314let V[0...n,0...W] and keep[0...n,0...W] be new arrays of all 0 for i = 1 to n do % put things for j = 1 to W do % each capacity if w[i]&lt;=j and v[i]+V[i-1,j-w[i]] &gt; V[i-1,j] then V[i,j] = v[i]+V[i-1,j] keep[i,j] = 1 else V[i,j] = V[i-1,j] keelp[i,j] = 0K = Wfor i = n downto 1 do if keep[i,K] = 1 then print i K = K-w[i] input: i 1 2 3 4 v~i~ 10 40 30 50 w~i~ 5 4 6 3 running: V[i,j] 0 1 2 3 4 5 6 7 8 9 10 i=0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 10 10 10 10 10 10 2 0 0 0 0 40 40 40 40 40 40 40 3 0 0 0 0 40 40 40 40 40 50 70 4 0 0 0 50 50 50 50 90 90 90 90 AnalyzeRunning time: Θ(𝑛𝑊)Space: Θ(𝑛𝑊)without keep array can be improved to Θ(𝑛+𝑊). Longest Common Subsequence LCS Given two sequences X = (x~1~,x~2~,…,x~m~) and Y = (y~1~,y~2~,…,y~n~), we say that Z is a common subsequence of X and Y if Z has a strictly increasing sequence of indices i and j of both X and Y such that we have x~ip~= y~jp~ = z~p~ for all p = 1, 2, … , k. The goal is to find the longest common subsequence of X and Y. RecurrenceLet c[i,j] be the length of the longest common subsequence of X[1…i] and Y[1…j].So we need 2D that i for X and j for Y. Firstly, go through i, and then go through jIf X[i] = Y[j], we match.c[i,j] = 1 + the longest common subsequence of X[1…i-1] and Y[1…j-1].else , it’s max ofthe longest common subsequence of X[1…i-1] and Y[1…j] andthe longest common subsequence of X[1…i] and Y[1…j-1] If X[i] = Y[j] , c[i,j] = max{ c[i,i-1], c[i-1,j] } else c[i,j]= c[i-1,j-1] +1 Base case c[0,j] =0c[i,0] = 0 pseudo-code1234567891011121314151617181920212223LSC(X,Y):let c[0...m,0...n] and b[0...m,0...n] be new arrys of all 0for i=1 to m for j=1 to n if X(i) = Y(i) then c[i,j] = c[i-1,j-1]+1 b[i,j] = &quot;↖&quot; else if c[i-1,j] &gt;= c[i,j-1] then c[i,j] = c[i-1,j] b[i,j] = &quot;↑&quot; else c[i,j] = c[i,j-1] b[i,j] = &quot;←&quot;Print-LCS(b,m,n)Print-LCS(b,i,j): if i=0 or j=0 then return if b[i,j]=&quot;↖&quot; then Print-LCS (b,i−1,j−1) print X(i) else if 𝑏[𝑖,𝑗]=&quot;↑&quot; Print-LCS( b,i−1,j) else Print-LCS( b,i,j−1) AnalyzeRunning time: Θ(𝑚𝑛)Space: Θ(𝑚𝑛)without b array can be improved to Θ(𝑚+𝑛). Longest Common Substring Given two strings X = x~1~x~2~…x~m~ and Y = y~1~y~2~…y~m~, we wish to find their longest common substring Z, that is, the largest k for which there are indices 𝑖 and 𝑗 withx~i~x~i+1~…x~i+k-1~ = y~j~y~j+1~…y~j-k-1~ RecurrenceLet d[i,j] keep track of k, the longest string length of X[1…i] Y[1…j].Firstly go through X and then Y so we need 2D If X(i) = Y(j), d[i,j] = 1 + the longest string length of X[1…i-1] Y[1…j-1].else it’s 0! If X(i) = Y(j), d[i,j] = 1 + d[i-1.j-1]else d[i,j] = 0 Base case d[0,j] = 0, d[i,0]= 0 pseudo-code1234567891011let d[0...m,0...n] be a new array of all 0length = 0, endIndex = 0for i = 1 to m for j = 1 to n if X(i) = Y(i) then d[i,j] = d[i-1,j-1] + 1 if d[i,j] &gt; length then length = d[i,j] endIndex = ifor i = endIndex - length + to endIndex print X(i) AnalyzeRunning time: Θ(𝑚𝑛)Space: Θ(𝑚𝑛) but can be improved to Θ(𝑚+𝑛). Over Intervals Goal is to find optimal (min or max ) solution on problem with Problem of size n Ordered input of items 1,2,…n Define substructures as Ordered input of items i..j Problem of size j-i+1 Recurrence gives optimal solution of subproblem as function of optimal solution of smaller subproblems Algorithm fills in DP table from smallest to largest problem size Often, final subproblem filled is solution for original problemSometimes, solution of original problem is min/max over table values Longest Palindromic SubstringA palindrome is a string that reads the same backward or forward. Given a string X = x~1~x~2~…x~n~, find the longest palindromic substring. Ex:X =ACCABAPalindromic substrings: CC, ACCA ABALongest palindromic substrings: ACCA RecurrenceLet p[i,j] be true if and only if X[i…j] is a palindrome.Obviously, we need 2D, though i &lt;= j. It’s like a half plane. If X[i] = X[j], p[i,j] is true iff p[i+1,j-1] is true. If X(i) = X(j), p[i,j] = p[i+1,j-1] Order: from (i,j) to (i+1,j-1) it’s a diagonal path. Base If i = j, p[i,j] = true pseudo-code12345678910111213141516let p[0...n,0...n] be a new array of all falsemax = 1 for i = 1 to n-1 do p[i,i] = true if X(i) = X(i+1) then p[i,i+1] = true max = 2% updating along diagonal% started from the third diagonalfor k = 3 to n do for i = 1 to n-k+1 do j = i+k-1 if p[i+1,j-1] = true and X(i) = X(j) then p[i,j] = true max = kreturn max AnalyzeRunning time: 𝑂(𝑛^2^)Space: 𝑂(𝑛^2^) but can be improved to 𝑂(𝑛) Optimal BST Given n keys a~1~ &lt; a~2~ &lt; … &lt; a~n~, with weights f(a~1~), … , f(a~n~), find a binary search tree T on these n keys such thatB(T) = sum{ f(a~i~)*(d(a~i~)+1) } i from 1 to nis minimized, where d(a~i~) is the depth of a~i~. RecurrenceLet T~i,j~ be some tree on the subset of nodes a~i~ &lt; a~i+1~ &lt; … &lt; a~j~Define w[i,j] = f(a~i~) + … + f(a~j~)The cost is defined as B(T~i,j~) = sum{ f(a~i~)*(d(a~i~)+1) } from i to jDefine e[i,j] = optimal value of B(T~i,j~) The optimal cost of T~i,j~ isThe optimal cost of left subtree + The optimal cost of right subtree + root’s weight e[i,j] = e[i,k-1 + e[k+1,j] +w[i,j] To find k, try every value between i and j to figure out the min. e[i,j] = min{ e[i,k-1 + e[k+1,j] +w[i,j] } for i&lt;=k&lt;=j Order: (i,j) (i,k-1) (k+1,j) Base e[i,j]= 0 for i&gt;je[i,i] = f(a~i~) for all i pseudo-code1234567891011121314151617181920212223242526Optimal-BST(a,n):let e[1...n,1...n],w[1...n,1...n],root[1...n,1...n] be new arrays of all 0for i = 1 to n w[i,i] = f(a(i)) for j = i + 1 to n % complete the w[] table w[i,j] = w[i,j-1] + f(a(i))for l = 1 to n for i = 1 to n-l+1 j = i+l-1 e[i,j] = inf % find k minimizes e[] for k = i to j t = e[i,k-1]+e[k+1,j]+w[i,j] if t &lt; e[i,j] then e[i,j] = t root[i,j] = kreturn Construct-BST(root,1,n)Construct-BST(root,i,j):if i &gt; j then return nilcreate a node zz.key = a[root[i,j]]z.left = Construct-BST(root,i,root[i,j]-1)z.right = COnstruct-BST(root,root[i,j]+1,j)return z AnalyzeRunning time: Optimal 𝑂(𝑛^3^) Construct 𝑂(𝑛^2^)Space 𝑂(𝑛^2^)]]></content>
      <categories>
        <category>Algorithm and Graph</category>
      </categories>
      <tags>
        <tag>algorithm</tag>
        <tag>DP</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Finite State Machine]]></title>
    <url>%2F2018%2F12%2F08%2FFinite-State-Machine%2F</url>
    <content type="text"><![CDATA[State Machine State: A set of particular condition that the machine is in at a specific time FSM is a generic synchronous sequential circuit FSM has finite state memory, inputs &amp; outputs State memory is implemented using flip-flops State transition logic implemented using combinational circuit Output logic implemented using combinational circuit Output depends on state only - Moore Output depends on state and input - Mealy State equationsDescribe the behavior of a clocked sequential circuit algebraically.e.g. A(t+1) = D~A~(t) = A(t)x(t) + B(t)x(t)Simple form:A(t+1) = Ax + Bx or A^+^ = Ax + Bx State Diagram and Transition Tables The diagram is a Mealy Machine Moore Machine vs. Mealy MachineMoore MachineThe output value is inside the state bubbles.Outputs are function solely of the current states.Outputs change synchronously with states. Mealy MachineThe output value is on the transition edge.Outputs depend on state AND inputsChange of inputs causes an immediate change of outputs. Example Basic Design Steps of FSM Understand the Specificationswith a block diagram Obtain an abstract specification of the FSMin state-transition diagram or table Perform state reductionequivalent states can be merged Perform state assignmentassign binary values to the state in a way that next-state logic can be simplified Choose FF to implement the FSM state register D-FF : Q+ = D T-FF : Q^+^ = TQ’ + T’QT = 0 hold ;T = 1 toggle; JK-FF: Q^+^ = JQ’ + K’Q| JK | Data | function || :—: | :—: | :———: || 0/0 | 1 | hold || 0/1 | 0 | reset || 1/0 | Q | set || 1/1 | Q’ | toggle | Implement the FSMdesign of next-state and output logic Happy DEBUGGING!! FSM Implementation Procedure Start with a state diagram (bubble diagram) Describes the Finite State Machine according to the specification Reduce the number of states if necessary/possible(state reduction) equivalent-states Two internal states are said to be equivalentif for each input combination they give exactly the same output AND send the circuit to the same(equivalent) state. Decide on the State Encoding (how many flip-flops to use and what should be the FF outputs be for each state) Produce the binary-code state table. Decode what kind of FFs to use. D-type FFs are normally used (JK or T are more complicated and may give you smaller circuit if you play with TTLs) Determine the FF input equations (use FF excitation rules for JK and T type FFs) and general output equations from the transition tables Implement the next state logic, output logic using combinational circuit techniques Draw the complete logic diagram]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>FSM</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[VHDL]]></title>
    <url>%2F2018%2F12%2F08%2FVHDL%2F</url>
    <content type="text"><![CDATA[EntityIdentify a distinct logic block Consists of the input and output ports (I/O pins) Each port can have several variables 1234entity nand2 isport (a,b : in std_logic; y : out std_logic);end nand2; Ports can be in (read), out (write), inout (bi-directional) Std_logic is the type of signal being carried at the ports Std_logic can have values such as ‘0’, ‘1’, ‘X’, ‘Z’, ‘-’ and others.(‘ z ‘ means high impedance (tristate)) Another signal type is “bit” which only assumes binary values of ‘0’ and ‘1’. ArchitectureFollow immediately after the entity declaration,to define the internal operation (function) of the logic module Defines the relationship between the input and output signals May have internal signals in the definition AN entity can have several different architecture specifications to describe different views or different levels 12345678entity inverter_gate is port (A: in std_logic; Z: out std_logic);end inverter_gate;architecture DATAFLOW of inverter_gate isbegin z &lt;= not A after 10 ns;end DATAFLOW; 1234567Entity OR3 isport (I1,I2,I3: in std_logic; O: out std_logic);end OR3;architecture BEHAVIOR of OR3 isbegin O &lt;= I1 or I2 or I3;end BEHAVIOR; Internal signalsused to simplify the model ordefine connections between sub-components inside the architecture 1234architecture example of special_mus is signal control, tmp : std_logic;beginend example; Concurrent Statements Signal Assignments Component Instantiations for structural description) Processes IN ORDER Signal Assignments1C &lt;= '0'; Delayed1c &lt;= A and B after 10ns; --Only for simulation. Selected (outside PROCESS)connect target to many cases of one source. 1234with sel select DAta_out &lt;= a when '0', b when '1', 'X' when others; Conditional (outside PROCESS)connect target to many cases of several sources. 123Y &lt;= a when en = ‘1’ else ‘Z’ when en = ‘0’ else ‘X’; Component Instantiations for structural description123COMPONENT xor_gate PORT ( A, B: IN STD_LOGIC; C: OUT STD_LOGIC);END COMPONENT; Processes IN ORDERIf-else Branchmany signals many cases.1234567if sth = '0' then A = '1'; B = '0'; -- IN-ORDER processeselsif sth = '1' then -- optional B = '1'; C = '0'; -- optionalelse -- IMPORTANT! C = '1';end if； case-when Branchone signal many cases.12345case sth is when '0' =&gt; A = '1'; B = '0'; when '1' =&gt; B = '1'; C = '0'; when others =&gt; C = '1'; -- IMPORTANT!end case; Variable and SignalThe variable is updated without any delay as soon as the statement is executed.Variables must be declared inside a process (and are local to the process).Assignment := executed sequentially! 123456789101112131415architecture VAR of EXAMPLE is signal TRIGGER, RESULT: integer :=0;begin process variable variable1: integer := 1; variable variable2: integer := 2; variable variable3: integer := 3; begin wait on TRIGGER; variable1 := variable2; --2 variable2 := variable1 + variable3; --2+3=5 variable3 := variable2; --5 RESULT &lt;= variable1 + variable2 + variable3; --2+5+5 end process;end VAR; The signal is updated after a time delay (delta delay if no delay is specified).Signals must be declared outside the process.Assignment &lt;= executed concurrently! 123456789101112131415architecture SIGN of EXAMPLE is signal TRIGGER, RESULT: integer :=0; signal signal1: integer := 1; signal signal2: integer := 2; signal signal3: integer := 3;begin process begin wait on TRIGGER; signal1 &lt;= signal2; --2 signal2 &lt;= signal1 + signal3; --1+3 signal3 &lt;= signal2; --2 RESULT &lt;= signal1 + signal2 + signal3; --1+2+3 end process;end VAR; Dataflow architectureSpecify input/output relations in Boolean expression or conditional statements 1234567891011ENTITY 2_to_4_decoder IS PORT (A, B, E: IN STD_LOGIC; D: OUT STD_LOGIC_VECTOR (3 downto 0);END 2_to_4_decoder;ARCHITECTURE dataflow OF 2_to_4_decoder ISBEGIN D(0) &lt;=‘0’ WHEN (A=‘0’ AND B=‘0’ AND E=‘0’) ELSE ‘1’; D(1) &lt;=‘0’ WHEN (A=‘0’ AND B=‘1’ AND E=‘0’) ELSE ‘1’; D(2) &lt;=‘0’ WHEN (A=‘1’ AND B=‘0’ AND E=‘0’) ELSE ‘1’; D(3) &lt;=‘0’ WHEN (A=‘1’ AND B=‘1’ AND E=‘0’) ELSE ‘1’;END dataflow; Structural architectureDescribes a set of interconnected components. 1234567891011121314151617ARCHITECTURE structural OF four_bit_adder_sub IS COMPONENT four_bit_adder PORT ( A: IN STD_LOGIC_VECTOR (3 downto 0); B: IN STD_LOGIC_VECTOR (3 downto 0); C0: IN STD_LOGIC; C4: OUT STD_LOGIC; S: OUT STD_LOGIC_VECTOR (3 downto 0)); END COMPONENT; COMPONENT xor_arrays PORT ( X: IN STD_LOGIC_VECTOR (3 downto 0); Z: IN STD_LOGIC; Y: OUT STD_LOGIC_VECTOR (3 downto 0)); END COMPONENT; signal Y: STD_LOGIC_VECTOR (3 downto 0);BEGIN Block1: xor_arrays PORT MAP (B, M, Y); Block2: four_bit_adder PORT MAP (A, Y, M, C, S);END structural; Behavioral architectureAny change in the values of Sensitivity List will cause immediate execution of the process 1234567891011121314151617ENTITY mux4 ISPORT ( S: IN STD_LOGIC_VECTOR (1 downto 0); I: IN STD_LOGIC_VECTOR (3 downto 0); Y: OUT STD_LOGIC);END mux4;ARCHITECTURE behavioral OF mux4 ISBEGIN PROCESS (S, I) -- Sensitivity List BEGIN CASE S IS WHEN “00” =&gt; Y &lt;= I(0); WHEN “01” =&gt; Y &lt;= I(1); WHEN “10” =&gt; Y &lt;= I(2); WHEN “11” =&gt; Y &lt;= I(3); END CASE; END PROCESS;END behavioral;]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>VHDL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[COMP2611 Final Review]]></title>
    <url>%2F2018%2F12%2F07%2FCOMP2611-Final-Review%2F</url>
    <content type="text"><![CDATA[@[TOC] Arithmetic for ComputersOverflow Detection Operation Sign bit of X Sign Bit of Y Sign Bit of Result X+Y 0 0 1 X+Y 1 1 0 X-Y 0 1 1 X-Y 1 0 0 Multiplication Division PipelineStages:IF : Fetch the instructions from meomryID : Instruction decode &amp; register readEX : Perform ALU operationMEM : Memory access (if necessary)WB : Write result back to register Registers:located between the stages:​ IF/ID, ID/EX, EX/MEM, MEM/WB Control:IF : no control signalsID : no control signalsEX : RegDst, ALUOP, ALUSrcMEM : Branch, MemRead, MemWriteWB : MenToReg, RegWrite Hazards:cause: Data dependence and Control dependence.types: Structural hazards, Data hazards, Control hazards Structural hazardsConflict for use of memory(MEM/IF) and registers(WB/ID). Solution: separate instruction and data memories Fact: Register very fast. Registers can Write during 1st half of clock cycle and Read during 2nd half. Data hazards add $s0, $t0, $t1sub $t2, $s0, $t3 Solution: Forwarding (partial, can’t solve below code) lw $s0, 20$(t1)sub $t2, $s0, $t3 Scheduling (assume forwarding is used) Control hazardsWhen we need to beq , bne operations Solution: Fetch instruction after branch. If wrong, flush them away. Add comparator to compare earlier Datapath with HazardsData forwardingTwo types forwarding: EX/MEM -&gt; ID/EX and MEM/WB -&gt; ID/EXUsing two multiplexers to decide what is the input of operands A and B of the ALU.note: A is for Rs. B is for Rt. EX forwarding:123456if (EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠ 0) and (EX/MEM.RegisterRd = ID/EX.RegisterRs )) ForwardA = 10 if (EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠ 0) and (EX/MEM.RegisterRd = ID/EX.RegisterRt )) ForwardB = 10 MEM forwarding (only when NOT EX forwarding ):12345678910if (MEM/WB.RegWrite and (MEM/WB.RegisterRd ≠ 0) and not (EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠ 0) and (EX/MEM.RegisterRd = ID/EX.RegisterRs ))and (MEM/WB.RegisterRd = ID/EX.RegisterRs )) ForwardA = 01 if (MEM/WB.RegWrite and (MEM/WB.RegisterRd ≠ 0) and not (EX/MEM.RegWrite and (EX/MEM.RegisterRd ≠ 0) and (EX/MEM.RegisterRd = ID/EX.RegisterRt ))and (MEM/WB.RegisterRd = ID/EX.RegisterRt )) ForwardB = 01 Load-use detectionWhen Load-use hazard occur, we can do nothing but stall a clock cycle.To detect it: 123Load-use hazard = ID/EX.MemRead and ((ID/EX.RegisterRt = IF/ID.RegisterRs ) or (ID/EX.RegisterRt = IF/ID.RegisterRt )) How to Stall Force control values in ID/EX register to 0 :​ EX, MEM and WB do EX, MEM and WB do nop (no -operation) Prevent update of PC and IF/ID registerUsing instruction is decoded againFollowing instruction is fetched again1-cycle stall allows MEM to read data for lw (with forwarding) Data Hazard when branchIn pipeline datapath, branch target address calculation is executed in another ALU NOT in EXE stage. It is executed during ID staged. ID Stage: Target address calculator, Register file, Register comparator. add $1, $2, $3add $4, $5, $6add $7, $8, $9 # another instructionbeq $1, $4, target Can resolve using forwarding. From MEM/WB and EXE/MEM to ID add $1, $2, $3add $4, $5, $6beq stalled…beq $1, $4, target Need one cycle stalled. lw $1,addrbeq stalled…beq stalled…beq $1, $0, target Need two cycles stalled. MemoryRAM(Random Access Memory) technology includes two types: Static RAM — mostly used for cache0.5ns-2.5ns, $2000 -$5000 per GBconsists only Transistors Dynamic RAM(DRAM) — mostly used for main memory 50ns - 70ns, $20-$75 per GB consists of Transistor and Capacitor Magnetic disk: 5ms - 20ms, $0.2-$2 per GB Disk Sectors and AccessesEach sector records:Sector IDData (512 bytes, 4096 bytes proposed)Error correcting code (ECC)Synchronization fields and gaps Access to a sector involves:Queuing delay if other accesses are pendingSeek: move the headsRotational latencyData transferController overhead Principle of Locality Programs access a small proportion of their address space at any time Temporal locality Items accessed recently are likely to be accessed again soon Spatial localityItems near those accessed recently are likely to be accessed soon This property is the KEY to memory hierarchy! Memory HierachyInitially,Instructions and data are loaded into DRAM from disk. Upon first access,A copy of the referenced instruction or data item is kept in cache In subsequent accesses,First, look for the requested item in the cache​ If the item is in the cache, return the item to the CPU​ If NOT in the cache, look it up in the memory We can say: If not found in this level, look it up at next level until foundKeep (cache) a copy of the found item at this level after use CacheEach memory locations is mapped to ONE location in the cache. average latency = hit + miss= r*t1 + ((1-r)t1 + rt2) = t1 + (1-r)t2NOT r*t1 + (1-r)t2 Three basic organizations: Direct-mapped( one memory block to one possible cache block) Set-mapped( one memory block to one set of possible cache blocks) Fully-mapped( one memory block to all possible cache blocks) Direct Mapped CacheIf the number of cache blocks (N) is a power of 2; N=2^mcache_location = the low-order m bits of block addresse.g. map the value1200 to a 8 blocks, 32 byte/block cache. Block address = 1200/32 = 37Block number = 37 % 8 = 5 Disadvantage:block access sequence: 100011, 001011, 100011later arrival will kill out previous one. called cache conflict. Tags and Valid BitsTags: decide which particular block is stored in a cache location.Store block address as well as the data.Only need the high-order bits.tags size= original address size- block size- blocks number(binary) - 1(valid bit) Valid Bits: decide whether the data exists. Initially 0. Consider a direct-mapped cache with 8 cache frames and a block size of 32 bytesMemory (byte) address generated by CPU|Block address|Hit or miss|Assigned cache block—-|—-|—|—-0010 1100 0010|0010 110|Miss|1100011 0100 0000|0011 010|Miss|0100010 1100 0100|0010 110|Hit|1100010 1100 0010|0010 110|Hit|1100010 0000 1000|0010 000|Miss|0000000 0110 0000|0000 011|Miss|0110010 0001 0000|0010 000|Hit|0000010 0100 0001|0010 010|Miss|010 Associate CacheFully associative(FA):Each block can be placed anywhere in the cache.pros: No cache conflict. But still have misses due to size.cons: Costly (hardware and time) N-way Set associative(SA):Each block can be placed in a certain number(N) of cache locations.A good compromise between DM &amp; FA Block Replacement Random Least recently used(LRU):Upon miss:Replace the LRU with missed address -&gt; Move the miss address to MRU positionUpon hit:Move the hit address to MRU position -&gt; Pack the rest Deal with Dirty DataWrite-backCPU only write to cache.CPU can write individual words at the rate of the cache.Multiple writes to a block are merged into one write to main memory.more and more caches use this strategy. Write-throughCPU write both on cache and memory.(memory always up-to-date)]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>MIPS</tag>
        <tag>assemble</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hashing]]></title>
    <url>%2F2018%2F12%2F07%2FHashing%2F</url>
    <content type="text"><![CDATA[Hash TableA hash table is an array of some fixed size,containing all the data items.Each item has a key search is performed based on the keys.Each key is mapped into some position in the array in the range 0 to m − 1, where m is thearray size.The mapping is called hash function. Hashing table is a data structure that supports: searching insertion deletion The implementation of hash tables is called hashing.Hashing is a technique which allows the executions of above operations in constant time on average.Unlike other data structures such as linked list or binary trees, data items are generally not ordered in hash tables.As a consequence, hash tables don’t support the following operations: find_min and find_max finding successor and predecessor reporting data with a given range listing out the data in order Hashing FunctionDesignA simple and reasonable strategy: h(k) = k mod m. Its a good practice to set the table size m to a prime number. h(key) =(key[0] + 37*key[1] + 37^2*key[2] + … ) mod m Handling collisionMake hash table an array of linked lists Seperate ChainingInsertionTo insert a key k: Compute h(k)If T(h(k)) contains nullptr , initial this table entry to point to linked list node of kk .If T(h(k)) contains non-nullptr, add k to the beginning of the list. DeletionTo delete a key k: Compute h(k) to determine which list to traverse.Search for the key k, in the list that T(h(k)) points to.Delete the item with key k if it is found. Open addressingInstead of putting keys of the same hash table into a chain, open addressing will relocate the key k to be inserted if it collides with an existing key. Linear probing f(i) = ihi(k) = (hash(k) + i) mod m Disadvantage: fall into cluster quadratic probing f(i) = i^2^hi(k) = (hash(k) + i^2^) mod m Disadvantage: second cluster… same key same probe steps. double hashing f (i) = i × hash2(k)h~i~(k) = (hash(k) + i × hash~2~(k)) mod m hash~2~(k) must be relatively prime to the table size m.Otherwise, we will only be able to examine a fraction of the table entries.]]></content>
      <categories>
        <category>School Notes</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>hashing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AVL Trees]]></title>
    <url>%2F2018%2F12%2F06%2FAVL-Trees%2F</url>
    <content type="text"><![CDATA[(All codes below are written by Dr. Desmond) AVL(Adelson-Velsky and Landis) TreesEvery sub-tree of an AVL tree is itself an AVL tree.(An empty tree is an AVL tree too.)123456struct AVLNode&#123; T data; int height; // Important! AVLNode* left; AVLNode* right; &#125; AVL Tree Searchingsame as BST searching AVL Tree InsertionInsertion may violate the AVL tree property in 4 cases: Insertion into the right sub-tree of the right childLeft(anti-clockwise) rotation [single rotation] Insertion into the left sub-tree of the left childRight(clockwise) rotation [single rotation] Insertion into the right sub-tree of the left childLeft-right rotation [double rotation] Insertion into the left sub-tree of the right childRight-left rotation [double rotation] !!! Distinguish L/R of subtree or L/R of child !!! First insert the node to right place, then balance it by rotation.123456789AVLNode&lt;T&gt;* AVL&lt;T&gt;::insert(AVLNode&lt;T&gt;* p, const T&amp; d)&#123; if( !p ) return new AVLNode&lt;T&gt;(d); // Empty AVL tree if( d &lt; p-&gt;data ) p-&gt;left = insert(p-&gt;left, d); // Recursion on the left sub-tree else p-&gt;right = insert(p-&gt;right, d); // Recursion on the right sub-tree return balance(p);&#125; RotationSingle rotation operate on the node, replace original node by its childDouble rotation first operate on the child, then on the node, replace original node by its grandchild. 1234567891011121314151617181920212223242526272829303132AVLNode&lt;T&gt;* AVL&lt;T&gt;::rotateRight(AVLNode&lt;T&gt;* p)&#123; AVLNode&lt;T&gt;* pl = p-&gt;left; p-&gt;left = pl-&gt;right; pl-&gt;right = p; updateHeight(p); updateHeight(pl); return pl;&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::rotateRightLeft(AVLNode&lt;T&gt;* p)&#123; rotateRight(p-&gt;right); return rotateLeft(p);&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::balance(AVLNode&lt;T&gt;* p)&#123; h-&gt;height = max(h-&gt;right-&gt;height,h-&gt;left-&gt;height); // heightDiff() return (right subtree height) - (left subtree height) if( heightDiff(p) == 2 )&#123; if( heightDiff(p-&gt;right) &lt; 0 ) // if need double rotation p-&gt;right = rotateRight(p-&gt;right); return rotateLeft(p); // single rotation &#125; if( heightDiff(p) == -2 )&#123; if( heightDiff(p-&gt;left) &gt; 0 ) // if need double rotation p-&gt;left = rotateLeft(p-&gt;left); return rotateRight(p); // single rotation &#125; return p;&#125; AVL Tree DeletionThe idea is same as BST.When delete node with two children, the following code delete the node andrise the min node up.Then balance the whole tree.123456789101112131415161718192021222324252627282930313233AVLNode&lt;T&gt;* AVL&lt;T&gt;::findMin(AVLNode&lt;T&gt;* p) const&#123; return p-&gt;left ? findMin(p-&gt;left) : p;&#125;// NOT delete the Min-node!!AVLNode&lt;T&gt;* AVL&lt;T&gt;::removeMin(AVLNode&lt;T&gt;* p)&#123; if(p-&gt;left == 0) // If it's the min node, not return itself return p-&gt;right; // Return the second min means removeMin! p-&gt;left = removeMin(p-&gt;left); // Recursion on the left sub-tree return balance(p);&#125;AVLNode&lt;T&gt;* AVL&lt;T&gt;::remove(AVLNode&lt;T&gt;* p, const T&amp; d) &#123; if( !p ) return 0; // Item is not found; do nothing if( d &lt; p-&gt;data ) p-&gt;left = remove(p-&gt;left,d); // Recursion on the left sub-tree else if( d &gt; p-&gt;data ) p-&gt;right = remove(p-&gt;right,d); // Recursion on the right sub-tree else &#123; // Item is found AVLNode&lt;T&gt;* pl = p-&gt;left; AVLNode&lt;T&gt;* pr = p-&gt;right; delete p; // Remove the node with value d if( !pr ) return pl; // Return left sub-tree if no right sub-tree AVLNode&lt;T&gt;* min = findMin(pr); // Find min. node of the right sub-tree min-&gt;right = removeMin(pr); // Did NOT delete the min node on right sub-tree min-&gt;left = pl; return balance(min); // Balance this node &#125; return balance(p); // Balance this node&#125; Explanationp-&gt;left = remove(p-&gt;left,d);Connect between parent and child. So no need to store parent pointer.delete p;Truly delete node with required data. But store its left and right nodes.Later return min of right sub-tree to its parent node. No memory leak.min-&gt;right = removeMin(pr);Return from the second min node and balanced from bottom up to the deleted node’s right child.Did not truly delete the min node. It should already be stored by findMin() function. Also refer here]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Trees, Binary Trees, Binary Search Trees]]></title>
    <url>%2F2018%2F12%2F05%2FTrees-Binary-Trees-Binary-Search-Trees%2F</url>
    <content type="text"><![CDATA[Binary TreeInorder traversalpreorder traversalpostorder traversal Binary Search TreeBST InsertionAlways insert element at bottom as leaf.Remember to check if the element already exists in the tree. Recursion12345678910bool insert(const T&amp; x, Node*&amp; n ) // pass by referenceNode* insert(const T&amp; x, Node* n ) // pass by value need return pointer&#123; // base case 1: reach the node beyond leaf if(!n) &#123;n-&gt;x = new T(x);return true / new T(x);&#125; // base case 2: the data already exists if(n-&gt;x == x) return false / nullptr ; if(x &lt; n-&gt;x) return insert(x,n-&gt;left); else return insert(x,n-&gt;right);&#125; The insert function parameter Node*&amp; is a reference.We always pass reference to it, thought the value of the reference may be nullptr.e.g. insert(x,t-&gt;left) .In this case, t always refers to a Node*. Iteration12345678910111213141516bool insertIterative(const T&amp; x, Node* p ) // pass by value&#123; // A new pointer to refer to the leaf Node* pp = nullptr; for(;p!=nullptr;) &#123; // already exists if(p-&gt;x == x) &#123;return false;&#125; pp = p; // save parent for reference if(p &lt; p-&gt;x) p = p-&gt; left; else p = p-&gt;right; &#125; // root is nullptr if(!pp) root = new T(x); else pp = new T(x);&#125; We only need to know the value of the root of the tree.But we need to record the reference of the parent of the insertion node. BST Searchpublic interface: 1234567891011bool contain(const T&amp; x) const&#123; Node* p = root; Node* pp = nullptr; // Non-recursion method COMMON return search(x,p,pp); // Recursion method return searchRecur(x,p) != nullptr; &#125; The implementation of search(x,p) and searchIterative(x,p,pp) is as below. Recursioncan NOT keep track of the parent node pointer.123456789Node* searchRecur(const T&amp; x, Node* n ) // pass by value&#123; // base case 1: reach the node beyond leaf if(!n) &#123;return nullptr;&#125; // base case 2: find the data if(n-&gt;x == x) return n; if(x &lt; n-&gt;x) return search(x,n-&gt;left); else return search(x,n-&gt;right);&#125; Iteration1234567891011bool search(const T&amp; x, Node*&amp; n, Node*&amp; pn )// pass by reference&#123; for(;n!=nullptr;) &#123; if(n-&gt;x == x) &#123;return true;&#125; pn = n; if(x &lt; n-&gt;x) n = n-&gt; left; else n = n-&gt;right; &#125; return false;&#125; Can keep track of parent node pointer.Note the Node*&amp; n and Node*&amp; pn is pas by reference.They must be created before calling the function BST deletionneed findMin() function to help. 123456Node* findMin(Node* p)&#123; if(!p)return nullptr; while(p-&gt;left)p=p-&gt;left; return p;&#125; Delete a node with no child : pointer: current parentSimply delete it and make its parent’s original pointer to it nullptr Delete a node with one child : pointer: current parent childPass its pointer to the only child to its parent’s pointer to it. Then delete it. Delete a node with two child : pointer: current min/max parent of min/maxOverride its data to the max of left or min of rightActually delete the min/max node of the BST, which has no child Recursion1234567891011121314151617181920212223242526272829303132bool remove(const T&amp; x,Node*&amp; n) // pass by reference&#123; // Base case: NOT found if(!n) return false; // Recursive steps if(x&lt;n-&gt;x) return remove(x,n-&gt;left); if(x&gt;n-&gt;x) return remove(x,n-&gt;right); // node with no child if(!n-&gt;left &amp;&amp; !n-&gt;right) &#123; delete n; n = nullptr; &#125; // node with two children else if(n-&gt;left &amp;&amp; n-&lt;right) &#123; Node* rightMin = findMin(n-&gt;right); n-&gt;x = rightMin-&gt;x; // always true remove(n-&gt;x,rightMin); &#125; // node with only one child else &#123; Node* child = root-&gt;left?root-&gt;left:root-&gt;right; // save &amp;n to curr, later delete Node* curr = n; n = n-&gt;child; delete curr; &#125; return true;&#125; IterationNeed the parent node pointer of Min node.Hard to implement.]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>data structure</tag>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Static Variables]]></title>
    <url>%2F2018%2F12%2F05%2FStatic-Variables%2F</url>
    <content type="text"><![CDATA[Static variablesGlobal Scope created only once in a program. reside on the static data region of the loaded program. have a lifetime across the entire run of a program. still may have limited scope: file, function, class. Function Scope initialized only once regardless how many times the function is called. retain their values across the function calls. can be accessed only inside the function. Class Scope variables in different objects are actually one variable. initialize variable must be done in global scopecannot initialize it inside the class. NOT in any function! Not in main(), either!NO keyword statice.g. 1234&gt; class A &#123;static int a;&#125;;&gt; int A::a = 10;&gt; int main()&#123;&#125;&gt; variable exists even if no class object is created. Static FunctionsGlobal Functions almost same as static variables. Member Functions do not have the implicit this pointer like regular non-static member functions. may be used even when there are no objects of the class! can only make use of static data members of the class. cannot be const nor virtual functions. can be defined outside the class declaration NO keyword static e.g. 1234&gt; class A &#123;static void f();&#125;;&gt; void A::f()&#123;&#125;&gt; int main()&#123;&#125;&gt; &gt;&gt;]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
        <tag>static variable</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Inheritance and Polymorphism]]></title>
    <url>%2F2018%2F12%2F05%2FInheritance-and-Polymorphism%2F</url>
    <content type="text"><![CDATA[Inheritance_Inheritance_: describe “is-a relationship”_Polymorphism_: derived class can perform like base class(object,pointer,reference) _Construction Order_: from inner to outer_Destruction Order_: from outer to inner member access control_public_: member functions of the class(inner)any member funtions of other classes(other class)any global functions(outside) _protected_: member functions and _friends_ of the classmember functions and _friends_ of its _derived classes_NOT for outside functions _private_: member functions and friends of the class Without inheritance, private and protected have exactly the same meaning. public protected private inheritance inheritance\member public protected private public inheritance public protected private protected inheritance protected protected private private inheritance private private private Public inheritance implements the “is-a” relationship Private inheritance is similar to “has-a” relationship Friendfriend of Base is not friend of Derived PolymorphismDynamic Binding&amp;Virtual FunctionOnce a method is declared virtual in the base class, it is automatically virtual in all directly and indirectly derived classes. Even if derived classed do not announce it’s virtual functions. Virtual destructor can make deleting a base pointer to derived object operate correctly. Do not rely on the virtual function mechanism during the execution ofa constructor. Similarly, if a virtual function is called inside the base class destructor,it represents base class’ virtual function: when a derived class is beingdeleted, the derived-specific portion has already been deleted beforethe base class destructor is called. Abstract Base Class(ABC):NO object of ABC can be created Its derived classes must implement the pure virtual functions. ABC is just an interface.]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[C++ Memory Leak]]></title>
    <url>%2F2018%2F11%2F26%2FC-Memory-Leak%2F</url>
    <content type="text"><![CDATA[Memory detectionIn Windows using Visual Studiohere Open source toolshere]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
        <tag>memory</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[AI Chap.3 Searching]]></title>
    <url>%2F2018%2F06%2F28%2FAI-Chap-3-Searching%2F</url>
    <content type="text"><![CDATA[Describe problem formallystate: initial state:actions: each state corresponds to a set of available actionstransition model: return the result state of an action from the previous stategoal test:path cost: the essence of a solution is a sequence of actions that lead from the initial state to the goal state. Problem formulationincremental formulation: e.g. 8-queens puzzle. Initial state: no queen; Action: add a queen.complete-state formulation: e.g. 8-queens puzzle. Initial state: 8 queens; Action: move a queen. Searchsearch algorithms all use a search tree data structure with nodes. noden.state: correspond state in state space;n.parent: the node in the search tree that generated this node;n.action: the action that was applied to the parent to generate this node;n.path-cost: succeeded from parent; other conceptsexpanding: apply each legal action to the current state, thereby generating a new set of states.parent node ; child node ; leaf node: a node with no child nodefrontier(open list): the set of all leaf nodes avaiable for expansionexplored set(close list): the set of expanded nodesthe frontier separates the state space into explored region and unexplored regionrepeated state ; loopy path: a special case of the more general concept of redundant path Uninformed(blind) searchtree searchnot remember history causes loopy paths which is a special case of redundant path graph searchadd explored set(close list) to remember breadth-first searchFIFO queue; expand shallowest node;time complexity: O(b^d) (test while generating) O(b^(d+1)) (test while selected)space complexity: all nodes uniform-cost searchpriority queue; expand cheapest node;test while selected because while generating it may not be the optimal one.time complexity: e be the minimum step-cost,C* be the optimal cost. O(b^floor(1+(C*/c)))space complexity: near nodes depth-first searchLIFO stack; first to deepest node that has no successors;common to implement with a recursive function, recursive depth-first search(RDFS)time complexity: O(b^m^)space complexity: m is the maximum depth. O(bm)or O(m)( backtracking search in which expanded node remembers which successor to generate next) depth-limited searchinfinite state space cause depth-first search fall. It can be alleviated by a depth limit l. iterative deepening depth-first searchoften used in combination with depth-first search to find the best depth limit.gradually increase the depth limit till the goal is found.upper levels generate multiple times: N(IDS) = db+(d-1)b^2^+…+(1)b^d^ bidirectional search b^(d/2)^ + b^(d/2)^ &lt; b^d^ but action step must be reversible Summary Informed(Heuristic) searchgreedy best-first searchevaluates nodes by using just the heuristic function f(n) = h(n) time complexity: depends on heuristic function, worst O(b^m^), m is the maximum depth. space complexity: depends on heuristic function, worst O(b^m^), m is the maximum depth. A* search f(n) = g(n) + h(n) admissible heuristic: never overestimates the cost to reach the goalconsistent heuristic: the estimated cost of n is no greater than n’tree-search optimal if h(n) is admissible;graph-search optimal if h(n) is consistent absolute error: E=h*-h; relative error: e=(h*-h)/h time complexity: O(b^E^) or O(b^ed^)(constant step costs)space complexity: all nodes that f(n) &lt; C* Memory-bounded heuristic searchiterative-deepening A*(IDA*) cutoff = f(n) - g(n) - h(n) each iteration cutoff value is the smallest (f-g-h) of any node exceeded the cutoff on the previous iterationtoo little memory: between iteration, only retains cutoff = f-g-h recursive best-first search(RBFS)use f_limit variable to keep track of f-valueuse alternative variable to record the second-lowest f-value among successors (backed-up value)too little memory: each time change mind to alternative path, forget what it have done and regenerate nodes MA* and SMA*MA*(memory-bounded A*) and SMA*(simplified MA*) SMA* expands the best leaf until memory is full, drops the worst leaf node(highest h),like RBFS, SMA* back up the value of the forgotten node to its parent learning to search bettermetalevel state space: the internal(computational) state of a program that is searching in anobject-level state space: the real world problem Heuristic functionsFrom relaxed problemsignore some restriction rules and estimate the costrelaxed problem must be able to solved without searchwe can use more than one heuristic function like below: h(n) = max{h1(n), h2(n), …, hm(n)} From subproblems: Pattern databases subproblems: just solve part of the problem like going just half waypattern database: store these exact solution costs for every possible subproblem instancedisjoint pattern database: only consider partial cost of the subproblem that matters learning heuristics from experienceEach solution is an example for study.From these examples, a learning algorithm can be used to construct a function h(n)to predict solution costs for other states that arise during search.Inductive learning works best if supplied with features of a state relevant to predicting state’s valuethe feature is x1(n), x2(n), …, xm(n) after solved the problem, we found that h1(n), h2(n), …, hm(n) then we approach is to use a linear combination h(n) = c1*x1(n) + c2*x2(n) + … + cm*xm(n) Summary ![AI-Ch3-summary-2]/assets/AI-Chap-3-Searching/AI-Ch3-summary-2.png)]]></content>
      <categories>
        <category>AI study</category>
      </categories>
      <tags>
        <tag>searching</tag>
        <tag>AI</tag>
        <tag>algorithm</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Dancing Video]]></title>
    <url>%2F2018%2F06%2F18%2FDancing-Video%2F</url>
    <content type="text"><![CDATA[Locking Judge Showhere Poppin Semi-final (win)here Poppin Final (lose)here]]></content>
      <categories>
        <category>life</category>
      </categories>
      <tags>
        <tag>life</tag>
        <tag>dance</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Linux Notes]]></title>
    <url>%2F2018%2F06%2F08%2FLinux-notes%2F</url>
    <content type="text"><![CDATA[Keyboardhere About finding fileshere]]></content>
      <categories>
        <category>Linux Is Not UniX</category>
      </categories>
      <tags>
        <tag>linux</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Cpp Notes]]></title>
    <url>%2F2018%2F05%2F27%2FCpp-Notes%2F</url>
    <content type="text"><![CDATA[About private member1 在C+的类的成员函数中，允许直接访问该类的对象的私有成员变量2 在类的成员函数中可以访问同类型实例的私有变量量3 拷贝构造函数里，可以直接访问另外一个同类对象（引用）的私有成员4 类的成员函数可以直接访问作为其参数的同类型对象的私有成员 Below codes work!!123456789101112private: int data;public: void lookOtherConst(const A&amp; a)&#123; cout&lt;&lt;"other's private data is "&lt;&lt; a.data &lt;&lt;endl;&#125; void lookOtherReference(A&amp; a)&#123; cout&lt;&lt;"other's private data is " &lt;&lt;a.data&lt;&lt;endl;&#125; void lookOther(A a)&#123; cout&lt;&lt;"other's private data is " &lt;&lt;a.data&lt;&lt;endl;&#125; About f*cking consthere 前缀 与 后缀 运算符前缀：在表达式计算中使用值之前递增或递减，表达式的值与操作数的值不同后缀：在表达式使用值之后递增或递减， 表达式的值与操作数相同e.g.1234567int a=5,b =5;cout&lt;&lt;"a&lt;b++ = "&lt;&lt;bool(a&lt;b++)&lt;&lt;endl;cout&lt;&lt;"a&lt;++b = "&lt;&lt;bool(a&lt;++b)&lt;&lt;endl;output:a&lt;b++ = 0 a&lt;++b = 1]]></content>
      <categories>
        <category>I don&#39;t know about C++</category>
      </categories>
      <tags>
        <tag>Cpp</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Vim Notes]]></title>
    <url>%2F2018%2F05%2F25%2FVim-notes%2F</url>
    <content type="text"><![CDATA[Neovim and vim-pluginA good blog link for both introduction of neovim and vim-plugin. search and substitutionhere fold and unfoldhere keyboard remaphere]]></content>
      <categories>
        <category>VIM is so hard to use</category>
      </categories>
      <tags>
        <tag>vim</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Origin]]></title>
    <url>%2F2018%2F05%2F23%2FOrigin%2F</url>
    <content type="text"><![CDATA[Trust me, you would not like to read the shix inside, nor do I. Decrypt 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]]></content>
      <categories>
        <category>life</category>
      </categories>
  </entry>
</search>
